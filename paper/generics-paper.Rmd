---
title: "Communicating generalizations about categories"
short-title: "Communicating generalizations"
output: kmr::apa_manuscript
csl: apa6.csl
bibliography: generics.bib

document-params: "a4paper,man,apacite,floatsintext"

bib-tex: "library.bib"

author-information:
    - \author{Michael H. Tessler \\ Noah D. Goodman}

affiliation-information:
    - \affiliation{Department of Psychology, Stanford University}

author-note:
    "Correspondence concerning this article should be addressed to: Michael H. Tessler, Stanford University, Department of Psychology, 450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305, mtessler@stanford.edu, 757-561-7971."
    
abstract: 
    "Generalizations about categories are central to human understanding, and generic language (e.g.*Dogs bark.*) provides a simple and ubiquitous way to communicate these generalizations. 
    Yet the meaning of generic language is philosophically puzzling and has resisted precise formalization.
We explore the idea that the core meaning of a generic sentence is simple but underspecified, 
and that general principles of pragmatic reasoning are responsible for establishing the precise meaning in context.
Building on recent probabilistic models of language understanding, we provide a formal model for the evaluation and comprehension of generic sentences. 
This model explains the puzzling flexibility in usage of generics in terms of diverse prior beliefs about properties.
We elicit these priors experimentally and show that the resulting model predictions explain almost all of the variance in human judgments for both common and novel generics.
We probe the theory in more detail, and find that generic language depends in a fundamental way on subjective beliefs, not mere frequency. 
This theory provides the mathematical bridge between the words we use and the concepts they describe."
    
keywords:
    "generics, pragmatics, semantics, bayesian modeling"
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T)
```

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

# Introduction 

Most would agree that \emph{Swans are white}, but certainly not every swan is.
This type of utterance conveys a generalization about a category (i.e. \textsc{swans}) and is known as a generic utterance [@Carlson1977; @Leslie2008].
Communicating generically about categories is useful because categories themselves are unobservable [@Markman1989].
<!--Knowledge about categories, while central to human reasoning, is tricky to acquire because categories themselves are unobservable [Markman1989}.-->
It is believed that every language can express generic meaning [@Behrens2005; @Carlson1995], and that generics are essential to the growth of conceptual knowledge [@Gelman2004] and how kinds are represented in the mind [@Leslie2008].
Generic language is ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008], and children as young as two or three understand that generics refer to categories and support generalization [@Cimpian2008].
<!--% and though English and many other languages do not possess an unambiguous form devoted to generic meaning [@Behrens2000, @AlMalki2014]. -->
Additionally, generics are the primary way by which speakers discuss social categories, making them key to propagating stereotypes [@GelmanEtAl2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010motivation].
Despite their psychological centrality, apparent simplicity, and ubiquity, a formal account of generic meaning remains elusive.

The major issue in formalizing generic language is determining what makes a generic sentence true or false.
At first glance, generics feel like universally-quantified statements as in \emph{All swans are white}. 
Unlike universals, however, generics are resilient to counter-examples (e.g., \emph{Swans are white} even though there exist black swans). 
Intuitions, then, fall back to something more vague like \emph{Swans, in general, are white} because indeed most swans are white.
But mosquitos, in general, do not carry malaria, yet everyone agrees \emph{Mosquitos carry malaria}.
%Interpreting the generic as meaning "most" (i.e. \emph{Most swans are white}) captures many cases, but cannot explain why \emph{Robins lay eggs} and \emph{Mosquitos carry malaria} are so intuitively compelling: Only adult female robins lay eggs and a very tiny fraction of mosquitos actually carry malaria.
Indeed, it appears that any \emph{truth condition} stated in terms of how common the property is within the kind violates intuitions.
Consider the birds: for a bird, being female practically implies you will lay eggs (the properties are present in the same proportion), yet we say things like \emph{Birds lay eggs} and we do not say things like \emph{Birds are female}.


How particular interpretations arise from generic language is also a mystery.
The case of \emph{Mosquitos carry malaria} suggests the generic must in some way be analogous to "some" (i.e. \emph{Some mosquitos carry malaria.}). 
Yet, the empirical literature suggests generics are often interpreted as implying the property is widespread:
There is a difference between \emph{Some swans have hollow bones} and \emph{Swans have hollow bones} [@Gelman2002].
When interpretations are compared directly against truth conditions (i.e., how prevalent a property would need to be for the generic to be true), both children and adults infer the property is \emph{more} prevalent when told the generic than when judging the generic true [@Cimpian2010; @Brandone2014], suggesting that speakers who use generics can exaggerate evidence to their listeners.

How can generics have such flexible truth conditions while simultaneously carrying strong implications?
In this paper, we explore the idea that the core meaning of a generic statement is simple, but underspecified, and that general principles of communication may be used to resolve precise meaning in context. 
In particular, we develop a mathematical model that describes pragmatic reasoning about the degree of prevalence required to assert the generic.  
We find that this formalism resolves the philosophical and empirical puzzles.

In Section 1, we review extant theories of generics from the linguistic and psychological literatures.
In Section 2, we describe our pragmatic theory of generic language in formal terms and illustrate the model through simulation. 
In Section 3, we conduct a set of experiments to test the \emph{truth conditions of generic statements}, and find our model predicts the patterns in human endorsement of familiar generic sentences.
We also find that the prior beliefs used in the Bayesian model reflect conceptual structure, and this provides an understanding of the conceptual implications of generics.
In Section 4, we conduct a set of experiments to test \emph{interpretations of novel generic sentences}, again finding our model predicts human judgments with high quantitative accuracy.
In Section 5, we investigate the underlying semantic scale in more detail, and find that the core meaning of generic language depends in a crucial way on subjective beliefs, not mere frequency.


# The Philosophical Puzzle of Generic Language: Truth Conditions

Generics express a relation between a kind K (e.g., \textsc{robins}) and a property F (e.g., \textsc{lays eggs}), such that the property can also be said to be applicable of an individual (i.e., the bird in my backyard lays eggs).
Bare plural statements (e.g., \emph{Robins lay eggs}) tend strongly to yield a generic meaning [@Carlson1977], though other forms can express such a meaning sometimes (e.g., \emph{A mongoose eats snakes.}).

Given that generics express a property that can be applied to individuals, it would seem intuitive that the number of individuals with the property would be what makes the statement true or false.
Counter-examples like \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} v. \emph{Birds are female} stifle such intuition. 
Semantic theories that appeal to the statistics of the world (i.e., how many Ks have F) try to rescue the notion that a generic expresses something like \emph{Ks, in general, have F}, where \emph{in general} is often either restricted to particular domains or is calculated with particular distortions.
Alternative, \emph{conceptually}-based theories take a generic to mean \emph{Ks, in virtue of being the kind of things that Ks are, have F}.
These accounts appeal to structured, conceptual representations in deciding what kinds of generic statements are true or false.
Statistical and conceptual theories express the major contrasting views of the truth conditions of generic statements [@Carlson1995essay].^[We use the terms statistical and conceptual to refer to what @Carlson1995essay referred to as "inductive" and "rules and regulations" views, respectively.]


## Statistical Accounts of Generics

Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}. 
Of course, birds do not in general lay eggs (it's only the adult, female ones that do).
The primary way of dealing with such issues is to posit domain restrictions ("implicitly, we are only talking about the females") when there are "salient partitions" [@Carlson1995].
%Not all accounts of generic language reduce to puzzles about mental representation of kind--property relations.
%Accounts stemming from formal semantics tend to look towards precise, quantifiable conditions to determine if a sentence is true or false.
The most fully-developed theory on this front is due to @Cohen1999. 
Let's first introduce some notation:

For a given kind $K$ (e.g.~\textsc{robins}) and property $F$ (e.g.~\textsc{lays eggs}), we refer to the probability that an instance of kind $K$ has property $F$, that is $P(F\mid K)$, as the \emph{prevalence} of $F$ within $K$.
Logical quantifiers can be described as conditions on prevalence (i.e.~\emph{some} is $P(F\mid K)>0$, \emph{all} is $P(F\mid K)=1$). 
Assuming the generic relates to the property prevalence, then the simplest meaning would similarly be a threshold on prevalence: $P(F\mid K)>\theta$. @Cohen1999 takes $\theta = 0.5$; that is, \emph{Robins lay eggs} is roughly taken to mean \emph{More than half of (relevant) robins lay eggs}. 

Cohen introduces constraints into the computation of prevalence: $P(F\mid K)$. In particular, prevalence is computed with respect to a \emph{partition set}. 
For example, the property \emph{lays eggs} induces a set of alternatives that all have to do with procreation (e.g. \emph{gives birth to live young}, \emph{undergoes mitosis}, ...).
The individuals that enter into the prevalence computation are only those individuals that would satisfy one or another alternative. 
That is, the only individuals under consideration are the female members of kinds because only the female members can plausibly satisfy one or another of the alternatives.
The inference that this sort of domain restriction is applied, as well as other constraints posited in the theory, are thought to be a pragmatic inference, though specifying the details of the pragmatics is beyond the scope of Cohen's theory.
Without a well specified theory of pragmatics, however, we have no general solution to map from property prevalence to truth judgment.
Conceptual information seems to be behind the pragmatic inference, but how Cohen's and other prevalence-based accounts relate to such knowledge remains obscure [@Carlson1995essay].
^[However, see @Cohen2004 for a discussion of how his semantic constraints relate to different kinds of generics and different kinds of conceptual representational frameworks found in cognitive science.]

## Conceptual Accounts of Generics

Conceptual accounts of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as the way of expressing special mental relationships between kinds and properties [@Leslie2008; @Prasada2012].
From this perspective, \emph{Bishops move diagonally} because those are the rules of the game, not because they \emph{tend to} move diagonally.
How do we come to know the rules of the game, though?

@Leslie2007's influential theory posits that generics are tied to a "default mode of generalization". 
This "default mode" comes equipped with the ability to single-out \emph{striking properties} (e.g. properties which are dangerous or appalling) as particularly useful aspects of the world to know about. 
Hence, \emph{Mosquitos carry malaria} is true because carrying malaria is striking, and thus, a useful bit of information to convey.
The default mode can also distinguish "negative" counter-instances of a property   (e.g. a bird that doesn't lay eggs) from "positive" counter-instances (e.g. a hypothetical bird that bears live young).
Generics are much less reasonable when positive counter-instances exist. Hence, \emph{Birds are female} seems weird because "being male" is a positive counter-instance of "being female", but since there are no birds that bear live young,  \emph{Birds lay eggs} is fine.
%For properties that have no principled relationship to the kind (e.g., in \emph{Barns are red}), conceptual accounts fall back to statistical information: \emph{Ks, in general, have F}.


Parallel work in the psychology of concepts supports this perspective.
@Prasada2006 and later @Prasada2013 distinguish between \emph{principled}, \emph{statistical}, and \emph{causal} relations within concepts. 
Generics like \emph{Birds lay eggs} (in which only a minority of K have F) exhibit characteristics of principled connections (operationalized by endorsement of the phrase \emph{In general, Ks have F}), supporting @Leslie2007's typology.
Striking generics (e.g. \emph{Mosquitos carry malaria}) show characteristics of \emph{causal connections} (operationalized using the phrase \emph{There is something about Ks that cause them to F}). 
The fact that generics about different properties license different kinds of inferences is taken as evidence that the generics themselves represent different kinds of relations. Statistical information takes a backseat to the conceptual structure.


## The Pragmatic Perspective

We find both the statistical and conceptual accounts compelling, but an important perspective is missing.
We note that generic language is not unique in its flexibility.
Understanding language in general depends upon assumptions interlocutors make about each other and what content is under discussion. 
The pragmatic lens reveals that utterances carry a mosaic of interpretations with a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000]. 
Can the puzzles of generic language be understood as effects of pragmatic reasoning?
If so, we may be able to get away with a relatively simple, statistical,  semantic theory.
Abstract mental representations, then, may be the conceptual backdrop against which generic language is interpreted. 

We pursue such a line of inquiry, assuming the simplest truth-conditional meaning: a threshold on prevalence $P(F\mid K)>\theta$ [c.f., @Cohen1999].
No fixed value of the threshold, $\theta$, would allow for the extreme flexibility generics exhibit (e.g. \emph{Mosquitos carry malaria}; \emph{Robins lay eggs} v. \emph{Robins are female}), % simultaneously with the interpretations of some novel generics being strong. 
so instead we allow this threshold to be established in context through pragmatic reasoning. % and is not a fixed property of the language.
Such an inference would depend on background knowledge about properties and categories---potentially structured, conceptual knowledge.
This inference, nonetheless, is a general mechanism of understanding language, not specific to interpreting generic statements.
We formalize this hypothesis in the Rational Speech Act (RSA) theory---a formal, probabilistic theory of language understanding.
RSA is derived from social reasoning, formalized as recursive Bayesian inference between speaker and listener [@Frank2012; @Goodman2013; @Goodman2016; see also, @Franke2009; @Franke2015].
@Goodman2016 provides a good introduction to the RSA framework and Appendix A presents a brief tutorial on RSA for the reader unfamiliar.

We follow the treatment of RSA applied to vague adjectives (e.g.~\emph{tall}), where a semantic variable is left underspecified [@Lassiter2013; @Lassiter2015].
We model the interpretation of generic statements by a pragmatic listener ($L_1$), concerned with learning the prevalence of a certain property in a certain category, $x=P(F \mid K)$.
She resolves the likely prevalence $x \in [0, 1]$ upon hearing a generic utterance $u$, by integrating her prior beliefs about $x$ with the generative process of the utterance -- an informative speaker ($S_1$).
\begin{eqnarray}
L_{1}(x , \theta \mid u) &\propto& S_{1}(u \mid x, \theta) \cdot P(x) \cdot P(\theta) \label{eq:L1}
\end{eqnarray}
In addition to not knowing $x$ \emph{a priori}, the listener does not know the value of the prevalence threshold, $\theta$, used to establish the meaning of the generic (as described below). Because the speaker's choice of utterance can depend on the threshold, the listener must reason about the threshold as well, based on her prior distribution over possible thresholds $P(\theta)$.
In principle, thresholds could be learned over time for different contexts, but here we assume the listener has no informative knowledge about the semantic variable: $\theta \sim \text{Uniform([0, 1])}$.

To resolve an appropriate meaning, the listener uses her background knowledge about the property $P(x)$ and her intuitive theory of a speaker $S_1$ as a rational actor who has the goal of providing information to a hypothetical literal listener ($L_0$):
\begin{eqnarray}
S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln  ( {L_{0}(x \mid u, \theta)} ) ) }\label{eq:S1}
\end{eqnarray}
The listener believes the speaker $S_1$ knows the threshold $\theta$.
That is, the speaker had a particular meaning in mind, and the listener can use pragmatic reasoning to help resolve that meaning [@Lassiter2013; @Lassiter2015; @GoodmanLassiter]. 

The proportionality in Eq.~\ref{eq:S1} implies normalization over a set of alternative utterances.
We consider the simplest set of alternatives, including only the production of a generic utterance and the possibility of staying quiet (and we assume no difference in production cost between these alternatives). 
We adopt the simplest meaning of a generic utterance in terms of prevalence, a threshold function: $\denote{\text{K F}}(x, \theta)=x>\theta$.
The silent or \emph{null} utterance alternative carries no information (i.e.~$\denote{null}(x, \theta)=T$) and produces a posterior distribution in the listener identical with the prior.^[
This alternative can be realized in at least two other ways: the speaker could have said the negation of the utterance (i.e. \emph{It is not the case that Ks have F.}) or the negative generic (i.e. \emph{Ks do not have F.}). All results reported are similar for these two alternatives, and we use the alternative of the \emph{null} utterance for simplicity.
]
The \emph{null} utterance alternative gives the generic its communicative force; the option of staying silent makes the generic utterance into a \emph{speech-act}.

\begin{eqnarray}
L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0}
\end{eqnarray}

The pragmatic listener $L_1$ (Eq.~\ref{eq:L1}) is a model of generic interpretation: Upon hearing a generic, what prevalence is a listener likely to infer?
This model assumes a threshold semantics, but doesn't require us to specify the threshold \emph{a priori}: It is inferred with pragmatic reasoning.
We can now imagine a speaker who reasons about this kind of listener:

\begin{equation} 
S_{2}(u \mid x) \propto  \exp{(\lambda_2 \cdot \ln \int_{\theta} L_{1}(x , \theta \mid u) \diff \theta ) }% =  P_{L_{1}}(x \mid u)
\label{eq:S2}
\end{equation}

This speaker considers the thought-processes of a listener who understands generics ($L_1$, Eq.~\ref{eq:L1}) and decides if the generic is a good (albeit, vague) way to describe the  prevalence $x$. 
$S_2$'s decision (like the simpler $S_1$'s decision) is with respect to the alternative of saying nothing: He will choose to produce the generic when the true prevalence $x$ is more likely under $L_1$'s posterior than under her prior. 
Critically, in contrast to the hypothetical speaker $S_1$, speaker $S_{2}$ doesn't actually know what the generic means (i.e. doesn't have access to the threshold $\theta$), but knows that it will be inferred by $L_{1}$, and integrates over the likely values she'll consider.
In line with the Rational Speech Act theory, we assume this speaker is a soft-max optimal speaker, with degree of optimality governed by parameter $\lambda_2$.
$S_2$ is thus a mapping from prevalence $x$ to the probability of producing a generic utterance. 
Since the only alternative to producing the generic is to stay silent, $S_2$ is interpreted as a model of endorsement, felicity, or truth judgments [@Degen2014].
We implemented this model in the probabilisitic programming language WebPPL [@dippl], and a fully specified version of the model can be found at \url{http://forestdb.org/models/generics.html}


This model makes the prediction that the interpretation of a generic statement (and hence, it's corresponding truth judgment) depends on the prior on prevalence $P(x)$.
We can explore the model by running simulations of $L_1$ and $S_2$ under various schematic priors.
$P(x)$ is a distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across animal categories. 
%Thus, a sample from this prior distribution would be an animal category (e.g., \textsc{ducks}), and the corresponding property prevalence for that category (e.g., 50\%).
In Figure \ref{fig:schematic-unif}, are $L_1$ (Eq.~\ref{eq:L1}) posterior prevalence distributions on $x$ (red solid line) for several example prior prevalence distributions $P(x)$ (blue dashed line), as well as the $S_2$ generic endorsement probability for different levels of prevalence.
We name these example prior distributions to suggest properties that might be associated with such priors. 
(Later, we will empirically measure these priors for properties of interest.)
First, we explore generic interpretation and endorsement for priors of three different shapes (left column). 
For each of these, the $L_1$ posterior distribution (red solid) over prevalence is heavily driven by the prior (blue dashed).
$S_2$ endorsement probabilities for the generic (black solid) increase as a function of prevalence, and what counts as "true" (in terms of the prevalence) depends on the prior. 

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/schematics_s2.pdf}
    \caption{Schematic prior distributions for prevalence $x$ (blue dashed), the pragmatic listener $L_1$ model's posterior distribution over prevalence upon hearing a generic utterance (red), and speaker $S_2$ model's endorsement of a generic utterance for different levels of prevalence (black).
    The names given to these priors are meant to be suggestive of what kinds of properties these distributions might correspond to.
    The left column uses prevalence priors modeled as Beta(15,15), Beta(4,1), and Beta(4,16) distributions.
      The right column uses a prior distribution that is a mixture of the distribution to the left of it with a second component, modeled as Beta(0.5, 4.5), reflecting categories with 0\% property prevalence.
    Horizontal dashed line at 0.5 is for convenience of comparing the point at which an utterance becomes judged as more true than false for $S_2$.
	Note that the prior distribution over prevalence will be the same as $L_1$'s posterior distribution upon hearing the "null" utterance.
    }
  \label{fig:schematic-unif}
\end{figure}


To take one example, consider the distribution over what might be the property "are female" (top-left facet).
The \emph{a priori} prevalence is centered around 0.5.
Because of pragmatics, the pressure to be \emph{truthful} will drive likely threshold values below 0.5 (lower values are more likely to be true). 
%At the same time, the maximum posterior density of the threshold is substantially greater than the lower bound. 
At the same time, the pressure to be \emph{informative} will drive the threshold values up: Higher values are more likely to be informative. 
The result is a posterior over prevalence that is only marginally greater than the prior, making higher prevalence values more likely after hearing the generic. 
But the relative information gain is very little (the posterior is not very different from the prior), and thus the $S_2$ model is reluctant to endorse the generic unless the property is exceedingly prevalent. 
The same basic phenomenon can be observed for the other two example priors (left column): The posterior over prevalence heavily depends upon the prior, but is also not very different from the prior. %the quantitative details of this inference are heavily dependent on the shape of the prior distribution (i.e. the posteriors look dramatically different).

We now imagine what would happen if there were some kinds where the property was completely absent, while being present at some rate in other kinds.
Figure \ref{fig:schematic-unif} (right column) shows this possibility: mixing the distribution to the left of it with a second component at 0\% prevalence.
Consider the schematic prior over "lay eggs".
We see the pragmatic listener $L_1$'s posterior prevalence distribution is not very different from the interpretation that doesn't include the second component in the prior (here, "are female"), but it is dramatically different from the prior with two components (compare red with blue dashed line for the right column).
This suggests that when many categories have 0\% prevalence, lower thresholds are informative. 
Indeed, the $S_2$ model predicts that the generic becomes felicitous at lower prevalence levels (compare black line of left v. right column).
For "lay eggs", when the property is prevalent in 50\% of the kind (e.g., 50\% of birds lay eggs), endorsement of the generic (e.g., \emph{Birds lay eggs}) by the $S_2$ is roughly 0.85; for "are female", with the same prevalence (50\% of birds are female), endorsement for \emph{Birds are female} is only 0.50: It is judged to be neither true nor false. 
The important result is the asymmetry: the first generic can be endorsed more strongly than the second, at the same prevalence level; the exact endorsement rates depend on quantitative aspects of the priors, which must be determined empirically.

The model thus predicts differences in truth judgments depending on the prevalence prior. 
The first test of our theory, then, will be to see if these predictions correspond to human truth judgments of familiar generic sentences. 


# Empirical Test 1: Flexible Truth Conditions

Any theory of generic language must explain their puzzling flexibility of usage with respect to prevalence.
That is, \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} are reasonable things to say, but \emph{Birds are female} is not.
The pragmatic speaker model $S_2$, Eq.~\ref{eq:S2}, is a model of truth judgments. 
We test our model on thirty generic sentences 
that cover a range of conceptual distinctions discussed in the literature  [@Prasada2013]: characteristic (e.g. \emph{Ducks have wings.}), minority (e.g. \emph{Robins lay eggs.}), striking (e.g. \emph{Mosquitos carry malaria.}), false generalization (e.g. \emph{Robins are female.}), and false (e.g. \emph{Lions lay eggs.}).
In additional to the canonical cases from the linguistics literature, we selected sentences to elicit the full range of acceptability judgments (intuitively, "acceptable", "unacceptable", and "uncertain") with low-, medium-, and high-prevalence properties. 

The pragmatic speaker model $S_2$ is fully-specified except for the prior distribution over prevalence $P(x)$, which plausibly varies by the type of property in question.
To compare the model to empirical truth judgments, we thus first measure the prior distribution over prevalence of these properties (Expt.~1a).
In Expt.~1b, we collect human judgements about the acceptability of the generic sentences. 

## Experiment 1a: Prevalence Priors

The prior $P(x)$ (in Eqs.~\ref{eq:L1}, \ref{eq:L0}) describes the belief distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across relevant categories. 
In exploring the model, we saw that the shape of this distribution affects model predictions, and this shape may vary significantly among different properties.
We thus measured this distribution empirically for the set of properties (e.g. \textsc{lays eggs, carries malaria}; 21 in total) used in our target sentences. 
 
### Method

#### Participants

We recruited 60 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
3 participants where unintentionally allowed to do the experiment for a second time; we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
In order to alleviate the dependence of the distribution on our animal categories of interest, participants generated half of the animal categories before viewing the properties; the other half were randomly sampled from a set corresponding to the generic sentences used in Expt. 1b (e.g. \textsc{robins, mosquitos}).

Participants began the experiment by seeing a list of 6 animal kinds and were asked to list 5 of their own.
A column then appeared to the right of the animal names with a property in the header (e.g. "lays eggs").
Participants were asked to fill in each row with the percentage of members of each of the species that had the property (e.g. "50\%").
Eight property--columns appeared in the table, and this whole procedure was repeated 2 times.
In total, each participant generated 10 animal names and reported on the prevalence of sixteen properties for 22 animals (their own 10 and the experimentally-supplied 12). 
Properties were randomly sampled from a set of 21 properties associated with generics of theoretical interest, as described above.
% motivated by conceptual distinctions pointed out by @Prasada2013. 
%The conceptual categories used to generate the properties (and their corresponding generics in Expt.~1b) were: majority characteristic (e.g. \emph{have spots}), minority characteristic (e.g. \emph{have manes}), striking (e.g. \emph{attack swimmers.}), and majority false generalizations (e.g. \emph{are female.}) [@Prasada2013].
For a full list of the properties, and generic sentences used in Expt.~1b, see Table 2 (Appendix).
The experiment can be viewed at \url{http://stanford.edu/~mtessler/experiments/generics/experiments/real-kinds/prior-2.html}.

### Data analysis and model predictions

To process the priors data, we discretize the prevalence judgments to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$, and look at the counts within each bin, after doing add-1 Laplace smoothing, as the relative probability of that prevalence. 
Using these priors, we can explore how $L_{1}(x , \theta \mid u)$, the pragmatic listener model, interprets a generic utterance (Figure \ref{fig:commongenerics}a, insets). 
The prior beliefs over the prevalence of the property, $P(x)$, can also be interpreted as the pragmatic listener's posterior upon hearing the null utterance, because the null utterance has no information content.
We see the interpretation of the generic is quite variable across our empirically measured priors.
For instance, in the case of \textsc{carries malaria}, the prior is very left-skewed; here, the threshold $\theta$ can plausibly be quite low while still being informative, since a low threshold still rules out many possible alternative kinds (and their corresponding degree of prevalence).
Properties like \textsc{doesn't attack swimmers} are very right-skewed; here, even a relatively high threshold would not result in an informative utterance (intuitively, as not many kinds would be ruled out), and so the generic is unlikely to be used by speaker $S_2$ unless the property is practically-universal within the target category. 
Some properties have priors that are unimodal with low variance (e.g. \textsc{is female}); these properties are present in every kind in almost exactly the same proportion and thus are too obvious and certain to allow for an informative generic utterance: The posterior is not very different from the prior. 
With $P(x)$ now empirically established, we can test if our speaker model predicts human truth judgments of generic statements about these properties.

## Experiment 1b: Truth Judgments

### Method

#### Participants

We recruited 100 participants over MTurk. 
%We chose this number of participants based on intuition from similar experiments which were designed primarily to test a quantitative model.
%Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating. 
4 participants were excluded for failing to pass a catch trial.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

Participants were shown thirty generic sentences in bare plural form, one after another.
They were asked to press one of two buttons (randomized between-participants) to signify whether they agreed or disagreed with the sentence (see Table 2 in Appendix for complete list). 
The thirty sentences were presented in a random order between participants and covered a range of conceptual categories described above.
Approximately 10 true, 10 false, and 10 uncertain truth-value generics were selected.

As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
4 participants were excluded for failing this trial.

#### Data analysis and results

As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
This was a significant predictor of the empirical truth judgments: true generics were significantly more likely to be agreed with than the indeterminate generics ($\beta = 3.14; SE = 0.15; z = 21.5$), as revealed by a mixed-effect logistic regression with random by-participant effects of intercept.
Indeterminate generics were agreed with \emph{less} likely than chance ($\beta = -0.49; SE = 0.09; z = -5.3$) but significantly more than false generics ($\beta = 2.09; SE = 0.14; z = 14.5$).

From the prevalence prior data (Expt.~1a), we estimate participants' beliefs about the prevalence of a property \emph{for a given kind} (e.g.~the percentage of \textsc{robins} that \textsc{lay eggs}; see green intervals on Figure \ref{fig:commongenerics}a insets, and Table 2 in Appendix).
As a simple baseline hypothesis, we first explore whether these prevalence values themselves predict generic endorsement (e.g.~does the fraction of \textsc{robins} that \textsc{lay eggs} predict the felicity of \emph{Robins lay eggs}?).
We find a little over half of the variance in truth judgments data is explained this way ($r^2 = 0.599$; MSE=0.065; Figure \ref{fig:commongenerics}b, right). 
This is not surprising given that our stimulus set included generics that are true with high-prevalence properties (e.g. \emph{Leopards have spots.}) and  generics that are false with low prevalence properties (e.g. \emph{Leopards have wings.}). 
However, large deviations from an account based purely on target-category prevalence remain: Generics in which the target-category has intermediate prevalence (prevalence quartiles 2 and 3: $ 20\% < prevalence < 64\%$), are not at all explained by prevalence within those categories ($r_{Q2,3}^2 = 0.029$; MSE = 0.110).

The pragmatic speaker model, $S_2$ in Eq.~\ref{eq:S2}, predicts an endorsement probability for a generic sentence, given prior beliefs about the property, $P(x)$, and a prevalence-level,  $x$, within the kind-of-interest. (That is, $S_2$ provides a model of whether someone who knows $x$ would say the generic to someone who doesn't, but shares prior $P(x)$; as discussed above we adopt this as a model of agreement judgments.)  
We use the empirically estimated within-kind prevalence as the $x$ that the speaker $S_2$ is trying to communicate, and use the empirically measured priors from Expt.~1a as the listener's prior $P(x)$. 
The $S_2$ model is then fully specified after setting the speaker optimality parameters $\lambda_1$ and $\lambda_2$ (in Eqs.~\ref{eq:S1}, \ref{eq:S2}).
%In order to get estimates of the $S_2$ model's predictive probability for endorsing generic sentences, we performed a bootstrap procedure on the empirical prior data. For each sample of the bootstrapped prior, we performed a Bayesian data analysis to infer the likely values of the model parameters. 
Rather than fitting the parameters, we inferred them and integrated over their plausible values using Bayesian data analysis [@LW2014]. We put uninformative priors over these parameters, with a range consistent with previous literature using the same model class: $\lambda_1 \sim \text{Uniform}(0,20)$, $\lambda_2 \sim \text{Uniform}(0,5)$.
We learn about the \emph{a posteriori} credible values of our model parameters by collecting samples from MCMC chains of 10,000 iterations removing the first 5,000 iterations, using the Metropolis-Hastings algorithm. 

The above analysis provides a single estimate for model predictions, but it is based on noisy empirical measurements of $P(x)$. In order to estimate the impact of this empirical noise on our model predictions, we resampled the prior data (with replacement) for 57 participants worth of data, discretizing and binning as we did above and then inferring parameters.
This procedure (re-sample prior, discretize and bin, infer parameters) was repeated 500 times to bootstrap the model predictions.
The Maximum A-Posteriori (MAP) estimate and 95\% Highest Probability Density (HPD) interval for $\lambda_1$ is 0.5 [0.004, 12.7] and $\lambda_2$ is 1.7 [1.3, 2.1].
^[
The fact that $\lambda_1$ is credibly less than 1 suggests that the generic utterance may be more costly than "staying silent" (our model assumes equal cost).
We maintain using only the two $\lambda$ parameters in the model for simplicity.
]

We compare the model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
(The posterior predictive distribution marginalizes over the inferred parameter values to produce predictions about what the data \emph{should look like} given the pragmatics model and the observed data. 
This is akin to fitting the parameters and is the critical step in model validation: It shows what data is actually predicted by the model.) 
As we see in Figure \ref{fig:commongenerics}b, the pragmatic speaker model $S_2$, using empirically measured priors, explains nearly all of the variance in human truth judgments ($r^2=0.98$; MSE=0.003; Figure \ref{fig:commongenerics}b). 

Generics that received definitive agreement or disagreement are predicted to be judged as such by the model (corners of Figure \ref{fig:commongenerics}b), including items for which target-category prevalence is not a good indicator of the acceptability (e.g. \emph{Mosquitos carry malaria}, for prevalence quartiles 2 and 3, $r_{Q2,3}^2=0.955$; MSE=0.005; Figure \ref{fig:commongenerics}b, intermediate shades).
We also see the generics truth judgment model predicts uncertain truth judgments: for instance, \emph{Robins are female} is judged by both the model and human participants to be neither true nor false.
\emph{Sharks don't attack swimmers}, while true of most sharks, is judged to be not a good thing to say by both participants and the model.
This is strong evidence that the puzzling flexibility of generic truth-conditions can be understood with a simple semantic theory coupled with basic communicative principles (\emph{be truthful}, \emph{be informative}) operating over diverse prior beliefs about the properties, all of which are at play in understanding language. 

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/generics-prior-prevalence-tj.pdf}
    \caption{Endorsing familiar generics. (a) 
    Prevalence prior distributions empirically elicited for twenty-one animal properties.
    Prior distributions summarized by two parameters of a structured Bayesian model: $\phi$----a property's potential to be present in a category----and $\gamma$----the mean prevalence when it is possible for the property to be present in a category.
    Inset plots display example empirical prior distributions over prevalence and corresponding $L_1$ model predictions: the posterior after hearing a generic utterance. 
    Intervals on the top of insets show human judgments about the prevalence of the property within a target category.
    (b)
    Human acceptability judgments compared with model predictions (left) and the target-category prevalence (right) for thirty generic utterances about familiar animals and properties. 
    Color denotes target-category prevalence of the property, with lighter colors indicating higher prevalence. 
     Error bars denote 95\% Bayesian credible intervals.
%    Error bars correspond with 95\% bootstrapped confidence intervals for the participant data and 95\% highest probability intervals for the model predictions (same for Figures \ref{fig:impliedByItem} and \ref{fig:exp2b}).
    }
  \label{fig:commongenerics}

\end{figure}



## Extended Analysis of Priors: Conceptual Structure 

The pragmatics model only has a prevalence-based semantics yet it is able to explain the flexibility in truth judgments for a diverse range of generic statements.
Conceptual accounts of generic statements have looked beyond prevalence, to structured knowledge representations as the critical factor in generic meaning [@Leslie2007; @Prasada2013]. 
We can interrogate our formal model to see what is driving its predictions, and, in particular, we ask whether structured representations might effect our model after all.

For a given property,  the prior distribution on prevalence $P(x)$ is a single distribution.
However, the distribution may be structured as the result of deeper conceptual knowledge. 
For instance, if participants believe that some kinds have a causal mechanism that \emph{could} give rise to the property, while kinds others do not, then we would expect $P(x)$ to be structured as a mixture distribution [cf., @Griffiths2005].
We know that prior knowledge plays a fundamental role in leading the pragmatics model to endorse or reject generics. 
We now explore the hypothesis that this is at least partly because these priors are structured into two components: kinds that \emph{can} have the property and other kinds that \emph{cannot}.
We explore this possibility by formulating a mixture model for the prevalence priors, and exploring how well it fits the prior data elicited in Expt.~1a.

### Data analysis

If a kind can have the property, we assume the prevalence follows a Beta distribution with mean $\gamma$ and concentration $\xi$. 
If a kind cannot, we assume the prevalence is a Delta distribution, with all probability mass at 0\%. ^[There are other ways to formulate the second component ("the kind doesn't have a causal mechanism that would give rise to  the property") of the prior. 
It could reflect accidental causes of the property, in which case, the prevalence could be a distribution that allows for non-zero prevalence. 
While an interesting possibility, its full consideration is beyond the scope of this article.
]
The relative contribution of these two components is governed by mixture parameter $\phi$, inferred from the data.^[This is similar in spirit to Hurdle Models of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., adverse events to vaccines; @hurdleModels])]
We think of $\phi$ the \emph{potential of a property to be present in a kind} and $\gamma$ is the \emph{mean prevalence of the property among the kinds with the potential to have it}.\footnote{
We note that $\phi$ is not what other authors have described as \emph{cue validity} [@Beach1964; Khemlani2012], or $P(K \mid F)$. 
 $\phi$ is a mixture component in the prior distribution over prevalence: $P(F\mid K$). 
Cue validity and prevalence are related via Bayes Rule': $P(K \mid F) \propto P(F \mid K) \cdot P(K)$. 
%Due to this relationship, there will be a correlation between \emph{cue validity} and our model using prevalence priors.
%The details of the precise mathematical relationship between cue validity and our model are beyond the scope of this article.
}
If this model is correct, the prevalences given by participants would then be distributed as: $P(d) = \phi \cdot \text{Beta}(d \mid \gamma,\xi)+ (1 - \phi) \cdot \delta_{d=0}$. 

We performed Bayesian inference over this model, given the observed prevalence data, to examine how well the model's posterior predictive distribution reconstructs the prevalence prior data.
We put uninformative priors over all the parameters, $\phi \sim \text{Uniform}(0,1)$, 
$\gamma \sim \text{Uniform}(0,1)$, $\xi \sim \text{Uniform}(0, 50)$, 
and performed Bayesian inference separately for each property using the Metropolis-Hastings algorithm 
%to estimate the posterior distribution over the latent parameters $\phi, \gamma$, and $\xi$ 
collecting 50,000 samples removing the first 25,000 iterations for burn-in.

### Results

Estimates of the mixture parameter $\phi$ and the mean of the "has the potential" component $\gamma$ for each property are shown in Figure \ref{fig:commongenerics}a.
We see significant diversity among our properties in both parameters, corresponding to priors over prevalence with dramatically different shapes (insets). 

Again, we look to the posterior predictive distribution to validate the structured prior model.
Using the model with its inferred parameters, we generate prevalence judgments for different properties and compare that to the empirical counts. 
We discretize the prevalence values of both the model and the data to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$.
This statistical model reproduces the prior elicitation data very well ($r^2 = 0.94$), while a model that assumes just a single generative component fails ($r^2 = 0.14$). This is strong evidence in support of a structured prior. 

The other test of this hypothesis is to re-examine the truth judgments from Expt.~1b using the pragmatics model with the inferred structured priors (as opposed to bootstrapping the raw empirical counts). 
We find the same correspondence to the empirical truth judgments data ($r^2 = 0.98$).
This provides further evidence that the prior distribution over prevalence $P(x)$ is structured.
The implication of this finding is that conceptual structure may indeed find its way into generic judgments, but via the prevalence prior, rather than directly in the semantics of the generic. We return to this idea in the General Discussion.

\begin{figure}
\centering
    \includegraphics[width=0.6\columnwidth]{figs/postPred-priorModel.pdf}
    \caption{Posterior predictive distribution of the structured, statistical model thought to give rise to the human data in the prior elicitation task. The close alignment between model and data suggests the assumption of a structured prior is warranted.}
  \label{fig:pp-priorModel}
\end{figure}

# References 
