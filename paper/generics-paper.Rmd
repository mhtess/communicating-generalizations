---
title             : "Communicating generalizations about agents, objects, and events"
shorttitle        : "Communicating generalizations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}

author_note: >
  Enter author note here.

abstract: >
    Acquiring abstract knowledge requires making generalizations.
    Generalizations form the foundation for abstract thought. 
    Everyday generalizations typically rest not upon observational data, but upon testimony: Generalizations conveyed with language.
    The language of generalizations---expressions of *genericity* (e.g., *Dogs bark.*)---appears early in development and is ubiqutious in everyday conversation.
    However, sentences that convey generalizations are extremely flexible in their usage, while often conveying a strong inference: That the generalization should hold in the future. 
    These phenonena have confounded previous attempts to formalize the meaning of linguistic expressions of genericity.
    We formalize the idea that the meaning of such linguistic expressions is simple, but underspecified, and that general cognitive principles can be used to establish a more precise meaning in context.
    We test our computational model on endorsements of generalizations about events and show that our model explains the extreme flexibility in usage that no simpler model could account for. 
    We then manipulate participants' background knowledge about the domains and show that our the priors are causally related to their endorsements of generalizations.
    Finally, we show how our computational framework naturally accounts for a number of well-documented psychological phenomena concerning generalizations about categories, a special kind of genericity --- so called, *generic language*.
    Our model shows the central phenomena of genericity emerge from the interaction of diverse prior beliefs with communicative principles.
    This theory provides the formal bridge between the words we use and the generalizations they describe.
  
keywords          : "genericity, generalizations, generics, pragmatics, semantics, bayesian modeling"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
<!-- \newcommand{\ndg}[1]{\textcolor{Green}{$[ndg: #1 ]$}} -->
<!-- \newcommand{\mht}[1]{\textcolor{Blue}{$[mht: #1 ]$}} -->
<!-- \newcommand{\red}[1]{\textcolor{Red}{$#1$}} -->

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=F, message=FALSE, sanitize = T)

```


```{r include = FALSE}
library("papaja")
```


```{r}
library(rwebppl)
library(xtable)
library(tidyverse)
library(forcats)
```

# Introduction 
<!--
If we are starting with habituals and/or doing all 3 kinds of generalizations, what puzzles do you present in the intro?
- flexible truth
- all generalizations ? ( causuals, habituals, generics)
- strong interpretations ? (seems to be special to generics)
- asymmetry ? (only for generics)
- predictives (future frequency)
-->

<!-- Generalizations. Quantitative models. Communication / Cultural ratchet. RSA but  -->
<!-- only specific language. One case study is generics. Puzzles. -->

<!-- Quantitative model, different kinds of generalizations -->

Figuring out that an agent tends to do an action, a cause tends to produce an effect, or an object tends to have a property is crucial knowledge for a cognitive agent.
Such generalizations give rise to the productivity of human thought and form of foundations of cultural knowledge.
They allow us to understand the complex world around us and dream up how possible future situations might unfold.
Crucially, hard-won generalizations can pass between agents. 
Humans communicate generalizations to each other, something which other animals struggle to accomplish.
Generalizable knowledge can be picked up in pedagogical contexts when ostensive cues are present [@Csibra]. By and large, however, language provides the most flexible and generative way to communicate generalizations. 
A generalization gives us a relation between a category and a property: It tells us to predict that property of future instances of the category [@HumeTHN].
Thus, generalization can be described as a probability and inductive inference as a probabilistic model [@Tenenbaum2011].
From a very young age, children make strong inductive inferences from their experience in a way consistent with the Bayesian probability calculus [@Dewar2010; @Gelman1986; @Markman1989].

If generalizations can be described by predictive probabilities, it would stand to reason that the most direct way to communicate a generalization would be to convey probability. 
Ironically, humans are very bad at describing and interpreting probabilities directly [@Tversky1974].
Rather than being a deficit, however, the difficulty we have in understanding numerical descriptions of probability might reflect the flexible adaptivity of language to make vague or ambiguous that which shared context can be relied upon to guide interpretation [@Piantadosi2012].
If language has evolved in a way to make efficient the kinds of inductive inferences that are central to human cognition, we would expect the language of generalizations to be simple [c.f., @Leslie2008].

Indeed, generalizations in language (e.g., *Dogs are friendly*, *Mary smokes cigarettes*, *Yeast makes bread rise*) are among the simplest forms of compositional expressions. 
Generalizations do not come in just one syntactic form: "*Dogs* are friendly", "*A library book* should be treated with respect", "*The contemporary foodie* seeks culinary experinces in menu-less restaurants and grandmothers' kitchens" [@Carlson1995; @NickelBlackwell; @sepGenerics].
Each of these statements has in common the fact that they predicate a property of a category, but the properties themselves apply to instances of the category (unlike, e.g., *Dinosaurs are extinct*). 
In this way, these statements exhibit *genericity*; they support generalization to new, unobserved, concrete instances.

In this paper, we explore the language of generalizations: How speakers convey generalizations and how listeners interpret them. 
We describe computational models based on a state-of-the-art language interpretation framework --- the Rational Speech Act theory [@Frank2012; @Goodman2013; @Goodman2016].
Computational modeling has deepened our scientific understanding of complex linguistic phenomena such as politeness [@Yoon2016], hyperbole [@Kao2014], and vagueness [@Lassiter2013]. 
Until now, however, rational models of communication have focused on statements about *specific* situations: utterances that convey information about a particular entity or event. 
A formal theory of statements that convey generalizations about entities or events --- statements that extend beyond the here-and-now --- has heretofore been unexplored.

We examine three case studies of generalizations in language: Generalizations about agents, objects, and events. 
In Section 1, we introduce an information-theoric communicative theory of how generalizations are conveyed in language and flesh out the model's predictions alongside alternative models.
In order to do so, we provide a unified mathematical framework for dealing with agents, objects, and events, casting each as a category over which generalization can occur.
Our model predicts the endorsements of generalizations should vary with the (A) the probability of an instances having the property and (B) the prior distribution over the property probability (e.g., how likely instances of other categories are to have the property).
In Section 2, we introduce our empirical paradigm which we use to measure endorsement and interpretation of linguisitic generalizations, examining generalizations about *agents* in what is known as *habitual language* (e.g., "Mary smokes cigarettes."). 
In Section 3, we extend our paradigm to generalizations about *events* (i.e., *causal language* e.g., "Yeast makes bread rise.") and experimentally manipulate participants' background knowledge, showing that prior beliefs about probabilities are causally related to endorsement and interpretation of generalizations.
In Section 4, we use our paradigm to address long-standing theoretical and empirical puzzles regarding generalizations about *objects*, so-called *generic language*. 
We show our how model explains three known psychological puzzles surrounding generic language. 
In all case studies, we provide formal comparison to simpler models and in all cases, find that only our communicative theory explains the patterns of empirical data.
We conclude with a general discussion about the relationship of our theory to extant theories in the psychology, linguistics, and philosophy of genericity, and highlight the implications for modeling language acquisition, conceptual development, and other avenues of research that this framework opens.


<!-- the truth conditions of generalizations of events (so called *habitual language*) by manipulating the target propensity (A) while surveying a wide variety of events (which should induce variability in B). -->
<!-- In Section 3, we manipulate both (A) and (B) while testing the model on generalizations about causes (*causal language*). -->
<!-- If our theory is truly a theory about communicating generalizations, it should be able to explain the known psychological puzzles surrounding *generic language* (generalizations about categories).  -->






<!-- In stark constrast, a computational understanding of communicating generalizations is not well understood at all.  -->

<!-- Gathering data and observing evidence, however, is costly.  -->
<!-- The human problem of induction even displays a complex sensitivity to not only the type of data, but how the data was generated [@Gweon2010; @Shafto2012]. -->
<!-- A learner cannot form all of her abstract knowledge through direct experience, however. -->
<!-- Life is only so long, and you can only be in one place at a time. -->



Empirical work done on category generalizations --- so called *generic language* --- has elucidated another troubling phenomenon.
<!-- One case study in the language of generalizations is about category generalizations. -->
<!-- Category generalizations, called *generic statement*, has been studied extensively in cognitive and developmental psychology: *generic* statements, or generalizations about categories. -->
Generic statements often imply the property is widespread in the category [e.g., *Swans have hollow bones* is a strong statement; much more so than *Some swans have hollow bones*; @Gelman2002; @Cimpian2010; @Brandone2014].
This characteristically strong interpretation interacts with the indeterminacy in truth conditions to exaggerate evidence. 
For a variety of features, both children and adults infer that a property is *more* prevalent when told the generalizations than when judging it true or not [@Cimpian2010; @Brandone2014].

<!-- These *strong implications* might underly stereotype formation in social categories [@Rhodes2012]. -->
<!-- In addition to the flexibility in truth conditions, two phenomena have been noted.  -->
<!-- The case of *Mosquitos carry malaria* suggests the utterance must in some way be analogous to the quantifier "some" (e.g., *Some mosquitos carry malaria*).  -->
<!-- Generic language, however, is often interpreted as implying the property is widespread: *Swans have hollow bones* means something different than *Some swans have hollow bones* [@Gelman2002]. -->

In this paper, we introduce a formal model for understanding the language of generalizations.
<!-- The fact that generalizations in language seem to convey something about the propensity but that there seem to be as many interpretations of generalized statements as there are generalizations have confounded formal models aimed to capture their meaning. -->
We take as a starting point the idea that the core meaning of a linguistic generalization is simple (i.e., it conveys something about the propensity), but underspecified (i.e., there is uncertainty in the meaning).
We extend a rational model of communication --- the Rational Speech Act theory --- to be able to understand general statements that extend beyond the here-and-now.
We provide a number of stringest tests of this theory to explain generalizations about categories, events, and causal forces. 


<!-- These three phenomena---flexible truth, strong interpretations, and evidence exaggeration---have confounded all formal models aimed to capture the meaning of generalizations in language. -->
<!-- Informal theories have argued that the nature of concepts and their relations are inextricably tied to their linguistic expression [@Leslie2007; @Prasada2013]. -->
<!-- Formal theories have limited themselves to a measurable quantitative degree (e.g., how prevalent a property is in a category), which, in isolation, is difficult to reconcile with the extreme flexibility of generic language. -->
<!-- Here, we present a theory formalizing the inuition that the core meaning of a linguistic generalization is simple, but underspecified, and that general principles of communication may be used to resolve the precise meaning in context.  -->
<!-- This theory inherently relies upon measurable quantities (i.e., prevalence) but situates it within a Bayesian agent with potentially structured prior knowledge, providing a bridge to conceptual theories.  -->
<!-- We find that this formalism (and not simpler ones) explains all three empirical puzzles. -->

<!-- When interpretations are compared directly against truth conditions (i.e., how prevalent a property would need to be for the generic to be true),  -->

<!-- Three empirical phenomena make generalizations in language difficult to formalize. -->
<!-- The conditions under which a generalizations becomes "true" are variable: -->


<!-- Existing work in both adults and children suggests that choosing to communicate a generalization can be used to exagerate evidence [@Cimpian2010; @Brandone2014]. -->

<!-- In addition to having manifold interpretations, what makes linguistic generalizations true or false (i.e., their truth conditions) is also a mystery. -->
<!-- The statement *Birds lay eggs* is true even though only about half of them do (namely, the females). -->
<!-- Half of birds are also female, and yet *Birds are female* sounds like a strange thing to say. -->
<!--  while *Canadians are right-handed* is also weird, even though the vast majority of them are.  -->

<!-- How can generalizations in language have such flexible truth conditions while simultaneously carrying intuitive and, at times, very strong implications? -->
<!-- This question has been investigated by psychologists, linguists, and philosophers since @Carlson1977. -->
<!-- Here, we provide a unified formal theory of generalizations conveyed in language. -->
<!-- We test theory on the preeminent case study of linguistic generalizations: generic language, or generalizations about categories. -->
<!--  -->
<!-- To be precise, we develop a computational model that describes pragmatic reasoning about the propensity required to assert the generaization.   -->
<!-- We find that this formalism resolves known philosophical and empirical puzzles. -->
<!--We also provide the first set of experimental results about linguistic generalizations about events and causal forces, so called *habitual* and *causal statements*.-->

<!-- and find extreme flexibility in usage, which our model predicts.  -->

<!-- In Section 2, we conduct a set of experiments to test the *truth conditions* of familiar generalizations about categories (generic statements), and find our model predicts key patterns in human judgments -->
<!-- In Section 3, we conduct a set of experiments to test *interpretations* of novel generic sentences, again finding our model predicts human judgments with high quantitative accuracy. -->
<!-- In Section 4, we begin to test the generality of the theory, examining the *truth conditions* of generalizations about events. -->
<!-- In Section 5, we further probe our theory's generality, investing the *truth conditions* and *interpretations* of novel generalizations about causal forces.  -->
<!-- In all cases, we provide formal comparison to simpler models and in all cases, find that only our communicative theory with an underspecified meaning function explains the pattern of human data. -->
<!-- We also find that the prior beliefs used in our model reflect conceptual structure, and this provides an understanding of the conceptual implications of generics. -->
<!-- Finally, we investigate the underlying semantic scale in more detail, and find that the core meaning of generalizations in language depends in a crucial way on subjective beliefs, not mere frequency. -->

<!--
Most would agree that \emph{Swans are white}, but certainly not every swan is.
This type of utterance conveys a generalization about a category (i.e. \textsc{swans}) and is known as a generic utterance [@Carlson1977; @Leslie2008].
Communicating generically about categories is useful because categories themselves are unobservable [@Markman1989].
Knowledge about categories, while central to human reasoning, is tricky to acquire because categories themselves are unobservable [Markman1989}.
It is believed that every language can express generic meaning [@Behrens2005; @Carlson1995], and that generics are essential to the growth of conceptual knowledge [@Gelman2004] and how kinds are represented in the mind [@Leslie2008].
Generic language is ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008], and children as young as two or three understand that generics refer to categories and support generalization [@Cimpian2008].
and though English and many other languages do not possess an unambiguous form devoted to generic meaning [@Behrens2000, @AlMalki2014].
Additionally, generics are the primary way by which speakers discuss social categories, making them key to propagating stereotypes [@GelmanEtAl2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010motivation].
Despite their psychological centrality, apparent simplicity, and ubiquity, a formal account of generic meaning remains elusive.

The major issue in formalizing generic language is determining what makes a generic sentence true or false.
At first glance, generics feel like universally-quantified statements as in \emph{All swans are white}. 
Unlike universals, however, generics are resilient to counter-examples (e.g., \emph{Swans are white} even though there exist black swans). 
Intuitions, then, fall back to something more vague like \emph{Swans, in general, are white} because indeed most swans are white.
But mosquitos, in general, do not carry malaria, yet everyone agrees \emph{Mosquitos carry malaria}.
Interpreting the generic as meaning "most" (i.e. \emph{Most swans are white}) captures many cases, but cannot explain why \emph{Robins lay eggs} and \emph{Mosquitos carry malaria} are so intuitively compelling: Only adult female robins lay eggs and a very tiny fraction of mosquitos actually carry malaria.
Indeed, it appears that any \emph{truth condition} stated in terms of how common the property is within the kind violates intuitions.
Consider the birds: for a bird, being female practically implies you will lay eggs (the properties are present in the same proportion), yet we say things like \emph{Birds lay eggs} and we do not say things like \emph{Birds are female}.



How can generics have such flexible truth conditions while simultaneously carrying strong implications?
In this paper, we explore the idea that the core meaning of a generic statement is simple, but underspecified, and that general principles of communication may be used to resolve precise meaning in context. 
In particular, we develop a mathematical model that describes pragmatic reasoning about the degree of prevalence required to assert the generic.  
We find that this formalism resolves the philosophical and empirical puzzles.
-->


# Formalizing Generalizations

Generalizations are used to make predictions about instances that an agent has yet to experience [@HumeTHN].
To form a generalization about a category or kind $k$, an observer must be able to individuate an exemplar or instance $x$ as belonging to the category.
The linguistic expression of such an individuation would manifest in utterances such as "$x$ is a $k$" or "this $k$ ..."
To generalize a feature $f$ to that class, it is also necessary to be able to determine if the particular instance has the feature.
That is, a speaker must be able to say "This $k$ (i.e., $x$) has $f$".
We call this kind of statement a *particular*.

```{r , results="asis"}
df.tab <- data.frame(
  Generalization = c("Dogs are friendly", "John smokes", "Drinking moonshine makes you go blind"),
  #x = c("an instance of K in space", "an instance of K in time", "an instance of a "),
  "Generalization over" = c("Object", "Agent", "Event"),
  "Linguistic construct" = c("Generic", "Habitual", "Causal"),
  x = c("a dog", "an instance of John", "an instance of a person drinking moonshine"),
  k = c("DOGS", "JOHN", "DRINKING MOONSHINE"),
  f = c("is friendly", "is smoking / smoked at time t", "caused person to go blind")
) #%>% xtable(., 
 #        caption = c("Decomposition of generalizations into concrete particulars and corresponding properties."),
  #       label = c("tab:genpart"))


apa_table(df.tab, 
          caption = "Decomposition of generalizations into concrete particulars and corresponding properties.", 
          escape = TRUE, small = TRUE
)

#print(df.tab,  type = "latex", tabular.environment = "tabularx", width = "\\textwidth",  include.rownames = FALSE, comment = F)
```

We can make particular statements about agents, objects, and events.
We can say things like "My dog is friendly.", "John ran this afternoon.", or "My friend drank moonshine and it made him blind."
To consider the corresponding generalizations (i.e., "Dogs are friendly", "John runs", "Drinking moonshines makes you go blind"), we must say exactly how the particular relates to the generalization.
We consider the generalization to be about a *category* and the particular to correspond to *instances* of that category.
Thus, *dog* is an instance of the category \textsc{dogs}, *John at a particular time* is an instance of the category of \textsc{john}, and *drinking moonshine at a particular time* is an instance of the category of \textsc{drinking moonshine} (see Table \ref{tabgenPart}).


Suppose we observe instances $\{x_1, x_2, ..., x_n\}$ of category $k$ and some number of them have feature $f$: What do we learn about possible future instances of $x_{n+1}$, particularly with respect to $f$?
For properties among instances of a category, we can describe an inductive inference by a probability $h$ $h = P(x \in f \mid x \in k)$^[
With a frequentist interpretation, this can be thought of as the *prevalence* of $f$ among $k$.
].
Here, $x$ is an object.
For a behavior exhibited by an agent, the inference concerns a rate --- how frequently (in time) does $x \in f$ occur --- which can be described as a probability integral: $h = \int_{t} P(x \in f_t \mid x \in k_t) \diff t$
Here, $x$ is a instance.
A Bayesian agent will have prior beliefs about the probability (potentially an uninformed prior) and uses Bayesian inference to learn about the probability from her observations.

<!-- Suppose *a priori* we have no informative beliefs about the propensity (i.e., $P(x \in f \mid x \in k) \sim \text{Uniform}(0, 1)$). -->
Observing some number of positive instances of $k$ with $f$ will update a prior belief distribution into a posterior belief distribution.
As a concrete example, consider the feature *is friendly* ($f$) for the category *dog* ($k$): $h = P(x \in \{\text{is friendly}\} \mid x \in \{\text{is a dog}\})$.
If an agent were to encounter some number of individual dogs, note how many of them were friendly ($d$), she could form a posterior belief distribution about the friendliness of dogs.

$$
P(h \mid d) = \frac{P(d \mid h) \cdot P(h)}{\int_{h'}P(d \mid h') \cdot P(h')}
$$

Our ability to use this updated belief $P(h \mid d)$ in order to make predictions about unobserved instances is a computational description of generalization [e.g., @Tenenbaum2006; @Tenenbaum2011].

The human capacity to generalize from observations has been studied extensively in cognitive and developmental psychology [@Nisbett1983; @Heit2000].
Inductive inferences show complex sensitivites to the kind of observations, and even how those observations came to be observed in the first place.
For instance, observing that an indigenous person living on a remote island *is obese* provides considerably less information about the weights of other islanders in comparison to inferences about skin color upon learning that one islander's skin color *is brown* [@Nisbett1983].
Confounded causal evidence can be suddenly become unconfounded when you know the evidence was generated by an intentional agent [@Goodman2009].
Quantitative modeling has been crucial in explaining how the subtleties of experience relate to subtleties in generalization [@Tenenbaum2006; @Kemp2008].

Studying generalizations from observational evidence can only take us so far in understanding how the mind understands the world or culture accumulates across generations.
Relying upon observations would be particularly problematic to learn, for example, about properties that are difficult to observe (e.g., *cows have four stomachs*) or events that are statistically unlikely (e.g., *lightning strikes tall objects*).
For abstract, generalizable knowledge to remain in a culture, it must be able to be faithfully transmitted between individuals and across generations [@Tomasello1999].


# Communicating Generalizations

To understand generalizations in language we must first understand language.
Understanding language depends upon assumptions interlocutors make about each other and can exhibit a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000].
Though once viewed as a wastebasket in which to dump unexplained loose ends in service of more rigorous formal analysis, *pragmatics* --- or language understanding in context --- has seen an emergence of formal frameworks that connect closely with behavioral data [@Franke2015].
One dominant framework --- the Rational Speech Act theory --- has had considerable success in providing a formal account of pragmatic language phenomena [i.e., interactions between language and context; @Goodman2016]
This framework has been used to explain, in precise mathematical terms, how it is that utterances like "some of the kids failed the test" are interpreted to mean "some (but not all) of the kids failed the test" [@Goodman2013], how "I waited a million years to get a table" is interpreted as "I waited an obnoxiously long time to get a table" [@Kao2014], how "John is tall" can mean John is 5'4" is John if a 12-year-old but means John is 6'2" if he is 24-years-old [@Lassiter2015], and how "Your talk was fine" can be a polite way of saying your talk was less than fine [@Yoon2016]. 
Computational modeling has pushed our understanding of these complex linguistic phenomena, but so far, rational models of language use have focused on *specific* statements: utterances that convey information about a particular entity or situation. 
A formal theory of statements that convey generalizations about entities or situations ---  statements that extend beyond the here-and-now --- has remained elusive.

Formalizing the meaning of linguistic expressions requires specifying *truth conditions*.
Using the tools of formal semantics, we need to specify a truth function: A function from states of the world to truth-values. 
This kind of machinery is used to formalize the intuition that utterances can truthfully apply to some possible worlds and would be false of others.
A simple meaning function for a generalization would be a threshold on an agent's posterior probability: $\denote{u_{generalization}}(h, \theta): \{ h: h > \theta\}$, where $\denote{u_{generalization}}$ represents the semantic meaning of a statement conveying a generalization.
Following convention in formal semantics, we treat this as a function, applied to variables $h$ (a probability or rate) and $\theta$ (a threshold).
This function returns the sets of worlds that are consistent with $h > \theta$.

This simple, threshold meaning for a generalization is problematic.
<!-- fixed value of $\theta$ seems unlikely. -->
The generalization *Mary smokes cigarettes* suggests a daily-habit, while *Bill writes poems* implies something much weaker, perhaps a poem every few weeks.
*Drinking moonshine makes you go blind* and so does *staring at the sun*, but neither causal statement tells you to precisely the increased relative risk.
*Mosquitos carry malaria* is a true statement despite the prevalence of carrying malaria among mosquitos being a just a fraction of 1\%, while *Ducks are female* might seem like a weird thing to say, even though the rate is much higher than the malaria-carrying mosquitos.
Finally, *Ducks lay eggs* is a good generalization, though the rate is the same as *Ducks are female*.
The meaning of generalizations, it seems, cannot reduce to a single, precise quantitative statement; this is the major stumbling block to providing a formal account of genericity [@Carlson1977; @Leslie2008; @NickelBlackwell].

We hypothesize that meaning of a lingusitic generalization can be captured by a simple, but underspecified, meaning. 
Concretely, we adopt a threshold semantics as described above, but imagine that $\theta$ is not a fixed property of language.
In this formulation, listeners infer thresholds along with speakers' intended meanings in context.
We formalize this hypothesis with a "literal listener" model from the Rational Speech Act framework [@Frank2012; @Goodman2013; @Goodman2016].

<!-- *Bills writes poems* might true if Bill does it a few times a year, while we might be hesitant to say *Mary smokes cigarettes* if she only does it once every few months. -->
<!-- *Ducks lay eggs* is true despite only the female ducks laying eggs, while *Ducks are female* is strange to say (even though only the females are female). -->

<!-- The language of generalizations is so simple that it appears everywhere in human conversation (*Movie theaters are cold.*), pedagogy (*The blicket makes the machine play music.*), political and scientific discourse (*The President peddles falsehoods.*, *Psychology experiments don’t replicate.*), stereotypes (*Boys are good at math*), motivation (*Big girls eat their broccoli!*), emotion-regulation  (*Accidents happen.*), child-directed (*Mommy works late*) and child-produced speech (*Slides aren't for grown-ups!*), and many other facets of experience [@GelmanEtAl2004; @Gelman2008; @Cimpian2010motivation; @Rhodes2012]. -->

<!-- Though the language of generalizations is found everywhere and in every language [@Behrens2005; @Carlson1995], the meaning of such linguistic expressions is philosophically puzzling and has resisted precise formalization. -->
<!-- Intuitively, generalizations should convey something about the probability of an instance of the category to have a property, but exactly what is conveyed is unclear. -->

<!-- What if we leave the $\theta$ underspecified in the semantics? -->
<!-- That is, what would a listener believe if she drew $\theta$ from an uninformed prior belief distribution: $P(\theta)$?^[ -->
<!-- Previous attempts have been made to salvage this threshold semantics, using a semantic a technique called "domain restriction", wherein the scope of generalization is restricted to a smaller subset of entities or instances [e.g., "only female ducks"; @Cohen1999].  -->
<!-- Though effective at dealing with at least some of these semantic puzzles, this approach leaves unresolved why the domain should be restricted in some cases and not others. -->
<!-- We return to this more fully in the General Discussion. -->
<!-- ] -->

[MHT: talk about soft semantics here?]

\begin{eqnarray}
L_{0}(h, \theta \mid u) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

The $\delta_{\denote{u}(h, \theta)}$ likelihood is the Kronecker delta function returning $1$ for states $h$ compatible with utterance $u$ and $0$ otherwise. 
For the case of our semantics for generalizations, these are degrees of probability $h$ that are greater than the semantic threshold $\theta$.
The threshold $\theta$ is assumed to be drawn by an uninformative prior distribution, defined on the same scale as the probability $h$: $\theta \sim \text{Uniform}(0, 1)$.^[
Though it's plausible that different thresholds could be learned for different domains, we assume an uninformed prior distribution in our model to show how background knowledge about how the world works drives inferences.
]
Finally, the prior distribution $P(h)$ is a distribution over the degrees of probability.
The particular form of the prior plausibly depends upon the kind of property or event in question and could be structured as a result of deeper conceptual knowledge about properties and events [cf., @Griffiths2005; @Tenenabum2006; @Kemp2008].
This is a simple Bayesian model of interpreting a statement conveying a generalization.

For addressing the question of whether or not a generalization is "true" or "felicitous", we consider how a Gricean speaker would act, given the goal of being informative to the literal listener (Eq. \ref{eq:L0}). 

<!-- S_{1}(u \mid k) \propto \exp{(\lambda \cdot {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(x, \theta \mid u)}  \diff \theta ) } -->
\begin{equation} 
S_{1}(u \mid h) \propto \exp{(\lambda \cdot U(u; h)  }
\label{eq:S1}
\end{equation}

Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda$.
Inspired by Bayesian decision theory, the speaker takes actions (i.e., produces utterances) soft-max optimally in accord with his utility function \cite{Baker2009}.
The utility function of a speaker is based on the informativeness of the utterance, which is computed with respect to what a naive listener $L_0$ would believe after hearing the utterance, taking into account the cost of the utterance $C(u)$. 
\begin{equation}
U(u; h) = \ln{ \int_{\theta} L_0(h \mid u)}  \diff \theta - C(u)
\label{eq:utilS1marginalization}
\end{equation}
Since the speaker doesn't know the threshold $\theta$, he must integrate over the likely values that the listener will infer. ^[
A more general version of this model could relax the assumption that the speaker has access to a degree of propensity $h$ that he is trying to communicate.
Rather, the speaker may have a belief distribution over $h$, corresponding to his beliefs about the rate with which an event happens or the prevalence of the property in the kind. 
In this situation, we would define the utility function to be the expected value of the informativity, which integrates over his belief distribution: 
$U(u; k) = {\mathbb E}_{h\sim P_{k}} \ln{ \int_{\theta} L_0(h, \theta \mid u)}  \diff \theta$
]
<!-- $S_1$ thus seeks to minimize the surprisal of $s$ given $u$ for the naive listener, while bearing in the mind the utterance cost $C(u)$. -->

The proportionality in Eq. \ref{eq:S1} implies normalization over a set of alternative utterances.
In this paper, we aim to account for endorsements of generalizations using $S_1$.
As such, we consider as an alternative only the possibility of staying quiet in addition to saying the generalization; the model can then be interpreted as a model of felicity judgments [@Degen2014]. 
Again, the generalization is a threshold function: $\denote{u_{generalization}}(h, \theta)=\{h: h > \theta \}$.
The silent or \emph{null} utterance alternative carries no information (i.e., $\denote{u_{silence}}(h, \theta) = \{h\}$) and produces a posterior distribution in the listener identical with the prior.^[
This alternative can be realized in at least two other ways: the speaker could have said the negation of the utterance (i.e., *The generalization is not true.*) or the negative generalization (i.e., *The opposite is true.*, e.g., *Ks do not have F*).
All results reported are similar for these two alternatives, and we use the alternative of the *null* utterance for simplicity.
]
For model simplicity, we assume no difference in utterance cost between these two alternatives: $C(u_{generalization}) = C(u_{silence})$.

## Alternative formulations

The pair of speaker-listener models presented above correspond to the simplest possible communicative models with an underspecified threshold meaning for a generalization. 
As a computational level theory, the Rational Speech Act framework defines models with potentially infinite levels of recursion.
For the simplest formulation of RSA, the level of recursion for a model can be unidentifiable from the 
"speaker optimality" model parameter in \ref{eq:utility} [@FrankEtAlUnderReview].
For models that extend RSA to have additional forms of uncertainty (e.g., an uncertain threshold criterion), theoretically meaningful different models can be formulated in the pragmatic framework  
The comparison of these models will also us to test the kind of pragmatic sophistication required for a theory of understanding generalizations in language.

### Listener models

Eq. \ref{eq:L0} describes how a naive Bayesian agent should update her beliefs about the probability after hearing an utterance conveying a generalization. 
That model is not pragmatic because it doesn't reason about the generative process of the utterance (i.e., it doesn't reason about a speaker). 
Eq. \ref{eq:L1u} extends this model adding a level of recursion, modeling a pragmatic listener who reasons about the Gricean speaker in Eq. \ref{eq:S1} who reasons about the literal listener in Eq. \ref{eq:L0}.

\begin{eqnarray}
L_{1}(h \mid u) &\propto& S_{1}(u \mid h) \cdot P(h) \label{eq:L1u}
\end{eqnarray}

This $L_1$ model reasons about the conditions under which the speaker $S_1$ would produce one or another speech act (here, the generalization or staying silent), described above.
Still, the threshold variable gets resolved by $L_0$ (and marginalized out by $S_1$): The posterior distribution over thresholds is arrived at by $L_0$ trying to make the generalization true.
\mht{How does this posterior on states differ?}

Truthfulness is not the only process that could help set $\theta$.
If the listener were to reason about what threshold was likely to be *used by the speaker*, informativity would also help set $\theta$.
This can be modeled by a pragmatic listener who reasons about the threshold, but assumes the speaker had a particular threshold in mind (Eq. \ref{eq:S1l}). 

\begin{eqnarray}
L_{1}(h, \theta \mid u) &\propto& S_{1}(u \mid h, \theta) \cdot P(h) \cdot P(\theta) \label{eq:L1l} \\
S_{1}(u \mid h) \propto \exp{(\lambda \cdot U(u; h; \theta)  } \label{eq:S1l} \\
U(u; h; \theta) = \ln{  L_0(h \mid u, \theta) } - C(u) \label{eq:utilS1nomarginalization} \\
L_{0}(h \mid u, \theta) &\propto& {\delta_{\denote{u}(h, \theta)} \cdot P(h) \label{eq:L0l}
\end{eqnarray}

Eq. \ref{eq:L1} is mathematically equivalent to the gradable adjectives model of @Lassiter2013, with the modification that states of the world are defined with respect to the probability $h$ under discussion.
In this model, the listener's posterior distribution on thresholds is arrived at by trying to make the utterance conveying the generalization to be both truthful and informative. That is, the listener must believe the threshold used by the speaker is both true and informative; very low thresholds could only be informative in contexts when the listener believes most probabilities are all very low *a priori*.

These alternative pragmatic listener $L_1$ models give rise to corresponding alternative pragmatic speaker models.

### Speaker model

We can define two speaker models who reason about the two different pragmatic listeners.

The speaker who reasons about the pragmatic listener in Eq. \ref{eq:L1u} is defined by a utility function similar to Eq. \ref{eq:utilS1nomarginalization}, but with informativity computed with respect to $L_1$'s posterior distribution rather than $L_0$. 

\begin{eqnarray}
S_{1}(u \mid h) \propto \exp{(\lambda \cdot U(u; h; \theta)  } \label{eq:S1l} \\
U(u; h; \theta) = \ln{  L_1(h \mid u) } - C(u) \label{eq:utilS2nomarginalization} \\
\end{eqnarray}

The model who reasons about the pragmatic listener in Eq. \ref{eq:L1l} marginalizes over $L_1$'s posterior distribution on the threshold $\theta$ in order to decide which utterance to produce (similar to \ref{eq:utilS1marginalization}).

\begin{eqnarray}
S_{2}(u \mid k) \propto \exp{(\lambda_2 \cdot U_{S_2}(u; k)  } \\ 
U_{S_2}(u; h) = \ln{ \int_{\theta} L_1(h, \theta \mid u)}  \diff \theta - C(u) \label{eq:S2l}
\end{eqnarray}


All models were implemented in the probabilisitic programming language WebPPL [@dippl], and a complete, runable versions of the models in the following section can be found at \url{http://forestdb.org/models/generics.html}

## Model simulations

All of the RSA models defined in the previous section predict that the interpretation and endorsement of a generalization depends listener's *a priori* beliefs about the probabilty in question $P(h)$.
For purposes of illustration, we also present versions of each model where the threshold is not uncertain but is fixed to either 0 or 0.5 (corresponding intuitively statements like *some of the time* or *most of the time*).

We start with the simplest uncertain threshold models. 
Consider how an uniformly underspecified threshold would interact with a uniformly uncertain belief about a probability (recall the thresholds defines what probabilities or states of the world should be ruled out). 
If the threshold is very high, only the highest probabilities would be left (i.e., pass the threshold).
If the threshold is slightly lower but still high, the highest probabilities would still pass the threshold, as would some that are slightly lower.
As the threshold takes on lower and lower values, more and more probabilities would surpass it.
When the threshold is very low, almost all probabilities are consistent with it.
But since we don't know *a priori* what the threshold is likely to be, each of theses situations is plausible.
The posterior distribution on probabilities or states of the world will favor higher probabilities (because they pass more thresholds), but will still include probabilities that are quite low [though they are *a posteriori* unlikely; see Figure \ref{fig:simluations} ]. 
Speakers, thus, will produce generalizations with some probability in each state of the world, though the speaker will say the generalization more often when the propensities associated with the states of the world are higher [see Figure \ref{fig:simluations}].

```{r simluations, eval = T, cache = T}

l0.model <- '
var probability = function(Dist, x) {
    return Math.exp(Dist.score(x));
}
var targetUtterance = "generic";

var utterancePrior = Infer({model: function(){
  return uniformDraw([targetUtterance,"silence"])
}});

var thetaPrior = Infer({model: function(){
 return uniformDraw([
   0.01, 0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
   0.5, 0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95
 ])}
});

var bins = [
  0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,
  0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,0.99
];


var meaning = function(utt,state, theta) {
  return utt=="generic"? state > theta :
         utt=="generic is false"? state<=theta :
         utt=="silence"? true :
         utt=="some"? state>0.01:
         utt=="most"? state> 0.5:
         utt=="all"? state >= 0.99:
         true
}

var mixture = data.mix[0];
var priorParams = data.params[0];

var statePrior = Infer({model: function(){
  var component = flip(mixture);
  return component ?
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta(priorParams), b) + Number.EPSILON
      }, bins )
    }) :
    categorical({
      vs: bins,
      ps: map(function(b) {
        return probability(Beta({a:1,b:50}), b) + Number.EPSILON
      }, bins )
    })
}});

var listener0 = cache(function(utterance) {
  Infer({model: function(){
    var state = sample(statePrior)
    var state_prior = sample(statePrior)
    var theta = utterance == "generic" ? sample(thetaPrior) : -99
    condition(meaning(utterance, state, theta))
    return {
      state_Posterior: state, 
      state_Prior: state_prior
  }
 }})}, 10000)

var out = {generic: listener0("generic"), some: listener0("some"), most: listener0("most")}
out
'

priorNames <- c("uniform", "uniformRare", "isFemale", "laysEggs", "eatsFood", "hasWings", "carriesMalaria", "areSick")
priorShapes <- list(
  uniform =  list(params = data.frame(a = 1, b = 1), mix = 1), 
  uniformRare =  list(params = data.frame(a = 1, b = 1), mix = 0.25), 
  isFemale =  list(params = data.frame(a = 10, b = 10), mix = 1), 
  laysEggs =  list(params = data.frame(a = 10, b = 10), mix = 0.25), 
  eatsFood =  list(params = data.frame(a = 50, b = 1), mix = 1), 
  hasWings =  list(params = data.frame(a = 50, b = 1), mix = 0.25), 
  areSick =  list(params = data.frame(a = 1, b = 5), mix = 1), 
  carriesMalaria =  list(params = data.frame(a = 1, b = 5), mix = 0.25)
)

all.simulations <- data.frame()
for (p in priorNames){
  inputData = priorShapes[[p]]
  
  l0.rs <- webppl(l0.model, data = inputData, data_var = "data")
  
  all.simulations <- bind_rows(all.simulations,
                          bind_rows(
                            data.frame(l0.rs$generic) %>%
                              gather(key, val, -probs) %>%
                               group_by(key, val) %>%
                               summarize(prob = sum(probs)) %>%
                              ungroup() %>%
                                mutate(key = gsub("support.", "", key)) %>%
                               separate(key, into = c("Parameter","Distribution")) %>%
                                mutate(Distribution = gsub("Posterior", "Posterior_Generalization", Distribution)),
                            data.frame(l0.rs$some) %>%
                              gather(key, val, -probs) %>%
                              group_by(key, val) %>%
                              summarize(prob = sum(probs)) %>%
                              ungroup() %>%
                              mutate(key = gsub("support.", "", key)) %>%
                              separate(key, into = c("Parameter","Distribution")) %>%
                              mutate(Distribution = gsub("Posterior", "Posterior_0", Distribution)) %>%
                              filter(Distribution != "Prior"),
                            data.frame(l0.rs$most) %>%
                              gather(key, val, -probs) %>%
                              group_by(key, val) %>%
                              summarize(prob = sum(probs)) %>%
                              ungroup() %>%
                              mutate(key = gsub("support.", "", key)) %>%
                              separate(key, into = c("Parameter","Distribution")) %>%
                              mutate(Distribution = gsub("Posterior", "Posterior_05", Distribution)) %>%
                              filter(Distribution != "Prior")
                          ) %>%
                            mutate(PriorShape = p)
  )
  #print(p)
}
```

```{r fig.width = 10, fig.height=4.5}
all.simulations %>%
  mutate(Distribution = fct_relevel(Distribution, "Prior", "Posterior_0", "Posterior_05", "Posterior_Generalization"),
         PriorShape = fct_relevel(PriorShape, "uniform", "isFemale", "eatsFood","areSick", "uniformRare", "laysEggs",    "hasWings", "carriesMalaria")) %>%
  ggplot(., aes(x = val, y = prob, lty = Distribution, color = Distribution, fill = Distribution))+
    geom_density(stat= 'identity', size = 0.8, alpha = 0.3)+
    #geom_bar(stat = 'identity', position = position_dodge(), color = 'black')+
    theme_few() +
    scale_color_solarized()+
    scale_fill_solarized()+
    scale_linetype_manual(values = c(3, 2, 4, 1))+
    scale_x_continuous(breaks = c(0, 0.5, 1))+
    xlab("Degree of propensity (probability)") +
    ylab("Probability density") +
  facet_wrap(~PriorShape, scales = 'free', nrow = 2)
```


<!-- $P(x)$ is a distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across animal categories.  -->
<!-- In Figure \ref{fig:schematic-unif}, are $L_1$ (Eq.~\ref{eq:L1}) posterior prevalence distributions on $x$ (red solid line) for several example prior prevalence distributions $P(x)$ (blue dashed line), as well as the $S_2$ generic endorsement probability for different levels of prevalence. -->
<!-- We name these example prior distributions to suggest properties that might be associated with such priors.  -->
<!-- (Later, we will empirically measure these priors for properties of interest.) -->
<!-- First, we explore generic interpretation and endorsement for priors of three different shapes (left column).  -->
<!-- For each of these, the $L_1$ posterior distribution (red solid) over prevalence is heavily driven by the prior (blue dashed). -->
<!-- $S_2$ endorsement probabilities for the generic (black solid) increase as a function of prevalence, and what counts as "true" (in terms of the prevalence) depends on the prior.  -->

<!-- \begin{figure} -->
<!-- \centering -->
<!--     \includegraphics[width=\columnwidth]{figs/schematics_s2.pdf} -->
<!--     \caption{Schematic prior distributions for prevalence $x$ (blue dashed), the pragmatic listener $L_1$ model's posterior distribution over prevalence upon hearing a generic utterance (red), and speaker $S_2$ model's endorsement of a generic utterance for different levels of prevalence (black). -->
<!--     The names given to these priors are meant to be suggestive of what kinds of properties these distributions might correspond to. -->
<!--     The left column uses prevalence priors modeled as Beta(15,15), Beta(4,1), and Beta(4,16) distributions. -->
<!--       The right column uses a prior distribution that is a mixture of the distribution to the left of it with a second component, modeled as Beta(0.5, 4.5), reflecting categories with 0\% property prevalence. -->
<!--     Horizontal dashed line at 0.5 is for convenience of comparing the point at which an utterance becomes judged as more true than false for $S_2$. -->
<!-- 	Note that the prior distribution over prevalence will be the same as $L_1$'s posterior distribution upon hearing the "null" utterance. -->
<!--     } -->
<!--   \label{fig:schematic-unif} -->
<!-- \end{figure} -->


<!-- To take one example, consider the distribution over what might be the property "are female" (top-left facet). -->
<!-- The \emph{a priori} prevalence is centered around 0.5. -->
<!-- Because of pragmatics, the pressure to be \emph{truthful} will drive likely threshold values below 0.5 (lower values are more likely to be true).  -->
<!-- At the same time, the pressure to be \emph{informative} will drive the threshold values up: Higher values are more likely to be informative.  -->
<!-- The result is a posterior over prevalence that is only marginally greater than the prior, making higher prevalence values more likely after hearing the generic.  -->
<!-- But the relative information gain is very little (the posterior is not very different from the prior), and thus the $S_2$ model is reluctant to endorse the generic unless the property is exceedingly prevalent.  -->
<!-- The same basic phenomenon can be observed for the other two example priors (left column): The posterior over prevalence heavily depends upon the prior, but is also not very different from the prior. -->

<!-- We now imagine what would happen if there were some kinds where the property was completely absent, while being present at some rate in other kinds. -->
<!-- Figure \ref{fig:schematic-unif} (right column) shows this possibility: mixing the distribution to the left of it with a second component at 0\% prevalence. -->
<!-- Consider the schematic prior over "lay eggs". -->
<!-- We see the pragmatic listener $L_1$'s posterior prevalence distribution is not very different from the interpretation that doesn't include the second component in the prior (here, "are female"), but it is dramatically different from the prior with two components (compare red with blue dashed line for the right column). -->
<!-- This suggests that when many categories have 0\% prevalence, lower thresholds are informative.  -->
<!-- Indeed, the $S_2$ model predicts that the generic becomes felicitous at lower prevalence levels (compare black line of left v. right column). -->
<!-- For "lay eggs", when the property is prevalent in 50\% of the kind (e.g., 50\% of birds lay eggs), endorsement of the generic (e.g., \emph{Birds lay eggs}) by the $S_2$ is roughly 0.85; for "are female", with the same prevalence (50\% of birds are female), endorsement for \emph{Birds are female} is only 0.50: It is judged to be neither true nor false.  -->
<!-- The important result is the asymmetry: the first generic can be endorsed more strongly than the second, at the same prevalence level; the exact endorsement rates depend on quantitative aspects of the priors, which must be determined empirically. -->

<!-- The model thus predicts differences in truth judgments depending on the prevalence prior.  -->
<!-- The first test of our theory, then, will be to see if these predictions correspond to human truth judgments of familiar generic sentences.  -->




<!-- Next is a hypothetical speaker, who reasons about the literal listener when deciding which utterance to produce. -->
<!-- \begin{eqnarray} -->
<!-- S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln {L_{0}(x \mid u, \theta)} ) }\label{eq:S1} -->
<!-- \end{eqnarray} -->
<!-- This speaker $S_1$ also knows the threshold $\theta$ and chooses utterances to convey information to the literal listener $L_0$ . -->
<!-- Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda_1$. -->


<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->
<!-- In this model, a threshold $\theta$ is established in context by a Bayesian listener reasoning about what threshold would make this utterance true? -->



<!-- The model begins with a simple Bayesian agent---a literal listener---who updates her prior beliefs $P(x)$ according to the truth-functional meaning of the utterance heard $\denote{u}$. -->
<!-- \begin{eqnarray} -->
<!-- L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0} -->
<!-- \end{eqnarray} -->
<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->

<!-- The likelihood $\delta_{\denote{u}(x)}$ is the Kronecker delta function returning $1$ for states $x$ compatible with utterance $u$ (i.e., where the prevalence $x$ is above the threshold $\theta$), and $0$ otherwise. -->
<!-- The literal listener is a hypothetical agent who knows the threshold $\theta$. -->

<!-- Next is a hypothetical speaker, who reasons about the literal listener when deciding which utterance to produce. -->
<!-- \begin{eqnarray} -->
<!-- S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln {L_{0}(x \mid u, \theta)} ) }\label{eq:S1} -->
<!-- \end{eqnarray} -->
<!-- This speaker $S_1$ also knows the threshold $\theta$ and chooses utterances to convey information to the literal listener $L_0$ . -->
<!-- Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda_1$. -->



<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the propensity: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->
<!-- The prior distribution $P(h)$ is a distribution over likely degrees of propensity.  -->
<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->

<!-- Life is only so long, however, and a person can only be in one place at a time. -->
<!-- The amount of data we can gather about any particular category, event, or causal force is extremely limited by these physical constraints. -->
<!-- Thus, it would be useful if language provided a simple way to communicate these generalizations to one another. -->





<!-- # A Formal Theory of Generalizations in Language -->

<!-- We find both the statistical and conceptual accounts compelling, but an important perspective is missing. -->
<!-- Linguistic generalizations are not unique in their flexibility. -->
<!-- Understanding language in general depends upon assumptions interlocutors make about each other and what content is under discussion.  -->
<!-- Viewing language understanding as a social reasoning process reveals that utterances carry a mosaic of interpretations with a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000].  -->
<!-- Can the puzzles of generic language be understood as effects of pragmatic reasoning? -->
<!-- If so, we may be able to get away with a relatively simple, statistical,  semantic theory. -->
<!-- Abstract mental representations, then, may be the conceptual backdrop against which generic language is interpreted.  -->

<!-- We pursue such a line of inquiry, assuming the simplest truth-conditional meaning: a threshold on prevalence $P(F\mid K)>\theta$ [c.f., @Cohen1999]. -->
<!-- No fixed value of the threshold, $\theta$, would allow for the extreme flexibility generics exhibit (e.g. \emph{Mosquitos carry malaria}; \emph{Robins lay eggs} v. \emph{Robins are female}),  -->
<!-- so instead we allow this threshold to be established in context through pragmatic reasoning. -->
<!-- Such an inference would depend on background knowledge about properties and categories---potentially structured, conceptual knowledge. -->
<!-- This inference, nonetheless, is a general mechanism of understanding language, not specific to interpreting generic statements. -->
<!-- We formalize this hypothesis in the Rational Speech Act (RSA) theory---a formal, probabilistic theory of language understanding. -->
<!-- RSA is derived from social reasoning, formalized as recursive Bayesian inference between speaker and listener [@Frank2012; @Goodman2013; @Goodman2016; see also, @Franke2009; @Franke2015]. -->
<!-- @Goodman2016 provides a good introduction to the RSA framework and Appendix A presents a brief tutorial on RSA for the reader unfamiliar. -->


<!-- The model begins with a simple Bayesian agent---a literal listener---who updates her prior beliefs $P(x)$ according to the truth-functional meaning of the utterance heard $\denote{u}$. -->
<!-- \begin{eqnarray} -->
<!-- L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0} -->
<!-- \end{eqnarray} -->
<!-- For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$. -->
<!-- The prior distribution $P(x)$ is a distribution over likely states of the world.  -->
<!-- For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly). -->
<!-- The likelihood $\delta_{\denote{u}(x)}$ is the Kronecker delta function returning $1$ for states $x$ compatible with utterance $u$ (i.e., where the prevalence $x$ is above the threshold $\theta$), and $0$ otherwise. -->
<!-- The literal listener is a hypothetical agent who knows the threshold $\theta$. -->


<!-- To understand how language is interpreted pragmatically, we model a pragmatic listener $L_1$ who updates her prior beliefs $P(x)$ by reasoning about the generative process of the utterance, the speaker model $S_1$. -->
<!-- \begin{eqnarray} -->
<!-- L_{1}(x , \theta \mid u) &\propto& S_{1}(u \mid x, \theta) \cdot P(x) \cdot P(\theta) \label{eq:L1} -->
<!-- \end{eqnarray} -->
<!-- Following @Lassiter2013, we model the pragmatic listener as having uncertainty about the threshold: $P(\theta)$. -->
<!-- Though the listener doesn't know the threshold *a priori*, she resolves the threshold (i.e., the meaning) in context by integrating her prior beliefs $P(x)$ with the basic principles of communication to be truthful and informative, instantiated in her model of the speaker $S_1$ [@Lassiter2015; @GoodmanLassiter]. -->
<!-- In principle, thresholds could be learned over time for different contexts, but here we assume the listener has no informative knowledge about the semantic variable $P(\theta) = \text{Uniform([0, 1])}$. -->
<!-- The pragmatic listener $L_1$ (Eq. \ref{eq:L1}) is thus a model of *generic interpretation*: Upon hearing a generic, how should a listener update her beliefs? -->

# Communicating generalizations about events

*Habitual* language (e.g,. "Mary smokes."; "My car won't start.") conveys generalizations about events. 
In this first set of experiments, we test whether or not the context-sensitivity of these generalizations can be explained by an underspecified threshold semantics.
As our first case study, we focus on a particular class of events: people doing actions.

The internal structure of events can differ in several ways, one obvious way being the duration of the event [CITE event people?].
For example, the event of *smoking a cigarette* takes between 5 - 10 minutes, while the event of *wearing socks* probably lasts for several hours.
We take as a starting point the fact that events can be segmented and aggregated; the relevant dimension becomes the frequency with which the event occurs.

In our main test of the hypothesis, we collect felicity or truth judgments for habitual statements about people's actions.
The computational model described in Eq. \ref{eq:S1} is fully specified except for the prior distribution over the frequency of an event $P(h)$, which plausibly varies across different kinds of events.
First, we measure the prior distribution, and use it and the computational model to predict human endorsement of habitual statements.

## Prior elicitation

In this experiment we elicit the prior $P(h)$ for different events in order to generate model predictions for corresponding habitual statements.
For the case of statements about the behaviors of people, $P(h)$ can be conceived as a distribution over *different people*, each of whom do the behavior with a different frequency.
To better understand what frequencies should be expected for different actions, we had participants judge the frequency of action for people they were familiar with.


### Method

#### Participants

We recruited 50 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 12 minutes and participants were compensated \$1.25 for their work.

#### Materials

<!-- We created thirty-one events organized into pairs or triplets from 5 different conceptual categories:  -->
We created twenty-seven events organized into pairs or triplets from 5 different conceptual categories: 
food and drug (e.g. \emph{eats caviar}, \emph{eats peanut butter}), 
work (e.g. \emph{sells things on eBay}, \emph{sells companies}), 
clothing (e.g. \emph{wears a suit}, \emph{wears a bra}), 
entertainment (e.g. \emph{watches professional football}, \emph{watches space launches}), and 
hobbies (e.g. \emph{runs}, \emph{hikes}). 
Items were chosen to intuitively cover a range of likely frequencies of action, as well as to provide a minimal comparison to another item by having a common superordinate action (e.g. \emph{eating} caviar vs. peanut butter).

#### Procedure

The experiment began by having participants input the names of eight friends or family members that they could answer some questions about.
Participants were told that these names would only be used to assist them in reasoning through the second part of the experiment. 
They were advised to restrict themselves to consider only adult friends or family members.

In the second phase of the experiment, participants were asked: "For each of the following people, how often does he or she \textsc{do action}?"
Participants had to fill in the blanks in sentences of the form: "\text{friend does action} ___ times per ___".
The first blank was a text box where participants could enter a number.
The second blank was a drop-down menu where participants could select a time interval from the following options: \{week, month, year, 5 years\}
They completed this task for each of their eight listed friends or family members.
To make the task slightly less tedious, there was an option where participants could set the time window for all eight of their responses.

In order to get sufficient data for actions that are relatively uncommon (e.g., *climbs mountains*, *writes novels*), if participants responded that all of their friends do the action with a frequency of "0 times" (in whatever interval), a follow-up question appeared that read: "Do you know anybody who has \textsc{done action} before?". 
If they responded "Yes", another follow-up question appeared that read: "How often do you think that person \textsc{does action}?", with the same dependent measure format as before.

Participants completed these elicitation trials for a random subset of 15 items, presented in a randomized order.
The experiment can be viewed at \url{http://stanford.edu/~mtessler/generics-paper/experiments/habituals/priors/friends-and-family-1.html}.

\begin{figure*}[t]
\centering
  \includegraphics[width=0.84\textwidth]{figs/tj-scatters-3}
  \caption{
  Human acceptability judgments as a function of the log frequency of action (left) and speaker $S_2$ model predictions (right) for ninety-three unique items (event \textsc{x} frequency). 
  Color denotes target-individual frequency of action (log scale), with lighter colors indicating more frequent actions. 
  Actual frequency noted on x-axis for examples (left).
  Error bars correspond to 95\% bootstrapped confidence intervals for the participant data and 95\% Bayesian credible intervals for the model predictions. 
  Error bars suppressed and points jittered on left facet for visual clarity.
  }
  \label{fig:tjScatters}
\end{figure*}

### Data preprocessing

Every response for a frequency of a person doing an action is a sample from $P_{event}(h)$.
We first transformed all responses onto the same scale of "times per year" (assuming 52 weeks per year; 12 months per year).
An empirical distribution can be found by making a histogram of these responses. 
The empirical distributions follows a kind-of log-normal distribution, with the additional feature of having a substantial probability mass at the frequency of 0 (for those individuals who never do the action). 
We thus log-transformed the empirical data. 
To avoid values of -Infinity, we set the responses of 0 to mean roughly "once every twenty years".

We took the log-transformed (and "-Infinity"-adjusted) raw data and binned each response to the closest bins on a grid with binwidth of 0.5 in the log-"times per year" scale.
This yields a grid of 23 points with frequencies ranging from "once every twenty years" (log-frequency = -3) to "twenty times per day" (log-frequency = 9).

### Data analysis and results

We analyze the data using a Bayesian approach to allow for a consistent integration into the computational model.
The data we would like to explain are: the bins $n_i \in \{-3, -2.5, ..., 8.5, 9\}$ that participants gave in response to items $i \in \{1, ..., 27\}$.
The data is explained as a function of subjective beliefs $P_i$, with $P_{ij}$ being participants' (as a collective) belief about the relative likelihood for bin $j$ for item $i$. 
Each $P_i$ defines a likelihood for our data, assuming an appropriate linking function.

Following @Franke2016, the linking function for this data treats each bin $n_{i}$ as a draw from a categorical distribution where the probability of bin $j$ is proportional to $\exp{(a\cdot P_i)}$, i.e., a soft-max choice from $P_{i}$. 
The higher parameter $a$, the more likely $n_{i}$ is the mode of $P_{i}$. 
For $a \rightarrow 0$, all bins become equiprobable.

\begin{eqnarray}
P_{i} &\sim& \text{Dirichlet}(1, ..., 1) \\ 
a & \sim & \text{Gamma}(2,1) \\
n_{i} &\sim & \text{Categorical}(\exp(a \cdot P_i))
\end{eqnarray}

VIZ (as before)?

<!-- We assume each "binned" response is a sample from a multinomial distribution with unknown probability vector. -->
<!-- We put a Dirichlet-prior over this probability vector. -->

<!-- We built a Bayesian data analysis model for this prior elicitation task. -->
<!-- Question 1 elicits the proportion of people who have done an action before.  -->
<!-- We model this data as coming from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma_{1}, \xi_{1})$.  -->
<!-- Question 2 elicits the rate, or relative frequency, with which a person does the action. -->
<!-- This was modeled by a log-normal distribution: $\ln d_{2} \sim \text{Gaussian}(\mu_{2}, \sigma_{2})$.  -->
<!-- Each item was modeled independently for each gender. -->
<!-- We implemented this model using the probabilistic programming language WebPPL \cite{dippl}, and found the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burnin. -->

<!-- The priors elicited cover a range of possible parameter values as intended (Figure \ref{fig:priorScatter}, scatter), resulting in parametrized distributions of dramatically different shapes (insets).   -->
<!-- We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency  of action (Question 2) ($r_{1,2} = 0.74$). -->
<!-- Items that tend to be more popular actions also tend to be more frequent actions (e.g. \emph{wears socks}) and visa-versa (e.g. \emph{steals cars}), though there are notable exceptions (e.g. \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done very often).  -->
<!-- This diversity is relevant because the speaker model (Eq.~\ref{eq:S2}) will produce habitual sentences (e.g. \emph{Sam goes to the movies vs. the ballet.}) contingent on the shape of the prior distribution.  -->

<!-- From the inferred parameters and assumed functional forms, we get an inferred $P(p)$ modeled as a mixture of individuals with the possibility of carrying out the action and those without the possibility of doing it.  -->
<!-- That is, $P(p)$ was constructed by sampling $p$ as follows: -->

<!-- \begin{align} -->
<!-- \theta & \sim \text{Beta}(\gamma_{1}, \xi_{1}) \nonumber \\  -->
<!-- \ln \lambda & \sim \begin{cases} -->
<!-- 		\text{Gaussian}(\mu_{2}, \sigma_{2}) &\mbox{if } \text{Bernoulli}(\theta) = \textsc{t} \label{eq:priorModel}  \\ -->
<!-- 				\delta_{\lambda=-\infty} &\mbox{if } \text{Bernoulli}(\theta) = \textsc{f} \\ -->
<!-- 		\end{cases} -->
<!-- \end{align} -->

<!-- In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior in our language model resolves two technical difficulties: (1) It smooths effects that are clearly results of the response format^[ -->
<!-- For example, a very common rating is *1 time per year*. Presumably participants would be just as happy reporting *approximately* 1 time per year; the raw data does not reflect this due to demands of the dependent measure. -->
<!-- ] -->
<!-- and (2) it better captures the tails of the prior distribution which have relatively little data and need to be regularized by the analysis. -->
<!-- Figure \ref{fig:priorScatter} (right) shows example inferred priors. -->

<!-- Some items show substantial differences between the genders (e.g., *wears a bra*) and some show subtle differences (e.g., *watches professional football*).  -->
<!-- We will explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2, for select items with priors that differ substantially by gender. -->

## Experiment 1: Endorsing generalizations about events

A present-tense habitual sentence is of the form \textsc{singular noun phrase} $+$ \textsc{present tense simple verb phrase} (e.g. \emph{Bill smokes cigarettes.}). 
We next explore the endorsements of habituals of this form made from the items whose propensity priors were measured the prior elicitation task. 

### Method

#### Participants

We recruited 150 participants from MTurk.
To arrive at this number, we performed a Bayesian precision analysis to determine the minimum sample size necessary to reliably ensure 95\% posterior credible intervals no larger than 0.3 for a parameter whose true value is 0.5 and for which the data is a 2-alternative forced choice.
This analysis revealed a minimum sample size of 50 per item; since participants only completed about one third of the items, we recruited 150 participants.
The experiment took 4 minutes on average and participants were compensated \$0.55 for their work.

#### Procedure and materials

On each trial, participants were presented with a \emph{past frequency statement} for a given event of the form: "In the past M \{weeks, months, years\}, \textsc{person} \textsc{did x} 3 times".
For example, \emph{In the past month, Bill smoked cigarettes 3 times}.
The particular intervals used (\{weeks, months, years\}) were selected after examining the predictions of the speaker model (Eq.~\ref{eq:S1}), for each item independently, to yield a range of predicted endorsement rates.
The items were the same as in the prior elicitation task.

Participants were asked whether they agreed or disagreed with the corresponding habitual sentence: "\textsc{person does x}" (e.g., \emph{Bill smokes cigarettes}).
Participants completed 37 trials, which were composed of the 27 items from the prior elicitation task and 4 additional filler items not in the prior elicitation task randomly paired with either a male or female character name. 6 of these items were then also paired with a name of the opposite gender (e.g., participants saw both a female character and a male character who watched professional football). These were used for an exploratory analysis on gender differences.
The 4 filler items were not used in the prior elicitation because they described illegal activities (e.g., "doing cocaine").
The experiment can be viewed at \url{http://stanford.edu/~mtessler/habituals/experiments/truth-judgments/tj-2.html}.

### Results

#### Behavioral results

On each trial of the experiment, the participant was told a person did a particular action 3 times during some time window. 
A simple null hypothesis is that habitual statements convey that the a person *has done this thing before*. 
Under this hypothesis, all of the items used in our experiment would qualify as good habitual statements.
A more sophisticated alternative hypothesis is that the degree to which the habitual statement is good is the frequency itself. 
Our computational model predicts that the the degree to which the habitual statement is good is the frequency of event, normalized by the alternative possible frequencies under the prior.

Both alternative hypotheses can immediately be ruled by out by visual inspection of Figure \ref{fig:tjScatters} (left), which shows the correspondence between the frequency of the event (transformed a log times-per-year scale) and the human-judged felicity of the corresponding habitual sentence. 
First, it is clear that not all habitual statements are strongly assented to, even though it is true that all the stimuli involved an agent doing the action with some non-zero frequency.
It is also clear that a habitual sentence can receive strong agreement even when the actions are very infrequent (log frequency $\sim$ -1; 3 times in a 5-year interval; e.g. \emph{writes novels}, \emph{climbs mountains}), arguing against the second alternative.
We also see even when actions are done relatively frequently (e.g. 3 times in a one month interval; log frequency $\sim$ 5), there are habitual sentences participants are reluctant to endorse completely (e.g. \emph{wears socks}, \emph{drinks coffee}). 
In our data, actions completed with a high frequency (3 times in a one week interval; log frequency $\sim$ 6.5) receive at a minimum 75\% endorsement, though there is still variability among them (e.g. between 10-25\% of people disagree with \emph{wears a watch} and \emph{wears a bra}). %, suggesting that even actions that are completed almost everyday can insufficient to generalize.
Overall, log frequency of action predicts only a fraction of the variability in responses ($r^2(93) = 0.33$).
For actions that are done on the time scale of years or longer (lower median of frequency), frequency itself no longer explains the endorsements ($r^2(50) = 0.07$)

We observe that none of our items receive less than 25\% endorsement (i.e., a maximum of about 75\% of participants disagree with the felicity of the utterance).
This reflects the fact that these statements are not altogether *false* even though the action is done very rarely.

We find no differences between endorsements of the habitual of characters with male and female names, and overall, the mean endorsements by gender are strongly correlated $r(93) = 0.91$. 
This may be because the felicity of habitual sentences depends on a comparison to individuals of both genders (i.e, the \emph{contrast class} is other people; not just other men or women). 
Less interestingly, the lack of a difference may be the result of gender being not very salient in our paradigm, perhaps because the names used were not sufficiently gendered.

\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figs/expt3-4-scatters-camera.pdf}
  \caption{Left: Predicted log frequency as a function of past log frequency given to the participant (Expt. 3a; CIs suppressed and jitter added for visual clarity).
  Middle: Human endorsements of habitual sentences (Expt. 3b) vs. Predicted log frequency (Expt. 3a), with data for corresponding items from Expt. 2 (assumed to have the same predictive log frequency as baseline). 
  Right: Endorsements (Expt. 3b) vs. Speaker $S_2$ model predictions using empirically elicited predictive frequencies (Expt. 3a).}
  \label{fig:tj3}
  \vspace{-7pt}
\end{figure*}

#### Model fit and results

We used the pragmatic speaker model $S_2$ (Eq.~\ref{eq:S2}) with the priors elicited above (Expt.~1) to predict felicity judgments in Expt.~2, assuming the target propensity (to be conveyed by $S_2$) is the provided frequency.
Because we observe no difference between the felicity judgments for habituals of male and female characters, we use a 50\% mixture of the inferred priors for each gender to construct a single frequency distribution $P(\lambda)$ across individuals.
The model has two free parameters---the speaker optimality parameters, $\alpha_i$, in Eqs.~\ref{eq:S2} \& \ref{eq:S1}. 
We use Bayesian data analytic techniques to integrate over these parameters \cite{LW2014}, comparing the posterior predictive distribution to the empirical data in Expt.~2.
To construct the posterior predictive distribution over responses, we collected 2 MCMC chains of 100,000 iterations, discarding the first 50,000 iterations for burn in.
The Maximum A-Posteriori value and 95\% highest probability density interval for $\alpha_1$ is 19.3 [14.9,19.9] and $\alpha_2$ is 1.5 [1.4,1.6].
%The MAP and HPD interval for the data-analytic guessing parameter is 0.004 [0.0003, 0.03], suggesting that there is not a substantial amount of the data that is better explained by a model of random guessing than by our pragmatic speaker model.

As shown in Figure \ref{fig:tjScatters}, right, the pragmatic speaker model does a good job of accounting for the variability in responses ($r^2(93) = 0.94$), including actions done on the time scale of years or more  ($r^2(50) = 0.92$).
The speaker model endorses the statement when the observed frequency is relatively high, compared to the prior distribution over people doing the action. 

As another potential alternative hypothesis, we formulate a model of a speaker who judges the felicity of the habitual utterance based on simple summary statistics of the prior, such as the mean and variance.

```{r alternativeRegressionSpeaker, eval = F}
Regression alternative
-- Degree of target
-- Mean and variance of distribution over that scale
```

Only our pragmatic speaker model who reasons about a listener is able to capture the quantitative variability in our data.

In this experiment, we supplied participants with a statement about how often a person has done the action in the past and asked them to judge the correspoding habitual statement.
This raises an interesting question: Does the propensity communicated by a generalization indicate an objective, past frequency or a subjective, future expectation?

## Experiment 2: Communicating predictive propensities

People change: They might remember how much they enjoyed hiking and plan to do it more in the future; They might finally decide to quit smoking.
Yet, there is an ambiguity in the propensity degree semantics we are positing.
On one hand, speakers can only *know* about what has happened in the past.
On the other hand, language has a communicative function, and it would seem useful for speakers to convey what they believe will be the case in the future.

While past frequency is often a good indicator of future tendency, people can change abruptly due to a variety of decisions and outside events.
Does habitual language communicate propensity in terms of past frequency or future expectations?
In this set of experiments, we address this by introducing causal events that enable or prevent future actions (e.g., buying a pack of cigarettes; developing an allergy).
In Expt. 2a, we measure *predictive frequency* both when past frequency alone is observed and when these causal factors are introduced.
In Expt. 2b, we examine felicity judgments of the habitual sentence (e.g., \emph{John smokes cigarettes.}; \emph{Susan eats peanut butter.}) in the presence of these causal modifiers.
This allows us to test whether habituals are best explained by a speaker $S_1$ who communicates the objective past frequency or the subjective future frequency.

### Experiment 3a: Prediction elicitation

#### Methods

We recruited 120 participants from MTurk, using the same criterion as Expt. 1.
The experiment took 4 minutes on average and participants were compensated \$0.40.

The procedure was identical to Expt. 1 except for the inclusion of a second sentence on a subset of trials and the use of a different dependent measure. 
On all trials, participants were presented with a \emph{past frequency sentence} (same as Expt. 1).
Additionally, on one third of the trials, participants were presented with a \textbf{preventative sentence} (e.g. \emph{Yesterday, Bill quit smoking.}).
On one third of the trials, participants were presented with an \textbf{enabling sentence} (\emph{Yesterday, Bill bought a pack of cigarettes.}) 
The final third of trials had no additional evidence and were identical to Expt.~2. 

Only twenty-one of the original thirty-one items were used in order to shorten the experiment.
To increase expected variability, participants saw only the frequencies that led to most intermediate endorsement of the habitual in Expt.~2. 
In addition, we did not include separate trials for both male and female names for the select items we did in Expt.~2, since we saw no differences in their endorsements of the habitual.

Participants were asked ``In the next \textsc{time window}, how many times do you think \textsc{person} does \textsc{event}?'', where the \textsc{time window} was the same as given in the \emph{past frequency statement}.
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/habituals/experiments/priors/predictive-1.html}.


\subsection{Behavioral results}

Figure \ref{fig:tj3} (left) shows the predicted future frequency as a function of the past frequency given to the participant and the type of causal information given. 
We observe in the baseline condition that future frequency perfectly tracks past frequency (e.g. participants believe if a person smoked cigarettes 3 times last month, they will smoke cigarettes 3 times next month). 
This means that our model makes identical predictions for Expt.~2 whether the target is past frequency or expected future frequency (indicating, as expected, that we must look to the new data to distinguish these models).
Critically, we observe the preventative information appreciably decreasing and the enabling information slightly increasing predicted frequency (Figure \ref{fig:tj3} left; blue and green dots).



\subsection{Experiment 3b: Felicity judgments}


%\subsubsection{Method}
%\subsubsection{Participants} 

\noindent {\bf Methods}
We recruited 150 participants from MTurk, using the same criterion as Expt.~2.
The experiment took 4 minutes on average on participants were compensated \$0.40 for their work.
None of the participants had completed Expt.~3a.
%\subsubsection{Procedure and materials}
The only difference from Expt.~3a is the dependent measure. 
On each trial, participants were asked if they agreed or disagreed with the corresponding habitual sentence (as in Expt.~2).
The experiment can be viewed at \url{http://stanford.edu/~mtessler/habituals/experiments/truth-judgments/tj-3-preventative.html}.


\subsection{Results}

There is a clear and consistent negative effect of preventative information on endorsements for the habitual sentence (Figure \ref{fig:tj3}, middle; blue points).
When collapsing across items and subjecting the data to a generalized mixed-effects model with random by-participant effects of intercept and random by-item effects of intercept and conditions, we find evidence for a small effect of \emph{enabling} conditions on endorsements (M =  0.89; 95\% Bootstrapped CI [0.88, 0.91]) as compared to baseline (M = 0.85 [0.83, 0.87]) [$\beta = 0.42; SE = 0.15; z = 2.8$]
, and a large effect of \emph{preventative} conditions on endorsements (M = 0.29 [0.26, 0.31]) [$\beta = -3.22; SE = 0.21; z = -15.2$]. 
<!-- %\mht{should I add points from Expt. 2 to left-most scatter, or maybe there is a better plot?} -->

We use the mean predicted log frequency from Expt.~3a as the input to the speaker $S_2$ model to predict the felicity judgments measured in Expt.~3b.
We infer the two model parameters using the same analysis approach in Expt.~2. 
The model matches the data well ($r^2(63) = 0.91$; Figure \ref{fig:tj3}, right).
The same model using the past frequency as the object of communication does not match the data at all ($r^2(63) = 0.02$).
These results suggest that the felicity of habituals is based on an underlying scale of predicted future propensity, not merely the observed frequency in the past.

Interestingly, we observe endorsements in this experiment that are appreciably higher than in Expt.~2 for the same items (Figure \ref{fig:tj3}, middle; red vs. purple points). 
This may be due, in part, to an effect of the experimental context on participants: 
in this experiment the overall population of frequencies is much lower (both because we selected moderate frequencies from Expt.~2 and because of the preventative information) and participants may infer that the experimenter believes this to be a representative range and adjust judgments accordingly. Future investigation into this issue is warranted.


## Discussion

So far, we have seen that our computational model endorses and interprets statements that communicate generalizations about events in a way compr

The model decides if a habitual sentence is a pragmatically useful way to describe a person’s behavior, taking into account the listener’s prior beliefs about the action—how common it is and the likely frequency (mea- sured in Expt. 1). We validated this model by eliciting felicity judgments for habitual sentences covering diverse activities with a wide range of experimentally manipulated frequencies of action (Expt. 2). We further investigated the nature of the underlying “propensity” scale by introducing enabling and disabling evidence, measuring the predicted future frequency (Expt. 3a) and using that, with the model, to predict the felic- ity of habitual sentences (Expt. 3b). To our knowledge, the experiments presented here are first empirical investigations into the truth conditions of habitual sentences and the first test of a formal model of habitual language.

# Communicating generalizations about causes

# Communicating generalizations about categories


# Empirical Test 1: Flexible Truth Conditions

Any theory of generic language must explain their puzzling flexibility of usage with respect to prevalence.
That is, \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} are reasonable things to say, but \emph{Birds are female} is not.
The pragmatic speaker model $S_2$, Eq.~\ref{eq:S2}, is a model of truth judgments. 
We test our model on thirty generic sentences 
that cover a range of conceptual distinctions discussed in the literature  [@Prasada2013]: characteristic (e.g. \emph{Ducks have wings.}), minority (e.g. \emph{Robins lay eggs.}), striking (e.g. \emph{Mosquitos carry malaria.}), false generalization (e.g. \emph{Robins are female.}), and false (e.g. \emph{Lions lay eggs.}).
In additional to the canonical cases from the linguistics literature, we selected sentences to elicit the full range of acceptability judgments (intuitively, "acceptable", "unacceptable", and "uncertain") with low-, medium-, and high-prevalence properties. 

The pragmatic speaker model $S_2$ is fully-specified except for the prior distribution over prevalence $P(x)$, which plausibly varies by the type of property in question.
To compare the model to empirical truth judgments, we thus first measure the prior distribution over prevalence of these properties (Expt.~1a).
In Expt.~1b, we collect human judgements about the acceptability of the generic sentences. 

## Experiment 1a: Prevalence Priors

The prior $P(x)$ (in Eqs.~\ref{eq:L1}, \ref{eq:L0}) describes the belief distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across relevant categories. 
In exploring the model, we saw that the shape of this distribution affects model predictions, and this shape may vary significantly among different properties.
We thus measured this distribution empirically for the set of properties (e.g. \textsc{lays eggs, carries malaria}; 21 in total) used in our target sentences. 
 
### Method

#### Participants

We recruited 60 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
3 participants where unintentionally allowed to do the experiment for a second time; we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
In order to alleviate the dependence of the distribution on our animal categories of interest, participants generated half of the animal categories before viewing the properties; the other half were randomly sampled from a set corresponding to the generic sentences used in Expt. 1b (e.g. \textsc{robins, mosquitos}).

Participants began the experiment by seeing a list of 6 animal kinds and were asked to list 5 of their own.
A column then appeared to the right of the animal names with a property in the header (e.g. "lays eggs").
Participants were asked to fill in each row with the percentage of members of each of the species that had the property (e.g. "50\%").
Eight property--columns appeared in the table, and this whole procedure was repeated 2 times.
In total, each participant generated 10 animal names and reported on the prevalence of sixteen properties for 22 animals (their own 10 and the experimentally-supplied 12). 
Properties were randomly sampled from a set of 21 properties associated with generics of theoretical interest, as described above.
For a full list of the properties, and generic sentences used in Expt.~1b, see Table 2 (Appendix).
The experiment can be viewed at \url{http://stanford.edu/~mtessler/experiments/generics/experiments/real-kinds/prior-2.html}.

### Data analysis and model predictions

To process the priors data, we discretize the prevalence judgments to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$, and look at the counts within each bin, after doing add-1 Laplace smoothing, as the relative probability of that prevalence. 
Using these priors, we can explore how $L_{1}(x , \theta \mid u)$, the pragmatic listener model, interprets a generic utterance (Figure \ref{fig:commongenerics}a, insets). 
The prior beliefs over the prevalence of the property, $P(x)$, can also be interpreted as the pragmatic listener's posterior upon hearing the null utterance, because the null utterance has no information content.
We see the interpretation of the generic is quite variable across our empirically measured priors.
For instance, in the case of \textsc{carries malaria}, the prior is very left-skewed; here, the threshold $\theta$ can plausibly be quite low while still being informative, since a low threshold still rules out many possible alternative kinds (and their corresponding degree of prevalence).
Properties like \textsc{doesn't attack swimmers} are very right-skewed; here, even a relatively high threshold would not result in an informative utterance (intuitively, as not many kinds would be ruled out), and so the generic is unlikely to be used by speaker $S_2$ unless the property is practically-universal within the target category. 
Some properties have priors that are unimodal with low variance (e.g. \textsc{is female}); these properties are present in every kind in almost exactly the same proportion and thus are too obvious and certain to allow for an informative generic utterance: The posterior is not very different from the prior. 
With $P(x)$ now empirically established, we can test if our speaker model predicts human truth judgments of generic statements about these properties.

## Experiment 1b: Truth Judgments

### Method

#### Participants

We recruited 100 participants over MTurk. 
4 participants were excluded for failing to pass a catch trial.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

Participants were shown thirty generic sentences in bare plural form, one after another.
They were asked to press one of two buttons (randomized between-participants) to signify whether they agreed or disagreed with the sentence (see Table 2 in Appendix for complete list). 
The thirty sentences were presented in a random order between participants and covered a range of conceptual categories described above.
Approximately 10 true, 10 false, and 10 uncertain truth-value generics were selected.

As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
4 participants were excluded for failing this trial.

#### Data analysis and results

As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
This was a significant predictor of the empirical truth judgments: true generics were significantly more likely to be agreed with than the indeterminate generics ($\beta = 3.14; SE = 0.15; z = 21.5$), as revealed by a mixed-effect logistic regression with random by-participant effects of intercept.
Indeterminate generics were agreed with \emph{less} likely than chance ($\beta = -0.49; SE = 0.09; z = -5.3$) but significantly more than false generics ($\beta = 2.09; SE = 0.14; z = 14.5$).

From the prevalence prior data (Expt.~1a), we estimate participants' beliefs about the prevalence of a property \emph{for a given kind} (e.g.~the percentage of \textsc{robins} that \textsc{lay eggs}; see green intervals on Figure \ref{fig:commongenerics}a insets, and Table 2 in Appendix).
As a simple baseline hypothesis, we first explore whether these prevalence values themselves predict generic endorsement (e.g.~does the fraction of \textsc{robins} that \textsc{lay eggs} predict the felicity of \emph{Robins lay eggs}?).
We find a little over half of the variance in truth judgments data is explained this way ($r^2 = 0.599$; MSE=0.065; Figure \ref{fig:commongenerics}b, right). 
This is not surprising given that our stimulus set included generics that are true with high-prevalence properties (e.g. \emph{Leopards have spots.}) and  generics that are false with low prevalence properties (e.g. \emph{Leopards have wings.}). 
However, large deviations from an account based purely on target-category prevalence remain: Generics in which the target-category has intermediate prevalence (prevalence quartiles 2 and 3: $ 20\% < prevalence < 64\%$), are not at all explained by prevalence within those categories ($r_{Q2,3}^2 = 0.029$; MSE = 0.110).

The pragmatic speaker model, $S_2$ in Eq.~\ref{eq:S2}, predicts an endorsement probability for a generic sentence, given prior beliefs about the property, $P(x)$, and a prevalence-level,  $x$, within the kind-of-interest. (That is, $S_2$ provides a model of whether someone who knows $x$ would say the generic to someone who doesn't, but shares prior $P(x)$; as discussed above we adopt this as a model of agreement judgments.)  
We use the empirically estimated within-kind prevalence as the $x$ that the speaker $S_2$ is trying to communicate, and use the empirically measured priors from Expt.~1a as the listener's prior $P(x)$. 
The $S_2$ model is then fully specified after setting the speaker optimality parameters $\lambda_1$ and $\lambda_2$ (in Eqs.~\ref{eq:S1}, \ref{eq:S2}).
%In order to get estimates of the $S_2$ model's predictive probability for endorsing generic sentences, we performed a bootstrap procedure on the empirical prior data. For each sample of the bootstrapped prior, we performed a Bayesian data analysis to infer the likely values of the model parameters. 
Rather than fitting the parameters, we inferred them and integrated over their plausible values using Bayesian data analysis [@LW2014]. We put uninformative priors over these parameters, with a range consistent with previous literature using the same model class: $\lambda_1 \sim \text{Uniform}(0,20)$, $\lambda_2 \sim \text{Uniform}(0,5)$.
We learn about the \emph{a posteriori} credible values of our model parameters by collecting samples from MCMC chains of 10,000 iterations removing the first 5,000 iterations, using the Metropolis-Hastings algorithm. 

The above analysis provides a single estimate for model predictions, but it is based on noisy empirical measurements of $P(x)$. In order to estimate the impact of this empirical noise on our model predictions, we resampled the prior data (with replacement) for 57 participants worth of data, discretizing and binning as we did above and then inferring parameters.
This procedure (re-sample prior, discretize and bin, infer parameters) was repeated 500 times to bootstrap the model predictions.
The Maximum A-Posteriori (MAP) estimate and 95\% Highest Probability Density (HPD) interval for $\lambda_1$ is 0.5 [0.004, 12.7] and $\lambda_2$ is 1.7 [1.3, 2.1].
^[
The fact that $\lambda_1$ is credibly less than 1 suggests that the generic utterance may be more costly than "staying silent" (our model assumes equal cost).
We maintain using only the two $\lambda$ parameters in the model for simplicity.
]

We compare the model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
(The posterior predictive distribution marginalizes over the inferred parameter values to produce predictions about what the data \emph{should look like} given the pragmatics model and the observed data. 
This is akin to fitting the parameters and is the critical step in model validation: It shows what data is actually predicted by the model.) 
As we see in Figure \ref{fig:commongenerics}b, the pragmatic speaker model $S_2$, using empirically measured priors, explains nearly all of the variance in human truth judgments ($r^2=0.98$; MSE=0.003; Figure \ref{fig:commongenerics}b). 

Generics that received definitive agreement or disagreement are predicted to be judged as such by the model (corners of Figure \ref{fig:commongenerics}b), including items for which target-category prevalence is not a good indicator of the acceptability (e.g. \emph{Mosquitos carry malaria}, for prevalence quartiles 2 and 3, $r_{Q2,3}^2=0.955$; MSE=0.005; Figure \ref{fig:commongenerics}b, intermediate shades).
We also see the generics truth judgment model predicts uncertain truth judgments: for instance, \emph{Robins are female} is judged by both the model and human participants to be neither true nor false.
\emph{Sharks don't attack swimmers}, while true of most sharks, is judged to be not a good thing to say by both participants and the model.
This is strong evidence that the puzzling flexibility of generic truth-conditions can be understood with a simple semantic theory coupled with basic communicative principles (\emph{be truthful}, \emph{be informative}) operating over diverse prior beliefs about the properties, all of which are at play in understanding language. 

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/generics-prior-prevalence-tj.pdf}
    \caption{Endorsing familiar generics. (a) 
    Prevalence prior distributions empirically elicited for twenty-one animal properties.
    Prior distributions summarized by two parameters of a structured Bayesian model: $\phi$----a property's potential to be present in a category----and $\gamma$----the mean prevalence when it is possible for the property to be present in a category.
    Inset plots display example empirical prior distributions over prevalence and corresponding $L_1$ model predictions: the posterior after hearing a generic utterance. 
    Intervals on the top of insets show human judgments about the prevalence of the property within a target category.
    (b)
    Human acceptability judgments compared with model predictions (left) and the target-category prevalence (right) for thirty generic utterances about familiar animals and properties. 
    Color denotes target-category prevalence of the property, with lighter colors indicating higher prevalence. 
     Error bars denote 95\% Bayesian credible intervals.
    }
  \label{fig:commongenerics}

\end{figure}



## Extended Analysis of Priors: Conceptual Structure 

The pragmatics model only has a prevalence-based semantics yet it is able to explain the flexibility in truth judgments for a diverse range of generic statements.
Conceptual accounts of generic statements have looked beyond prevalence, to structured knowledge representations as the critical factor in generic meaning [@Leslie2007; @Prasada2013]. 
We can interrogate our formal model to see what is driving its predictions, and, in particular, we ask whether structured representations might effect our model after all.

For a given property,  the prior distribution on prevalence $P(x)$ is a single distribution.
However, the distribution may be structured as the result of deeper conceptual knowledge. 
For instance, if participants believe that some kinds have a causal mechanism that \emph{could} give rise to the property, while kinds others do not, then we would expect $P(x)$ to be structured as a mixture distribution [cf., @Griffiths2005].
We know that prior knowledge plays a fundamental role in leading the pragmatics model to endorse or reject generics. 
We now explore the hypothesis that this is at least partly because these priors are structured into two components: kinds that \emph{can} have the property and other kinds that \emph{cannot}.
We explore this possibility by formulating a mixture model for the prevalence priors, and exploring how well it fits the prior data elicited in Expt.~1a.

### Data analysis

If a kind can have the property, we assume the prevalence follows a Beta distribution with mean $\gamma$ and concentration $\xi$. 
If a kind cannot, we assume the prevalence is a Delta distribution, with all probability mass at 0\%. ^[There are other ways to formulate the second component ("the kind doesn't have a causal mechanism that would give rise to  the property") of the prior. 
It could reflect accidental causes of the property, in which case, the prevalence could be a distribution that allows for non-zero prevalence. 
While an interesting possibility, its full consideration is beyond the scope of this article.
]
The relative contribution of these two components is governed by mixture parameter $\phi$, inferred from the data.^[This is similar in spirit to Hurdle Models of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., adverse events to vaccines; @hurdleModels])]
We think of $\phi$ the \emph{potential of a property to be present in a kind} and $\gamma$ is the \emph{mean prevalence of the property among the kinds with the potential to have it}.\footnote{
We note that $\phi$ is not what other authors have described as \emph{cue validity} [@Beach1964; Khemlani2012], or $P(K \mid F)$. 
 $\phi$ is a mixture component in the prior distribution over prevalence: $P(F\mid K$). 
Cue validity and prevalence are related via Bayes Rule': $P(K \mid F) \propto P(F \mid K) \cdot P(K)$. 
}
If this model is correct, the prevalences given by participants would then be distributed as: $P(d) = \phi \cdot \text{Beta}(d \mid \gamma,\xi)+ (1 - \phi) \cdot \delta_{d=0}$. 

We performed Bayesian inference over this model, given the observed prevalence data, to examine how well the model's posterior predictive distribution reconstructs the prevalence prior data.
We put uninformative priors over all the parameters, $\phi \sim \text{Uniform}(0,1)$, 
$\gamma \sim \text{Uniform}(0,1)$, $\xi \sim \text{Uniform}(0, 50)$, 
and performed Bayesian inference separately for each property using the Metropolis-Hastings algorithm 
collecting 50,000 samples removing the first 25,000 iterations for burn-in.

### Results

Estimates of the mixture parameter $\phi$ and the mean of the "has the potential" component $\gamma$ for each property are shown in Figure \ref{fig:commongenerics}a.
We see significant diversity among our properties in both parameters, corresponding to priors over prevalence with dramatically different shapes (insets). 

Again, we look to the posterior predictive distribution to validate the structured prior model.
Using the model with its inferred parameters, we generate prevalence judgments for different properties and compare that to the empirical counts. 
We discretize the prevalence values of both the model and the data to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$.
This statistical model reproduces the prior elicitation data very well ($r^2 = 0.94$), while a model that assumes just a single generative component fails ($r^2 = 0.14$). This is strong evidence in support of a structured prior. 

The other test of this hypothesis is to re-examine the truth judgments from Expt.~1b using the pragmatics model with the inferred structured priors (as opposed to bootstrapping the raw empirical counts). 
We find the same correspondence to the empirical truth judgments data ($r^2 = 0.98$).
This provides further evidence that the prior distribution over prevalence $P(x)$ is structured.
The implication of this finding is that conceptual structure may indeed find its way into generic judgments, but via the prevalence prior, rather than directly in the semantics of the generic. We return to this idea in the General Discussion.

\begin{figure}
\centering
    \includegraphics[width=0.6\columnwidth]{figs/postPred-priorModel.pdf}
    \caption{Posterior predictive distribution of the structured, statistical model thought to give rise to the human data in the prior elicitation task. The close alignment between model and data suggests the assumption of a structured prior is warranted.}
  \label{fig:pp-priorModel}
\end{figure}



# Empirical Test 2: Interpreting Novel Generics

One of the most important roles for generic language is to provide learners information about new or poorly understood categories. 
This role depends on how unfamiliar generic sentences are interpreted [e.g., @Gelman2002; @Cimpian2010].
The pragmatic theory we present includes such a theory of generic comprehension: the listener model (Eq.~\ref{eq:L1}) describes interpretation of a generic utterance---\emph{Kind \textsc{has property}}---without previously knowing the prevalence of the property within this kind.
In our theory, the meaning is uncertain, but the pressure to be informative operates over \emph{a priori} beliefs about properties to produce an interpretation. 
Classic work in generalization suggests beliefs about the prevalence of properties differ by type of property, including relatively fine distinctions among properties that are all biological in nature [@Nisbett1983]. 
We leverage these diverse expectations, using properties that explore a wide range of \emph{a priori} beliefs about prevalence. 

Measuring \emph{a priori} beliefs is tricky when the kinds are unknown.
We cannot, as before, have participants fill out a table with rows corresponding to different animal kinds and columns corresponding to different properties:  Nothing would distinguish the rows.
Instead, we leverage the latent structure uncovered in our extended model analysis of Expt.~1 and decompose prevalence priors into 2 components: the property's potential to be present in a kind and the mean prevalence when present.

We use this novel method for measuring \emph{a priori} beliefs about the prevalence of these properties for unfamiliar kinds (Expt.~2a).
We then test the predictions of the pragmatic listener model $L_1$ using these empirically derived priors against human \emph{interpretations} of novel generic sentences (Expt.~2b).
Finally, we explain a previously reported empirical asymmetry between truth conditions and interpretations by comparing the speaker $S_2$ and listener $L_1$ models in the same experimental context (Expt.~2c).

## Experiment 2a: Prevalence Priors for Unfamiliar Kinds

### Method

#### Participants

We recruited 40 participants over MTurk.  
All participants were native English speakers. 
The experiment took about 5-7 minutes and participants were compensated \$0.75.

#### Procedure and materials

We constructed forty different properties to explore a wide range of \emph{a priori} beliefs about prevalence. 
These items make up four categories of properties: body parts of a particular color (e.g. \textsc{has green feathers}), described vaguely (e.g. \textsc{has small wings}), in accidental or disease states (e.g. \textsc{has wet fur}, \textsc{has swollen ears}), and without modification (e.g. \textsc{has claws}).
Because pilot testing revealed more variability for items in the accidental category relative to the other types of properties, we used twice as many exemplars of accidental properties, yielding a more thorough test of the quantitative predictive power of the $L_1$ interpretation model. 
We used 8 exemplars of each of the three non-accidental properties ("parts", "colored parts", "vague parts"), and 16 exemplars of accidental properties, building on a stimulus set from @Cimpian2010.
All materials are shown in Table 3 in the Appendix.

In the task, participants were introduced to a "data-collection robot" that was tasked with learning about properties of animals. 
Participants were told the robot randomly sampled an animal to ask the participant about (e.g. The robot says: "We recently discovered animals called feps."). 
We then used a two-stage elicitation procedure, aimed to measure the two components of the structured prior model: (1) the potential of the property to be present in a kind and (2) the expected prevalence when present.
To get at (1), the robot asked how likely it was that "there was \emph{a} fep with \textsc{property}" (potential to be present), to which participants reported on a scale from "unlikely" to "likely".
For example, it is very likely that there is a fep that is female, less likely that there is a fep that has wings, and even less likely that there is a fep that has purple wings. 
To get at (2), the robot then asked, "Suppose there is a fep that has wings. What percentage of feps do you think have wings?" (expected prevalence when present). 
Participants completed a practice trial to make sure they understood the meanings of these two questions.

### Data analysis and results

We used the same structured, statistical model for the prior data from Expt.~1.
The only difference from Expt.~1a. is that our experimental data comes from inquiring about the parameters of the priors directly, as opposed to asking about particular samples from the prior (i.e. particular kinds) as was done in Expt.~1a. 
We assume these two measurements follow Beta distributions ($d_{potential} \sim \text{Beta}(\gamma_{1}, \xi_{1})$; $d_{expected} \sim \text{Beta}(\gamma_{2}, \xi_{2})$), and construct single prevalence distributions, $P(x)$, by sampling from the posterior predictive distribution of prevalence as we did before: $P(x) = \int [ \phi\cdot \text{Beta} (x \mid \gamma_{2}, \xi_{2}) + (1 -  \phi) \cdot \delta_{x=0} ] \cdot \text{Beta}(\phi \mid \gamma_{1}, \xi_{1}) d\phi$.
We used the same uninformative priors over parameters $\phi, \gamma_{i}, \xi_{i}$ as in Expt.~1a.

Figure \ref{fig:prior2}a shows a summary of the elicited priors, in terms of the diversity of $d_{potential}$ and $d_{expected}$.
Biological properties are expected to be \emph{a priori} more prevalent within a kind when present than accidental properties, with additional fine-grained differences within biological and accidental properties.
Like the priors elicited using familiar categories, these priors elicited using unfamiliar categories have diverse shapes (see insets). 
Biological properties ("biological", "vague", and "color" body parts) have prevalence distributions that are bimodal with peaks at 0\% and near-100\% prevalence. 
Interpretations of generics about these properties ($L_1$ model, Eq.~\ref{eq:L1}) update these distributions to concave posteriors peaked at 100\% (Figure \ref{fig:prior2}a; red, blue and green insets); the model predicts these novel generics will be interpreted as implying the property is widespread in the category.
By contrast, accidental properties (both "rare" and "common") follow unimodal prior distributions and update to convex posterior distributions, predicting weaker and more variable interpretations of novel generics for these properties. 

\begin{figure*}
\centering
    \includegraphics[width=\columnwidth]{figs/prevalence-implied-wPriors}
    \caption{Understanding novel generics. (a) Prevalence prior distributions empirically elicited for 40 animal properties.
    Parameters of the structured statistical model---$\phi$ and $\gamma$---reveal quantitative differences in beliefs about the prevalence of conceptually different types of properties (scatterplot). 
    Inset plots show differences in shapes between biological properties (red, green, blue; bimodal) and accidental properties (orange, purple; unimodal).   
  These differences in the prior (darker shade) give rise to the variability of $L_1$ interpretations of generic utterances (lighter shade).
  (b)
  Human interpretation of prevalence upon hearing a generic compared with the $L_1$ model posterior predictive. 
    Participants and the model interpret generics differently for different property types: Generics of biological properties (red, blue, green) have  strong interpretations while generics of accidental properties (purple, orange) are weaker. 
      Error bars denote Bayesian 95\% credible intervals.
  }
  \label{fig:prior2}
\end{figure*}


## Experiment 2b: Interpretations of Novel Generics

Our model of generic interpretation, the pragmatic listener model $L_1$ (Eq.~\ref{eq:L1}), predicts that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
Here, we test the degree to which the predictions based on the empirically elicited prevalence priors for 40 items (from Expt.~2a) match human judgments of how the widespread the property is upon hearing a generic.

### Method

#### Participants

We recruited 40 participants over MTurk to determine how widespread different properties are believed to be upon hearing a novel generic.  
The experimental design is very similar to @Cimpian2010, and we chose to have a sample size at least twice as large as the original study (original n=15). 
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with the generic (e.g., "Feps have yellow fur.") and asked to judge prevalence: "What percentage of feps do you think have yellow fur?". 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental (described in Expt.~2a).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 

### Analysis and results

The pragmatic listener $L_1$ model provides posterior beliefs about prevalence, given prior beliefs and a generic utterance.
This model has one parameter governing the optimality of the hypothetical speaker $S_1$ in Eq.~2. 
We put the same uninformative prior over this parameter as previously: $\lambda_1 \sim \text{Uniform}(0, 20)$.
We learned about the parameter's \emph{a posteriori} credible values by running 3 MCMC chains of 100,000 samples (removing 50,000 for burn-in) using the Metropolis-Hastings algorithm.
The MAP and 95\% credible interval for $\lambda_1$ are $14.8 [6.4, 19.9]$.

We look at the posterior predictive distribution of $L_1$, integrating out the model parameter.
We first explore two important trends predicted by the pragmatic listener model.
In Figure \ref{fig:exp2b} (solid lines) we see the implied prevalence judgments are predicted (at the property class level) to vary linearly with the \emph{a proiri} expected prevalence. 
A mixed-effects linear model with random by-participant effects of intercept and slope indeed reveals the more prevalent a property is expected to be \emph{a priori}, the stronger the implications of a generic statement ($\beta = 0.57; SE = 0.08; t(39) = 7.12; p < 0.001$).
The prevalence implied by a generic is also predicted to be greater than the \emph{a proiri} expected prevalence (i.e., greater than the prevalence expected among the kinds with the potential to have the property).
A mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the \emph{a priori} prevalence ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$).
As for the quantitative accuracy of the model, on a by-item level, the pragmatic listener model predictions closely align with the human judgments of prevalence for novel generics ($r^2(40)=0.94$, MSE=0.002).
Human participants and our model display the same sensitivity of generic interpretation to details of the property (Figure \ref{fig:prior2}b). 
We now have strong support for both of the major predictive components of our model: generic endorsement, modeled as a speaker $S_2$, and generic interpretation, modeled as a listener $L_1$.

## Experiment 2c: The Asymmetry Between Truth Conditions and Interpretations

There is a surprising d\'{e}colage between the truth conditions and interpretations of generic language: Interpretations are often strong while truth conditions are flexible. 
@Cimpian2010 found that upon reading a generic (e.g. \emph{Glippets have yellow fur.}), participants infer (in an \emph{implied prevalence} task) that the property is widespread (e.g. almost all glippets have yellow fur).
By contrast, participants endorse generics (in a \emph{truth conditions} task) for a wide range of prevalence levels (e.g. even when "30\% of glippets have yellow fur."), thus showing an asymmetry between truth conditions and implied prevalence. 
However, this mismatch is not found for the behavior of quantified statements involving "all" or "most," and 
is significantly reduced for generics of accidental properties  (e.g. \emph{Glippets have wet fur.}).

Below we replicate the basic asymmetry findings of @Cimpian2010 and reveal even more variability in the mismatch between \emph{truth conditions} and \emph{implied prevalence} using the expanded stimulus set from Expt.~2a.
In addition, we now test both our models (generic endorsement [speaker $S_2$] and generic interpretation [listener $L_1$]) in the same experimental paradigm. %models predicts the asymmetry and the context-sensitivity of this phenomenon.

%Expt.~2b revealed how generics imply the property is more widespread than what would be expected \emph{a priori}. 
%Comparing to the average prevalence required to assent is another way to explore how generics exaggerate the evidence. 

### Method

We re-analyze the data from Expt.~2b as the \emph{implied prevalence} data.
The following paradigm is to measure the corresponding \emph{truth conditions}.

#### Participants

We recruited 40 participants over MTurk.  
All participants were native English speakers. 
None of the participants completed Expt.~2b (interpretations of novel generics).
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

The cover story and materials were the same as in Expt.~2b.
On each trial, participants were given a statement about a property's prevalence within a novel kind (e.g. \emph{50\% of feps have yellow fur.}). Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. \emph{Feps have yellow fur.}). Prevalence varied between 10, 30, 50, 70, and 90\%.

The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Expt.~2a (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

### Analysis and results

For both behavioral data and model predictions (Eq.~\ref{eq:S2}) we computed the average prevalence that led to an assenting judgment (the \emph{average prevalence score}), for each property type and participant, following the procedure used by @Cimpian2010.
For example, if a participant agreed with the generic whenever the prevalence was 70\% or 90\% and disagreed at the other prevalence levels, that participant received an \emph{average prevalence score} of 80\%.

For our pair of models, there are two parameters (the two speaker optimality parameters).
We infer them using the same Bayesian data analytic approach as before. 
The MAP and 95\% HPD intervals for $\lambda_1$ is $19.5 [10.5, 19.9]$ and $\lambda_2$ is $0.4 [0.34, 0.49]$.
We then subjected the generic endorsement model to the same procedure as the human data. % subjected our model to the same procedure. 
The speaker model $S_2$ returns a posterior probability of producing the generic, for each level of prevalence. 
We sample a response (\emph{agree} / \emph{disagree}) from this posterior distribution for each prevalence level, simulating a single subject's data.
As with the human data, we took the trials where the model agreed with the generic, and took the mean of the prevalence levels corresponding to those trials, giving us the average prevalence at which the model assented to the generic.
We repeated this for each type of property 40 times to simulate a sample of 40 participants. 
We repeated this procedure 1000 times to bootstrap 95\% confidence intervals.

The generic endorsement model (speaker $S_2$) predicted that \emph{average truth conditions} should not vary appreciably across the different types of properties, consistent with the fact that generics are acceptable for broad range of prevalence levels for all property types.
A similar absence of a gradient was observed in the human data ($\beta = 2.82; SE = 4.02; t(39) = 0.70; p = 0.49$; Figure \ref{fig:exp2b}, dotted lines). 
Interpretations of generic utterances are stronger than their average truth conditions for the biological properties but not for the accidental properties (Figure \ref{fig:exp2b}) with both human data, replicating @Cimpian2010, and the model; the extent of the difference is governed by prior property knowledge (mean prevalence when present $\gamma$, from Expt.~2a).
The listener and speaker pair of models predicts human endorsements and interpretations of novel generic utterances well ($r^2(10) = 0.87$, MSE = 0.008).
Thus, our model predicts that the asymmetry between truth conditions and implied prevalence should hold, but only for properties with the most extreme prior beliefs.

\begin{figure*}
\centering
    \includegraphics[width=\columnwidth]{figs/unfamiliar-asymmetry-predictive-data.pdf}
    \caption{The asymmetry between truth conditions and interpretations. Human judgments and model predictions of prevalence implied by novel generic utterances (implied prevalence task; solid line) and average prevalence that leads to an acceptable generic utterance (truth conditions task; dotted line) as it relates to the \emph{a priori} mean prevalence when present $\gamma$.
    Expectations of prevalence are higher after hearing a generic than before hearing it (solid line compared to $y=x$ line; both for human data and model).
    Generic statements about biological properties, imply that the property is widespread in the category, for both human participants and the model (solid line: red, blue and green). 
    Generics about accidental properties do not result in such a high implied prevalence (solid line: purple and orange).  
	While the implications of generic utterances are highly variable across the different types of properties, the average prevalence that leads to an acceptable generic does not vary, for participants nor the model.
}
  \label{fig:exp2b}
\end{figure*}


# Prevalence is a Predictive Probability

So far, we have shown that property prevalence is sufficient to formalize the semantics of generic statements as an underspecified scalar denotation.
But what is property prevalence?
If generic language is truly conveying generalizations, it would be useful for it to reflect expectations, not just the current statistics in the world.
The current frequency of a property is often a good indicator of future frequency, yet statistics can be distorted by spurious events.
The causal history of a property may be more or less important for implying the property will be present in future situations.
Does generic language communicate prevalence in terms of past frequency or future expectations?

To answer this, we adopt an experimental paradigm used by @Gelman2007 to show that generic language is sensitive to theory-based considerations.
In the original paradigm, participants are told a story about a novel creature (e.g. \emph{dobles}) and a property of that kind (e.g., \emph{having claws}).
Participants are then either told that the creature was born with the property or that it acquired the property through extrinsic means (e.g., by finding claws and putting them on). 
Then, participants are told about an event that either causes the property to disappear (e.g., they drank a chemical and their claws fell off) or that leaves the property intact, and are asked whether or not the generic (e.g. \emph{Dobles have claws}) applies.
The original finding was that adult judgments were sensitive to the origins of the property (i.e., born vs. acquired), and insensitive to the outcome of the event (i.e., property maintained vs. lost): Participants fully-endorsed the generic when it was inborn, and rejected it when it was acquired, regardless of the current prevalence of the property.

In Experiment 3a, we use the same basic paradigm to measure \emph{predictive prevalence}: participants' expectations about future instances of the kind.
We explore the predictions of our truth judgments model, assuming that \emph{predictive prevalence} is what is being communicated.
In Experiment 3b, we use a truth judgment task similar to @Gelman2007 and compare participants' judgments to the model's predicted endorsements.

## Experiment 3a: Predictive Prevalence Elicitation

The design of this experiment is based on a study reported in @Gelman2007 with some slight modification.
%The original study was done on 14 undergraduates.

### Method

#### Participants
We recruited 80 participants over MTurk.  
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

On each trial, participants read a vignette about a novel creature. For instance,
\begin{quote}
These are dobles. [picture of 10 dobles with claws] Here is how they grew. They grew up with claws. First they were born, then they got bigger, then they were full size. [picture of a doble with claws, getting bigger and bigger; in some vignettes, the animal was first shown hatching out of an egg with the relevant property already visible] Then one day they drank a bad chemical. They got very sick and this is how they looked. [picture of 10 dobles without claws]
\end{quote}
The trial proceeded by participants reading the text, and clicking a button to continue to the next part of the story (at which time, the images changed according to the example above).

Participants saw 4 trials: 2 in which the creatures are born with the property (\textbf{intrinsic origins}), and 2 in which the creatures are shown discovering and acquiring the property (\textbf{extrinsic origins} e.g., painting themselves brown).
This was crossed with either the creatures drinking a "bad chemical" and losing the feature, or drinking a "yummy drink" and maintaining the feature.
The outcome of this event determined the final presentation of images that the participant saw (e.g., either 10 dobles with claws or 10 without).

While this final screen was present, we measured \emph{predictive prevalence} by telling participants: "A new doble was born today. When it becomes full grown, how likely is it that it would have claws?"
Participants responded using sliders ranging from "very unlikely" to "very likely".

We used 2 different types of properties: colors (e.g. \emph{Lorches are green.}) and body parts (e.g. \emph{Dobles have claws}).
For each type of property, there were approximately 8 different exemplars (different colors or different body parts for different creatures).
The creatures were either birds, bugs, or fish, with randomly sampled physical dimensions (e.g., sizes of body or tail).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/predictive/predictive-elicitation-1-elicitation.html}.


### Results and truth judgment predictions

The average predicted prevalences for the 4 experimental conditions are shown in Table \ref{tab:predictive}.
We observe a main effect of origins, such that when participants read that the creatures had the property from birth, future creatures  are much more likely to have the property as compared to when the property is acquired.
We see that, in our paradigm, participants are also sensitive to the outcome of the event. %This is surprising because @Gelman2007 observed no effect of the event outcome on generic endorsement.
When participants observe a creature who loses the property by drinking a chemical, they report future members of the category are less likely to have the property.
This inference may be driven by inferences about the property (e.g., that the property could be an unstable property, if you can lose it simply by drinking something) or by inferences about the event (e.g., participants may believe this "chemical drinking" event is a relatively normal event, and thus it could happen in the future).

\begin{table}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
Origins & Event Outcome & Collapsed Mean [95\% CI]  & Color  [95\% CI] & Body Part [95\% CI] \\
\hline
Extrinsic & Lost  & 0.15 [0.10, 0.21]& 0.13 [0.07, 0.21] & 0.17 [0.09, 0.26] \\
\hline
Extrinsic & Maintained & 0.32 [0.24, 0.39] & 0.24 [0.16, 0.35] & 0.38 [0.27, 0.50] \\
\hline
Intrinsic & Lost  & 0.69 [0.62, 0.76] & 0.73 [0.63, 0.83] & 0.66 [0.56, 0.75] \\
\hline
Intrinsic & Maintained  &0.95 [0.94, 0.97] & 0.96 [0.94, 0.98] & 0.94 [0.91, 0.97] \\
\hline
\end{tabular}
\caption{Predicted prevalence for the four experimental conditions of Expt.~3a. Right two columns show the summaries broken down by the two types of properties used in the experiment.}
\label{tab:predictive}
\end{table}

We use these predicted probabilities as the prevalence $x$ that the speaker model is trying to communicate: $S_2(u\mid x)$, and examine the model's predicted truth judgments.
We explore the model's predictions for each origin and event outcome, as well as when the data is split by property type (color vs. body parts). 
For priors $P(x)$, we use the body part and color priors elicited in Expt.~2a.
We see that the model predictions track closely the predicted prevalence (Figure \ref{fig:dobles}a, top, compare with predicted prevalence in Table \ref{tab:predictive}).
This is because both color and body part priors are relatively broad, and hence when the property is (predicted to be) more prevalent, the generic has a higher probability of applying (see schematic predictions from Figure \ref{fig:schematic-unif} "have wings" for comparison).
We also see that the model predicts a subtle by-item difference, such that the influence of the event outcome (lost or maintained) on generic endorsement is predicted to be stronger for body parts than for color terms (Figure \ref{fig:dobles}a, bottom).
This prediction is mostly due to the predicted prevalence for the conflict conditions (intrinsic-lost and extrinsic-maintained) being subtly different (Table \ref{tab:predictive}, right-most columns).

<!--
%\footnote{Note that our model's predictions do not deviate substantially from the predicted prevalence because there are only two different priors being used, and the shapes of those distributions do not vary appreciably (see Figure \ref{fig:prior2}). \ndg{huh? two priors does not explain why the model tracks predicted prevalence. instead it's that the priors are broad and unimodal or something right?}
%}.
-->

Our model thus makes two novel predictions for generic endorsement in the paradigm by @Gelman2007.
We predict that in addition to the main effect of origins, we should see a second main effect of event outcome.
Second, we predict that this effect should be slightly stronger in the case of color properties than in the case of body part properties.


## Experiment 3b: Truth Judgment Task

In this experiment, we test the predictions of our model using a truth judgment measure in the same paradigm.

### Method

#### Participants
We recruited 80 participants over MTurk.  
The experiment took about 3 minutes and participants were compensated \$0.35.
None of the participants completed Experiment 3a as well.

#### Procedure and materials
The procedure and materials are exactly the same as in Expt.~3a, with the exception of the dependent measure.
After reading each vignette, participants were asked: "Do you agree or disagree that: \textsc{generic statement} (e.g. Dobles have claws)".
Participants responded by choosing one of two radio buttons corresponding to agree or disagree.

### Results and discussion

<!--
%                                         Estimate Std. Error z value        Pr(>|z|)    
%(Intercept)                               -2.9444     0.5130  -5.740 0.0000000094624 ***
%event_outcomemaintained                    2.6931     0.5603   4.807 0.0000015342262 ***
%originsintrinsic                           3.6753     0.5658   6.496 0.0000000000824 ***
%event_outcomemaintained:originsintrinsic   0.2396     0.9400   0.255           0.799    
-->

Our pragmatics model with the elicited predicted prevalence from Expt.~3a made two novel predictions for this experiment: (1) in addition to a main effect of origins, we would find a main effect of event outcome; (2) this effect would be stronger for body part properties than for color properties.
As predicted, we found two main effects (Figure \ref{fig:dobles}b, top).
The main effect of property origins replicated ($\beta=3.6, SE=0.57, z=6.5, p<1\text{e}10$): participants were more likely to endorse the generic when it was about a property that the creature was born with.
In addition, we find a second main effect of event outcome 
 ($\beta = 2.69, SE = 0.56, z=4.8, p<1\text{e}5$): participants were more likely to endorse the generic when the property was maintained than when it was lost.\footnote{The fact that we find a second main effect of event outcome in addition to origins, whereas the original only found a main effect of origins, makes it worth noting the differences between our paradigm and the original study by Gelman \& Bloom.
In the original study, the first sentence of each vignette used the possessive "my": "These are my dobles.".
At the end of each vignette, the original study had participants judge two statements in counterbalanced order: "Do my dobles have claws?" and "Do dobles have claws?"
Finally, the original sample size was 14; ours was 80. 
}


When we break down the results by item, we see that this effect is  stronger for body part properties than for color properties (Figure \ref{fig:dobles}b, bottom).
The endorsement of a generic for color properties (e.g. \emph{Lorches are green}) seems to be less sensitive to the outcome of the event (i.e. Lorches losing their color as a result of drinking a chemical).
This may be due to participants' intuitive theories of properties and their stability (skin color is more stable than body parts like feathers).
Indeed this difference is apparent in the predictive prevalence task (Expt.~3a).
For the 8 data points of generic endorsement based on origins, outcome, and property type, our model's predictions match the data well ($r^2(8) = 0.96$).
We, thus, elaborate our theory:
The semantics of generics can be understood as a threshold on property prevalence, and this prevalence is a speaker's subjective belief about what is likely to be the case in the future.

\begin{figure*}
\begin{tabular}{l l}
(a) $S_2$ model predictions & (b) Human endorsement of generic statements \\
\\
\centering
    \includegraphics[width=0.5\columnwidth]{figs/dobles-model.pdf} & 
        \includegraphics[width=0.5\columnwidth]{figs/dobles-results.pdf} \\
      \includegraphics[width=0.5\columnwidth]{figs/dobles-model-byItem.pdf} & 
      \includegraphics[width=0.5\columnwidth]{figs/dobles-byItem-results.pdf} \\

\end{tabular}
    \caption{
    Prevalence is a predictive probability.
    (a) Truth judgment model predictions given the predicted prevalence elicited in Expt.~3a.
    (b) Average endorsement of the generic statement in Expt.~3b (replication of Gelman and Bloom, 2007).  
    Bottom row shows data and predictions broken down by property type.
  }
  \label{fig:dobles}
\end{figure*}

# General Discussion

It is a remarkable fact that so much is learned from ideas expressed vaguely in words.
Generalizations in language (e.g., *John runs.*, *Dogs are friendly.*, *The block makes the machine play music*) are a premier example of how simple statements --- statements understood by even the youngest language users --- can display complex sensitivity to context.
We have argued that the meaning of such linguistic expressions is, in fact, simple but underspecified. 
To our knowledge, this is the first formal theory of genericity in language that makes accurate and precise quantitative predictions about human behavioral data. 
It is also the first attempt in psychology to try to unify significant swaths of language that are seemingly quite different from one another: generalizations about events, causes, and categories.
What these expressions have in common is *genericity*, the fact that these statements convey generalizations. 
Indeed we have been able to capture the context-sensivity of these statements by positing a single underlying scale defined by the *propensity*, or probability (Expts. 1, 3, \& 4).
This scale is crucially defined by a subjective, predictive probability, not mere frequency (Expt. 2).
We have shown that listeners' prior knowledge about this scale is causally related to their endorsement of statements of genericity (Expt. 3).
Finally, our framework naturally accounts for a number of well-documented phenomena from cognitive and developmental psychology concerning *generic language*, or generalizations about categories (Expt. 5).
In addition to unifying seemingly disparate parts of language, this paper provides a theoretical framework for asking further questions about genericity, which we outline below.

## The Comparison Class and the Question Under Discussion

In this paper, we proposed a model for understanding generalizations in language that relies upon interlocutors' shared beliefs over the statistics of the event or propertyn under discussion.
We constructed the prior belief distribution for the propensity of an action, cause, or property by considering other possible people doing the action, other possible causes producing the effect, or other possible categories having the property.
These other people, causes, and kinds form *comparison classes* against which the target is evaluated. 
$P(x)$ is always relative to a class of other entities or events, a comparison class $K$: $P_K(x)$.

The existence of comparison classes is uncontroversial in the study of *vague language* [e.g., gradable adjectives like *tall* and vague quantifiers like *many*; @Bale2011; @Solt2009].
Adult judgments of the felicity of gradable adjectives like *tall* or *dark* depend upon fine-grained details of the statistics of the comparison class [@Qing2014; @Schmidt2009; @Solt2012].
We have shown similar sensitivity to the statistics of comparison class in the language of generalizations, thus connecting generic, habitual, and causal language to the study of *vague language*. 

In this work, we made the design decision to construct the comparison class with respect to other people, causes, or kinds.
In this way, the statement of genericity is addressing the implicit Question Under Discussions (QUDs): *who \textsc{does action}?*, *what causes \textsc{effect}?*, or *what \textsc{has feature}?*, respectively.
In the minimal contexts employed in our experiments, this is a reasonable assumption.
There is a different assumption we could have made, however: Another clear way to construct the comparison class, with respect to other features. 
This *feature-wise* comparison class would correspond to addressing the implicit QUDs: *what does \textsc{person} do?*, *what effects does \textsc{cause} bring about?*, and *what features does \textsc{kind} have?*

Statements of genericity can be used to address either QUD, and this difference can even be observed the same sentence.
"Lawyers care about the law." can be interpreted with a category-wise comparison class (i.e., *Lawyers (as opposed to doctors, firefighters, ...) care about the law.*; say, in a pedagogical context) or a feature-wise comparison class (i.e., *Lawyers care about the law (as opposed to justice, ethics, ...)* e.g., said sardonically).
We chose to focus in this work on the category-wise comparison class for methodological convenience. 
In our three case studies, the *category-wise* comparison classes are fixed to *other people*, *other causes*, or *other kinds*, even as we vary the features.
*Feature-wise* comparison classes are likely to be modulated by the feature itself.
Future work should address the property-wise reading of generalizations, and explore what cues lead listeners to one or the other interpretation.


<!-- Throughout our experiments in this work, we have focused on sentences about animals.  -->
<!-- In addition to being the main focus of past theoretical and empirical work, focusing on animals is methodologically convenient as the comparison class for generics about animals is quite naturally \emph{animals}. -->
<!-- When we look beyond generics about animals, deciding what goes into a comparison class becomes less clear. -->
<!-- There are some hints that the comparison class can be derived with respect to the property [@Keil1979], but may involve pragmatic reasoning as well.  -->
<!-- For example, the statement "iPhones are useful" could be in comparison to other forms of technology (like a desktop computer), while "iPhones are heavy" could really only be informative relative to other handheld devices. -->

<!-- The incorporation of a comparison class into the study of generic language might help elucidate other puzzles concerning generics. -->
<!-- Recent work in philosophy and linguistics, for instance, suggest generic language is context-sensitive [@Nickel2008; Sterken2015]. -->
<!-- @Nickel2008 points out that \emph{Dobermans have floppy ears} may be true in the context of a discussion of evolutionary biology but that \emph{Dobermans have pointy ears} is true in a discussion of dog breeding. -->
<!-- Our theory provides a hint from where to begin to understand this context sensitivity: the comparison class. -->
<!-- Different conversational contexts could bring to mind different comparison classes, in a way analogous to the context-sensitivity of  gradable adjectives (e.g., \emph{tall}).  -->
<!-- Hearing that "Abigail is tall" means different things is Abigail is 20 years old or if she is 4.  -->
<!-- Future work will be needed to explore whether a pragmatic inference approach is also relevant to establishing the comparison class, and what background knowledge about properties, categories, and context is relevant. -->

## Acquiring the Language of Generalizations

The linguistic outlet for generalizations about categories---so called, *generic language*---has received tremendous attention from psychologists, linguists, and philosophers.
Generic language is one of the earliest emerging forms of complex, compositional language (CITE).
Somewhere between 2 and 3 years of age, children recognize that generics convey a generalization about a category, not directly tied to concrete instances in a scene [@Cimpian2008].
More on development.

What is perhaps surprising from a formal perspective is that generic language is far from trivial to characterize. 
If generics convey something about the prevalence of feature, we would expect them to be in some way comparable to quantifier statments (e.g., "some", "most", "all", ...).
Rehash linguistic puzzles.
The extreme flexibility of generic meaning stands in stark contrast to its early emergence in development.
In fact, quantified statements, whose formal meaning is much more straight-forward, emerge much *later* in development.
This has led some to conclude that the normal tools for describing the semantics of quantified utterances (namely, a truth-functional threshold) will not work for generic language [@Leslie2008].

This may be throwing out the baby with the bathwater.
When we consider the acquisition problem, there are two aspects of meaning that a language learner must acquire for the semantics of a quantified utterance: (1) that the meaning is a threshold-function (i.e., $\denote{u}(p, \theta) = \{p :p > \theta\}$ and (2) the fixed value of the threshold (e.g., $\theta = 0$ for "some", $\theta = 0.5$ for "most").
If there is no fixed value of the threshold, then a uniform prior over the threshold in the truth-functional threshold semantics is mathematically equivalent to a *soft semantics* wherein the degree to which the utterance is true is *the degree itself*, normalized by the prior distribution over the degree (i.e., the comparison class). 

$$
\int_{0}^{1} \delta_{p > \theta} \diff \theta =  p
$$

Given this mathematical equivalence, we can imagine the *more is better* soft semantics is what is acquired early.
The difficulty in acquiring the meaning of quantifiers is then a difficulty in recognizing a fixed-threshold semantics as a special case of this *more is better* sematnics.
This special case involves learning the value of the threshold (aspect 2) and not in recognizing that the utterance conveys something about the quantity (e.g., aspect 1).

## Communicating Predictive Probabilities

This paper puts forth the theory that the simplest language of abstractions (i.e., the language of generalizations) communicates predictive probabilities from speaker to hearer. 
This might seem contrary to a basic tenet in cognitive psychology, that human cognition falters when reasoning about probabilities [@Tversky1974].
In a multitude of demonstrations, Kahneman and Tversky observed that human reasoners perform poorly when reasoning about explicit probabilities.
This has been taken to imply that people are bad with probabilties.

We suggest that problems with communicating probabilities are a problem of the language used to convey *explicit* probabilties [cf., @Levinson1995]. 
A host of development evidence suggests that even the youngest learners are actually quite good about reasoning about probabilities [@Gweon2010; @Dewar2010].
In this work, we have shown that probabilities are central to even the most basic linguistic expressions.
Indeed they are causally related to communicating generalizations, as evidenced by Expt. 3.
We convey probabilities to each other using language, but the most basic ways we have of conveying them are *vague*. 



## Relationship to Other Theories

To our knowledge, this is the first formal theory of genericity in language that makes accurate and precise quantitative predictions about human behavioral data. 
A number of other theories of genericity have been proposed in the linguistics and psychological literature. By and large, these theories are concerned with explaining *generics* or generalizations about categories, so in keeping with the spirit of these authors, we will refer to the object of explanation, without loss of generality, as *generics*.

Semantic theories fall into one of two broad camps: Those that appeal to the statistics of the world (e.g., how many Ks have F) and those that appeal to structured, conceptual representations (e.g., there is something about being K which causes it to F).
Statistical and conceptual theories express the major contrasting views of the truth conditions of generic statements [@Carlson1995essay].^[We use the terms statistical and conceptual to refer to what @Carlson1995essay referred to as "inductive" and "rules and regulations" views, respectively.]


### Statistical Accounts

Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}. 
Our probabilistic account has antecedents in @Cohen1999 's theory of generics as a frequency adverb (e.g., "generally").
As with our formulation, Cohen looks to *prevalence* as a metric against which generics get evaluated.
For Cohen, there are two kinds of generic statements: "Absolute" and "Relative".
"Absolute generics" have a meaning with a fixed threshold on prevalence and that threshold is 0.5: $P(F\mid K)>0.5$. 
Roughly speaking, "Dogs have four legs" is true because a given dog is more likely than not to have four legs. 
"Relative generics" on the other hand depend upon an alternative set of kinds (his notation: $Alt(K)$), similar to our comparison class.
These statements (e.g., *Mosquitos carry malaria*) are true if an arbitrary member of the target kind is more likely than an arbitrary member of an alternative kind to have the feature.

There are two main points of deviation of our theory from Cohen's.
First, though his theory is framed in terms of probabilities, it still relies upon fixed threshold and deterministic truth conditions. 
On the contrary, our theory posits that the meaning (i.e., threhsold) is underspecified in the semantics.
Second, Cohen must draw the adhoc distinction between "Absolute" and "Relative" generics; our theory is able to handle both kinds of examples with the same basic machinery.

It should be noted that another mechanism used in Cohen's theory is to contextually restrict the entities that go into the computation of prevalence (i.e., which robins do we look to compute the prevalence of *laying eggs* among *robins*?).
These entities are restricted to those that could have *some* feature in a (contextually-specified) alternative set of features (so called, $Alt(F)$). 
For example, the property \emph{lays eggs} could induce a set of alternatives that all have to do with procreation (e.g. \emph{gives birth to live young}, \emph{undergoes mitosis}, ...).
That is, *Robins lay eggs* can be evaluated as an "absolute generic" as the only individuals under consideration are the female members of kinds because only the female members can plausibly satisfy one or another of the alternatives.
<!-- Of course, the inferential machinery behind this sort of domain restriction (i.e., what is $Alt(F)$?) requires further theorizing to explain.  -->
The inferential machinery behind this sort of domain restriction (i.e., what is $Alt(F)$?) relies upon conceptual information, but how exactly remains obscure [@Carlson1995essay].
^[However, see @Cohen2004 for a discussion of how his semantic constraints relate to different kinds of generics and different kinds of conceptual representational frameworks found in cognitive science.]
Our theory doesn't explicitly rely upon domain restriction for this reason.
However, we think it's plausible that there exists a reformulation of the uncertain threshold model to a fixed threshold that where the listener has uncertainty about the relevant domain restriction. 

Our theory bears some conceptual similarity to a recent proposal by @Sterken2015. 
Sterken treats the generic as a kind of *indexical* (e.g., "this" or "I").
She argues that kind of truth-conditional variability exhibited by generics is best accounted for by an account generics as an indexical where both domain restriction and "quantificational force" vary with context.
We think our uncertain threshold model is a way of formalizing Sterken's "quantificational force".
As noted above, we have not yet formalized uncertainty about domain restriction but think this is a promising avenue for future research.

Other theorists under the quantificational or statistical banner draw on the intuition that generics seem to express something what is *normal* in the world [@Asher1995; @Pelletier1997; @Nickel2008].
For example, it is a regrettable but true fact that due to unfortunate circumstances, not all dogs have four legs.
However, we might want to hold onto the idea that were the world to work *normally*, then all dogs would have four legs. 
This idea has intuitive appeal for rejecting accidentally true generalizations (e.g., the hypothetical situation where all Supreme Court justices had social security numbers that were even numbers and the intuitive ambivalence about the statement *Supreme Court justices have even social security numbers*).
It also may underly what is problematic about stereotyped language (e.g., *Boys are good at math*).
The idea that speaker's beliefs (e.g., about what is *normal*) plays a role in endorsing and interpreting generalizations is central to the formulation of our theory of generics in a Bayesian cognitive model.
Indeed, we have elaborated this view showing in Expt. 3 that is in fact speaker's beliefs about what is likely to be the case in the near future that matters for endorsing generalizations.
Prevalence or propensity in our theory is a posterior predictive probability, which incorporates background knowledge in order to make predictions about the future. 
It is likely that what is "normal" is captured in much of our intuitive theories of other domains.

### Conceptual Accounts

It is also the first attempt in psychology to try to unify significant swaths of language that are seemingly quite different from one another: generalizations about events, causes, and categories.

-- Leslie (2008): Default inferences, could be similar to the cognitive model described, but we don't make strong metaphysical commitments (e.g., to "negative" properties)

-- "default generalization" -- very close, the listener is doing a generalization, but not generalization based on evidence, but by reasoning about the threshold



Generic language is the simple and ubiquitous way by which generalizations are conveyed between people.
Yet the dramatic flexibility of generic language has confounded psychologists, linguists and philosophers who have tried to articulate what exactly generic statements mean. 
We evaluated a theory of generic language derived from general principles of language understanding using a simple, but uncertain, basic meaning---a threshold on property prevalence.
Our formal model is a minimal extension of the RSA theory of language understanding, together with an underspecified threshold semantics.
The model was able to explain two major puzzles of generics: their extreme flexibility in truth conditions and the contrastingly strong interpretation of many novel generics.
Both of these phenomena were revealed to depend in systematic ways on prior knowledge about properties.
This prior knowledge was revealed through Bayesian model analysis to be structured, providing a promising bridge to conceptual accounts of generic language.
To understand the nature of the underlying prevalence scale, we showed that generic language is about speakers' expectations of future prevalence, and not necessarily what the current state of the world is like. 
Across all experiments, the formal model predicted the quantitative details of participants' judgments with high accuracy.

There have been numerous demonstrations arguing that statistics (e.g., property prevalence) are insufficient to explain generic meaning [@Gelman2002; @Gelman2007; @Cimpian2010; @Cimpian2010c; @Khemlani2012; @Prasada2013].
In these experiments, the prevalence considered is only the prevalence of the property for the target category [e.g., the percentage of birds that lay eggs; @Khemlani2012; @Prasada2013], what we have referred to as \emph{within-kind prevalence}. 
Indeed, this simple statistic also fails to explain our data (Figure \ref{fig:commongenerics}b, right).
Our formal model of pragmatics, by contrast, considers not only within-kind prevalence, but a listener's prior beliefs about prevalence across kinds in order to arrive at a meaning for a generic utterance.
By establishing the validity of a semantics based on prevalence alone, we provide a formalism to learn about categories from generic statements. 
Further, since prevalence is a probability, our model can take information conveyed with a generic and be naturally extended to make predictions about entities in the world or support explanations of events or behavior.



<!-- ## Generic Identification -->

<!-- Throughout this paper we treated the bare plural construction as a generic utterance with a threshold semantics: $\denote{\text{K F}}(x, \theta)=x>\theta$. -->
<!-- The bare plural construction can also indicate a specific plural predication. -->
<!-- For example, "Dogs are on my lawn" picks out a specific group of dogs, while "Dogs have fur"  does not [@Carlson1977]. -->
<!-- The problem of \emph{identifying} a generic meaning from a bare plural construction is itself a challenging problem because generic meaning can be signaled using a diverse array of morphosyntactic cues. -->

<!-- @Declerck1991 suggests that generic and non-generic bare plurals can be treated in the same way, and that pragmatic considerations alone may resolve interpretative differences.  -->
<!-- Indeed it does appear that knowledge of the properties under discussion (e.g., the state of being on a front lawn; the state of having fur) could facilitate the generic identification process. -->
<!-- Other pragmatic factors, like knowledge of the identity of the speaker (e.g., a teacher vs. a veterinarian), can also disambiguate generic and non-generic meaning [@Cimpian2008]. -->
<!-- Recent work suggests that utterances that fail to refer to specific entities or events could pragmatically imply generic meaning [@Crone2016cogsci]. -->
<!-- Incorporating these insights about generic identification into an information-theoretic, communicative perspective is a natural extension of this work. -->

## Implications for Conceptual Structure

Previous psychological and philosophical work on generics has looked beyond prevalence and focused on conceptual distinctions and relations [@Gelman2003; @Prasada2013; @Leslie2007; @Leslie2008]. 
Prasada has argued for a distinction between \emph{characteristic} properties (e.g.~\emph{Diapers are absorbent.}) and \emph{statistical} properties (e.g.~\emph{Diapers are white.}).
Leslie suggests information that is striking (e.g.~\emph{Tigers eat people.}) is useful and thus permitted to be a generic.
Gelman outlines how generics tend to express \emph{essential} qualities that are relatively timeless and enduring. 
Where in the prevalence-based semantics could such conceptual distinctions come into play?

Our approach makes the strong claim that beliefs about predicted prevalence are the connective tissue between conceptual knowledge and generic language.
That is, the effect of conceptually meaningful differences on generic language is predicted to be mediated by differences in corresponding prevalence distributions.
It is important to note that our approach is based on \emph{subjective} probability, and not mere frequency.
Indeed, we elucidated in Expt.~3 that using participants' \emph{predictions} of probability in our formal model perfectly track generic endorsement, when the present frequency would make the wrong prediction.

The focus on subjective, predictive probability casts new light on puzzles surrounding accidentally-true situations.
An example is the statement "Supreme Court Justices have even social security numbers", which is predicted by linguists to be rejected even if every single Supreme Court Justice has an even social security number [@Cohen1999].
Our explanation is that abstract intuitive theories lead us to reject observed frequencies in forming our subjective probabilities.
That is, because we may believe selection for the Supreme Court is not influenced by one's social security number, we would assign roughly 50\% subjective probability to the \emph{next} justice having an even number.
Thus, we would still reject the generic \emph{Supreme Court Justices have even social security numbers}, because the predictive prevalence would not be any different for Supreme Court Justices than any other profession.^[Our perspective makes the intriguing prediction that if we learned much more surprising information, we might be compelled to revise our theory and then accept the generic. For instance, if every justice in history had \emph{prime numbered} social security numbers (a more suspicious coincidence), one might appeal to a conspiracy, which \emph{would} have predictive consequences.]

Turning back to conceptual relations and structure, it is natural to ask when subjective probabilities might reflect conceptual knowledge?
We found that empirical prevalence distributions are structured in a way that reflects intuitions about causal mechanisms underlying different properties; the differences in shape of these distributions in turn led to variable endorsements and interpretations of generic sentences. 
It is plausible that richer conceptual knowledge also influences these distributions, such as higher-order conceptual knowledge about the nature of properties and categories [@Gelman2003; @Keil1992].
Indeed, it has been argued that conceptual structure in general, including higher-order abstractions, can be captured by probabilisitic causal models and their generalizations [@pearl1988probabilistic; @Gopnik2003theory; @Goodmanconcepts].
Future work will be needed to explore whether probabilistic representations of conceptual knowledge can capture the relations identified in other accounts of generics (such as characteristic, essential, and striking properties), and whether the effect of these relations can then be adequately captured via their impact on subjective prevalence.

### The Problems with Communicating Generalizations

Thus, in order to define a generalization about a category, there must be some corresponding concrete particular instance of that category from which the generalization is formed. 
For example, if an agent observes $n$ instances of \textsc{dog} ($d_1, d_2, ..., d_n$)
For example, if an observer forms a generalization about \textsc{dogs}, she must some way of determining when she is in an instance of the category \textsc{dogs}: She must be able to *individuate*.
In Hume's words, generalizations are predictions about "instances of which we have had no experience resemble those of which we have had experience" [@HumeTHN].


Generalizations can be made about almost anything.
Here, we restrict our focus to generalizations about categories (whose linguistic expression is known as *generic language*), which have been the primary focus of psychologists, linguists, and philosophers.
We posit that our analysis should expect to generalizations about events (habitual language) as well as generalizations about causal forces (causal language).



Generics express a relation between a kind K (e.g., \textsc{robins}) and a property F (e.g., \textsc{lays eggs}), such that the property can also be said to be applicable of an individual (i.e., the bird in my backyard lays eggs).
Bare plural statements (e.g., \emph{Robins lay eggs}) tend strongly to yield a generic meaning [@Carlson1977], though other forms can express such a meaning sometimes (e.g., \emph{A mongoose eats snakes.}).

Given that generics express a property that can be applied to individuals, it would seem intuitive that the number of individuals with the property would be what makes the statement true or false.
Counter-examples like \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} v. \emph{Birds are female} stifle such intuition. 

### Statistical Accounts of Generics

Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}. 
Of course, birds do not in general lay eggs (it's only the adult, female ones that do).
The primary way of dealing with such issues is to posit domain restrictions ("implicitly, we are only talking about the females") when there are "salient partitions" [@Carlson1995].
The most fully-developed theory on this front is due to @Cohen1999. 
Let's first introduce some notation:



### Conceptual Accounts of Generics

Conceptual accounts of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as the way of expressing special mental relationships between kinds and properties [@Leslie2008; @Prasada2012].
From this perspective, \emph{Bishops move diagonally} because those are the rules of the game, not because they \emph{tend to} move diagonally.
How do we come to know the rules of the game, though?

@Leslie2007's influential theory posits that generics are tied to a "default mode of generalization". 
This "default mode" comes equipped with the ability to single-out \emph{striking properties} (e.g. properties which are dangerous or appalling) as particularly useful aspects of the world to know about. 
Hence, \emph{Mosquitos carry malaria} is true because carrying malaria is striking, and thus, a useful bit of information to convey.
The default mode can also distinguish "negative" counter-instances of a property   (e.g. a bird that doesn't lay eggs) from "positive" counter-instances (e.g. a hypothetical bird that bears live young).
Generics are much less reasonable when positive counter-instances exist. Hence, \emph{Birds are female} seems weird because "being male" is a positive counter-instance of "being female", but since there are no birds that bear live young,  \emph{Birds lay eggs} is fine.

Parallel work in the psychology of concepts supports this perspective.
@Prasada2006 and later @Prasada2013 distinguish between \emph{principled}, \emph{statistical}, and \emph{causal} relations within concepts. 
Generics like \emph{Birds lay eggs} (in which only a minority of K have F) exhibit characteristics of principled connections (operationalized by endorsement of the phrase \emph{In general, Ks have F}), supporting @Leslie2007's typology.
Striking generics (e.g. \emph{Mosquitos carry malaria}) show characteristics of \emph{causal connections} (operationalized using the phrase \emph{There is something about Ks that cause them to F}). 
The fact that generics about different properties license different kinds of inferences is taken as evidence that the generics themselves represent different kinds of relations. Statistical information takes a backseat to the conceptual structure.







## Other philosophical puzzles

-- Books are paperbacks. (failure of the constrast class)
-- Birds lay eggs and are female vs. Elephants live in Asia and Africa
--> can use pragmatics to resolve if the property is conjunctive or not. 
-- Women are submissive. (Haslanger, 2011; generics + politeness mechanism, some states have lower value than others)
-- Muslims are terrorists (vs. Mosquitos carry malaria)
-- Mary handles the mail from antarctica (predictive propensity; also highlights domain restriction on events)

# Conclusion

It might seem paradoxical that a part of language that is so common in communication and central to learning should be vague. 
Shouldn't speakers and teachers want to express their ideas as crisply as possible?
To the contrary, underspecification can be efficient, given that context can be used to resolve the uncertainty [@Piantadosi2012].
In our work, context takes the form of a listener and speaker's shared beliefs about the property in question. 
By leveraging this common ground, generics provide a powerful way to communicate and learn generalizations about categories, 
which would otherwise be difficult or costly information to learn through direct experience.

The dark side of this flexibility is the potential for miscommunication or deceit: A speaker might assert a generic utterance that he himself would not accept, conveying a too-strong generalization to a na\"{i}ve listener.  
Our model predicts this potential particularly for properties which, when present, are widespread in a category---we showed that biological properties are believed to have this distribution, but many properties of social categories may as well [@Cimpian2011a; @Cimpian2012b; @Rhodes2012].
Disagreements are also predicted when interlocutors fail to share background assumptions:
differences in the within-kind prevalence, the prior distributions on prevalence, or the comparison class.
For example, there is considerable disagreement as to whether or not "Humans cause global warming".
Our theory predicts this disagreement may be the result of differences in the estimated \emph{causal power} of humans influencing global warming as well as the causal power of \emph{other forces} (e.g., plate tectonics) on climate change.
This is a promising area for future research.


Categories are inherently unobservable. 
You cannot see the category \textsc{dog}, only some number of instances of it.
Yet we easily talk about these abstractions, conveying hard-won generalizations to each other and down through generations.
The theory presented here gives one explanation of how we do so, providing a computational perspective on how category generalizations are conveyed and how beliefs play a central role in understanding language.



# References 

<!-- \setlength{\parindent}{-0.1in}  -->
<!-- \setlength{\leftskip}{0.125in} -->
<!-- \noindent -->

<div id = 'refs'></div>

```{r child = 'appendix-prevalencePrior.Rmd'}
```

```{r child = 'appendix-cueValidity.Rmd'}
```

