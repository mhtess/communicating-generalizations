---
title             : "Communicating generalizations"
shorttitle        : "Communicating generalizations"

author: 
  - name          : "Michael Henry Tessler"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 316, Stanford, CA 94305"
    email         : "mtessler@stanford.edu"
  - name          : "Noah D. Goodman"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
    
header-includes:
  - \usepackage{tabularx}
  - \usepackage{multicol}
  - \usepackage{wrapfig}
  - \usepackage{caption}
  - \usepackage{booktabs}

author_note: >
  Enter author note here.

abstract: >
    Acquiring abstract knowledge requires making generalizations.
    Everyday generalizations typically rest not upon observational data, but upon testimony: Generalizations conveyed with language.
    The language of generalizations---so called, generic or habitual language (e.g., *Dogs bark.*)---appears early in development and is ubiqutious in everyday conversation.
    However, generic sentences are extremely flexible in their usage, while simultaneously conveying a strong inference: That the generalization should hold in the future. 
    These phenonena have confounded previous attempts to formalize the meaning of generalizations in language.
    We present a computational model that treats the core meaning of a generic sentence as simple but underspecified, and uses general principles of language understanding to establish a precise meaning in context. 
    Our model shows the central phenomena of generic language emerge from the interaction of diverse prior beliefs with communicative principles.
    We elicit these priors experimentally and show that the resulting model predictions explain almost all of the variance in human judgments for both common and novel generics and outperform alternative models.
    This theory provides the computational bridge between the words we use and the generalizations they describe.
  
keywords          : "generics, pragmatics, semantics, bayesian modeling"
wordcount         : "X"

bibliography      : ["generics.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
---

\newcommand{\denote}[1]{\mbox{ $[\![ #1 ]\!]$}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\definecolor{Red}{RGB}{255,0,0}
\definecolor{Green}{RGB}{10,200,100}
\definecolor{Blue}{RGB}{10,100,200}
<!-- \newcommand{\ndg}[1]{\textcolor{Green}{$[ndg: #1 ]$}} -->
<!-- \newcommand{\mht}[1]{\textcolor{Blue}{$[mht: #1 ]$}} -->
<!-- \newcommand{\red}[1]{\textcolor{Red}{$#1$}} -->

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=6, fig.height=5, fig.crop = F, fig.path='figs/',
                      echo=FALSE, warning=FALSE, cache=F, message=FALSE, sanitize = T)

```


```{r include = FALSE}
library("papaja")
```


```{r}
library(rwebppl)
library(xtable)
library(tidyverse)
```

# Introduction 
<!--
If we are starting with habituals and/or doing all 3 kinds of generalizations, what puzzles do you present in the intro?
- flexible truth
- all generalizations ? ( causuals, habituals, generics)
- strong interpretations ? (seems to be special to generics)
- asymmetry ? (only for generics)
- predictives (future frequency)
-->

<!-- Generalizations. Quantitative models. Communication / Cultural ratchet. RSA but  -->
<!-- only specific language. One case study is generics. Puzzles. -->

<!-- Quantitative model, different kinds of generalizations -->

Figuring out that a category tends to have a property, an entity tends to exhibit a behavior, or a cause tends to produce an effect is crucial knowledge in an uncertain world.
The productivity of human thought rests upon generalizations.
From a very young age, children make strong inductive inferences from their experience, generalizing to new situations or novel instances of a category [@Dewar2010; @Gelman1986; @Markman1989].
Understanding inductive inferences from observations is a limited enterprise, however.
Many properties are difficult to observe (e.g., *cows have four stomachs*) and many events are statistically unlikely (e.g., *lightning strikes tall objects*).
For abstract, generalizable knowledge to remain in a culture, there must be a way for it to be faithfully transmitted across individuals and generations [@Tomasello1999].

Indeed, language provides simple ways to communicate generalizations: *Dogs are friendly*, *Mary smokes cigarettes*, *Yeast makes bread rise*. 
Generalizations are among the easiest forms of compositional linguistic expression and appear early in development. 
The language is so simple that it appears everywhere in human conversation (*Movie theaters are cold.*), pedagogy (*Blickets make the machine go.*), political and scientific discourse (*The President peddles falsehoods.*, *Psychology experiments donâ€™t replicate.*), stereotypes (*Boys are good at math*), motivation (*Big girls eat their broccoli!*), emotion-regulation  (*Accidents happen.*), child-directed (*Mommy works late*) and child-produced speech (*Slides aren't for grown-ups!*), and many other facets of experience [@GelmanEtAl2004; @Gelman2008; @Cimpian2010motivation; @Rhodes2012].

The human capacity to generalize from observations has been studied extensively in cognitive and developmental psychology [@Nisbett1983; @Heit2000].
Inductive inferences show complex sensitivites to the kind of observations, and even how those observations came to be observed in the first place. 
For instance, observing that an indigenous person living on a remote island *is obese* provides considerably less information about the weights of other islanders in comparison to learning that the islander's skin color *is brown* [@Nisbett1983]. 
Confounded causal evidence can be suddenly become unconfounded when you know the evidence was generated by an intentional agent [@Goodman2009].
Quantitative modeling has been crucial in explaining how the subtleties of experience relate to subtleties in generalization [@Tenenbaum2006; @Kemp2008].
<!-- In stark constrast, a computational understanding of communicating generalizations is not well understood at all.  -->

<!-- Gathering data and observing evidence, however, is costly.  -->
<!-- The human problem of induction even displays a complex sensitivity to not only the type of data, but how the data was generated [@Gweon2010; @Shafto2012]. -->
<!-- A learner cannot form all of her abstract knowledge through direct experience, however. -->
<!-- Life is only so long, and you can only be in one place at a time. -->

Computational modeling has also made tremendous progress in formalizing how information is transmitted among interlocutors through language [@Goodman2016; @Franke2015].
As an example, the Rational Speech Act theory of language understanding has had considerable success in providing a formal account of *pragmatic* language phenomena (i.e., interactions between language and context).
This framework has been used to explain, in precise mathematical terms, how it is that utterances like "some of the kids failed the test" are interpreted to mean "some (but not all) of the kids failed the test" [@Goodman2013], how "I waited a million years to get a table" is interpreted as "I waited a long time and I was annoyed" [@Kao2014], and how "John is tall" can mean John is 5'4" is John if a 12-year-old but will mean John is 6'2" if he is 24-years-old [@Lassiter2015].
Computational modeling has pushed our understanding of these commplex phenomena, but so far, rational models of language use have focused on *specific* statements: utterances that convey information about a particular entity or situation. 
A formal theory of statements that convey generalizations about entities or situations ---  statements that extend beyond the here-and-now --- has remained elusive.

A computational understanding of communicating generalizations is a formidible challenge.
Though the language of generalizations is found everywhere and in every language [@Behrens2005; @Carlson1995], the meaning of such linguistic expressions is philosophically puzzling and has resisted precise formalization.
Intuitively, generalizations should convey something about the propensity or probability of an instance, of a property, or of an event to occur, but exactly what is conveyed is unclear.
The generalization *Mary smokes cigarettes* seems to suggest a daily-habit, while *Bill writes poems* implies something much weaker, perhaps a poem every few weeks.
*Drinking moonshine makes you go blind* and so does *staring at the sun*, but neither causal statement tells you to precisely the increased relative risk.
*Mosquitos carry malaria* is a true statement despite the prevalence of carrying malaria among mosquitos being a just a fraction of 1\%, while *Ducks are female* might seem like a weird thing to say, even though the rate is much higher than the malaria-carrying mosquitos.
As well, *Ducks lay eggs* is a good generalization, though the rate is the same as *are female*.
Generalizations, it seems, do not reduce to precise quantitative statements; this is the major stumbling block to providing a formal account of generalizations in language [@Carlson1977; @Leslie2008; @NickelBlackwell].

Empirical work done on category generalizations --- so called *generic language* --- has elucidated another troubling phenomenon.
<!-- One case study in the language of generalizations is about category generalizations. -->
<!-- Category generalizations, called *generic statement*, has been studied extensively in cognitive and developmental psychology: *generic* statements, or generalizations about categories. -->
Generic statements often imply the property is widespread in the category [e.g., *Swans have hollow bones* is a strong statement; much more so than *Some swans have hollow bones*; @Gelman2002; @Cimpian2010; @Brandone2014].
This characteristically strong interpretation interacts with the indeterminacy in truth conditions to exaggerate evidence. 
For a variety of features, both children and adults infer that a property is *more* prevalent when told the generalizations than when judging it true or not [@Cimpian2010; @Brandone2014].

<!-- These *strong implications* might underly stereotype formation in social categories [@Rhodes2012]. -->
<!-- In addition to the flexibility in truth conditions, two phenomena have been noted.  -->
<!-- The case of *Mosquitos carry malaria* suggests the utterance must in some way be analogous to the quantifier "some" (e.g., *Some mosquitos carry malaria*).  -->
<!-- Generic language, however, is often interpreted as implying the property is widespread: *Swans have hollow bones* means something different than *Some swans have hollow bones* [@Gelman2002]. -->

In this paper, we introduce a formal modelfor understanding the language of generalizations.
<!-- The fact that generalizations in language seem to convey something about the propensity but that there seem to be as many interpretations of generalized statements as there are generalizations have confounded formal models aimed to capture their meaning. -->
We take as a starting point the idea that the core meaning of a linguistic generalization is simple (i.e., it conveys something about the propensity), but underspecified (i.e., there is uncertainty in the meaning).
We extend a rational model of communication --- the Rational Speech Act theory --- to be able to understand general statements that extend beyond the here-and-now.
We provide a number of stringest tests of this theory to explain generalizations about categories, events, and causal forces. 

In Section 1, we introduce an information-theoric communicative theory of how generalizations are conveyed in language and flesh out the model's predictions.
Our model predicts the truth conditions of generalizations should vary with the (A) the propensity of the target to exhibit the feature (e.g., how often Bill smokes) and (B) the prior distribution over the feature propensity (e.g., how often other people smoke).
In Section 2, we interrogate the truth conditions of generalizations of events (so called *habitual language*) by manipulating the target propensity (A) while surveying a wide variety of events (which should induce variability in B).
In Section 3, we manipulate both (A) and (B) while testing the model on generalizations about causes (*causal language*).
If our theory is truly a theory about communicating generalizations, it should be able to explain the known psychological puzzles surrounding *generic language* (generalizations about categories). 
In Section 4, we show our how model explains three known psychological puzzles surrounding generic language. 
In all cases, we provide formal comparison to simpler models and in all cases, find that only our communicative theory with an underspecified meaning explains the patterns of empirical data.

<!-- These three phenomena---flexible truth, strong interpretations, and evidence exaggeration---have confounded all formal models aimed to capture the meaning of generalizations in language. -->
<!-- Informal theories have argued that the nature of concepts and their relations are inextricably tied to their linguistic expression [@Leslie2007; @Prasada2013]. -->
<!-- Formal theories have limited themselves to a measurable quantitative degree (e.g., how prevalent a property is in a category), which, in isolation, is difficult to reconcile with the extreme flexibility of generic language. -->
<!-- Here, we present a theory formalizing the inuition that the core meaning of a linguistic generalization is simple, but underspecified, and that general principles of communication may be used to resolve the precise meaning in context.  -->
<!-- This theory inherently relies upon measurable quantities (i.e., prevalence) but situates it within a Bayesian agent with potentially structured prior knowledge, providing a bridge to conceptual theories.  -->
<!-- We find that this formalism (and not simpler ones) explains all three empirical puzzles. -->

<!-- When interpretations are compared directly against truth conditions (i.e., how prevalent a property would need to be for the generic to be true),  -->

<!-- Three empirical phenomena make generalizations in language difficult to formalize. -->
<!-- The conditions under which a generalizations becomes "true" are variable: -->


<!-- Existing work in both adults and children suggests that choosing to communicate a generalization can be used to exagerate evidence [@Cimpian2010; @Brandone2014]. -->

<!-- In addition to having manifold interpretations, what makes linguistic generalizations true or false (i.e., their truth conditions) is also a mystery. -->
<!-- The statement *Birds lay eggs* is true even though only about half of them do (namely, the females). -->
<!-- Half of birds are also female, and yet *Birds are female* sounds like a strange thing to say. -->
<!--  while *Canadians are right-handed* is also weird, even though the vast majority of them are.  -->

<!-- How can generalizations in language have such flexible truth conditions while simultaneously carrying intuitive and, at times, very strong implications? -->
<!-- This question has been investigated by psychologists, linguists, and philosophers since @Carlson1977. -->
<!-- Here, we provide a unified formal theory of generalizations conveyed in language. -->
<!-- We test theory on the preeminent case study of linguistic generalizations: generic language, or generalizations about categories. -->
<!--  -->
<!-- To be precise, we develop a computational model that describes pragmatic reasoning about the propensity required to assert the generaization.   -->
<!-- We find that this formalism resolves known philosophical and empirical puzzles. -->
<!--We also provide the first set of experimental results about linguistic generalizations about events and causal forces, so called *habitual* and *causal statements*.-->

<!-- and find extreme flexibility in usage, which our model predicts.  -->

<!-- In Section 2, we conduct a set of experiments to test the *truth conditions* of familiar generalizations about categories (generic statements), and find our model predicts key patterns in human judgments -->
<!-- In Section 3, we conduct a set of experiments to test *interpretations* of novel generic sentences, again finding our model predicts human judgments with high quantitative accuracy. -->
<!-- In Section 4, we begin to test the generality of the theory, examining the *truth conditions* of generalizations about events. -->
<!-- In Section 5, we further probe our theory's generality, investing the *truth conditions* and *interpretations* of novel generalizations about causal forces.  -->
<!-- In all cases, we provide formal comparison to simpler models and in all cases, find that only our communicative theory with an underspecified meaning function explains the pattern of human data. -->
<!-- We also find that the prior beliefs used in our model reflect conceptual structure, and this provides an understanding of the conceptual implications of generics. -->
<!-- Finally, we investigate the underlying semantic scale in more detail, and find that the core meaning of generalizations in language depends in a crucial way on subjective beliefs, not mere frequency. -->

<!--
Most would agree that \emph{Swans are white}, but certainly not every swan is.
This type of utterance conveys a generalization about a category (i.e. \textsc{swans}) and is known as a generic utterance [@Carlson1977; @Leslie2008].
Communicating generically about categories is useful because categories themselves are unobservable [@Markman1989].
Knowledge about categories, while central to human reasoning, is tricky to acquire because categories themselves are unobservable [Markman1989}.
It is believed that every language can express generic meaning [@Behrens2005; @Carlson1995], and that generics are essential to the growth of conceptual knowledge [@Gelman2004] and how kinds are represented in the mind [@Leslie2008].
Generic language is ubiquitous in everyday conversation as well as in child-directed speech [@Gelman2008], and children as young as two or three understand that generics refer to categories and support generalization [@Cimpian2008].
and though English and many other languages do not possess an unambiguous form devoted to generic meaning [@Behrens2000, @AlMalki2014].
Additionally, generics are the primary way by which speakers discuss social categories, making them key to propagating stereotypes [@GelmanEtAl2004; @Rhodes2012; @Leslie2015] and impacting motivation [@Cimpian2010motivation].
Despite their psychological centrality, apparent simplicity, and ubiquity, a formal account of generic meaning remains elusive.

The major issue in formalizing generic language is determining what makes a generic sentence true or false.
At first glance, generics feel like universally-quantified statements as in \emph{All swans are white}. 
Unlike universals, however, generics are resilient to counter-examples (e.g., \emph{Swans are white} even though there exist black swans). 
Intuitions, then, fall back to something more vague like \emph{Swans, in general, are white} because indeed most swans are white.
But mosquitos, in general, do not carry malaria, yet everyone agrees \emph{Mosquitos carry malaria}.
Interpreting the generic as meaning "most" (i.e. \emph{Most swans are white}) captures many cases, but cannot explain why \emph{Robins lay eggs} and \emph{Mosquitos carry malaria} are so intuitively compelling: Only adult female robins lay eggs and a very tiny fraction of mosquitos actually carry malaria.
Indeed, it appears that any \emph{truth condition} stated in terms of how common the property is within the kind violates intuitions.
Consider the birds: for a bird, being female practically implies you will lay eggs (the properties are present in the same proportion), yet we say things like \emph{Birds lay eggs} and we do not say things like \emph{Birds are female}.



How can generics have such flexible truth conditions while simultaneously carrying strong implications?
In this paper, we explore the idea that the core meaning of a generic statement is simple, but underspecified, and that general principles of communication may be used to resolve precise meaning in context. 
In particular, we develop a mathematical model that describes pragmatic reasoning about the degree of prevalence required to assert the generic.  
We find that this formalism resolves the philosophical and empirical puzzles.
-->

# Communicating Generalizations

Generalizations are used to make predictions about instances that an agent has yet to experience [@HumeTHN].
What does it mean to form a generalization?
To form a generalization about a class $k$ (e.g., a category, an event, or a causal force), an observer must be able to individuate an exemplar or instance $x$ as belonging to the class $k$.
The linguistic expression of this manifests in utterances such as "$x$ is a $k$" or "this $k$ ..."
To generalize a feature $f$ to that class, it is also necessary to be able to determine if the particular has the feature.
That is, a speaker must be able to say "This $k$ (i.e., $x$) has $f$".
We call this kind of statement a *particular*.

```{r tabgenPart, results="asis"}
df.tab <- data.frame(
  Generalization = c("Dogs are friendly", "John smokes", "Drinking moonshine makes you go blind"),
  #x = c("an instance of K in space", "an instance of K in time", "an instance of a ")
  x = c("a dog", "an instance of John", "an instance of a person drinking moonshine"),
  k = c("DOGS", "JOHN", "DRINKING MOONSHINE"),
  f = c("is friendly", "is smoking / smoked at time t", "caused person to go blind")
) #%>% xtable(., 
 #        caption = c("Decomposition of generalizations into concrete particulars and corresponding properties."),
  #       label = c("tab:genpart"))


apa_table(df.tab, 
          caption = "Decomposition of generalizations into concrete particulars and corresponding properties.", 
          escape = TRUE, small = TRUE
)

#print(df.tab,  type = "latex", tabular.environment = "tabularx", width = "\\textwidth",  include.rownames = FALSE, comment = F)
```


Suppose we observe instances $\{x_1, x_2, ..., x_n\}$ of class $k$ and some number of them have feature $f$: What do we learn about the probability of that feature in a future instance $x_{n+1}$ of $k$?
Let's call this probability $P(x \in f \mid x \in k)$ the *propensity* of $k$ to have $f$ (or, the *prevalence* of $f$ among $k$).
We can treat this probability as an unknown latent cause of $f$ in $k$, and use the Bayesian probability calculus to learn from our observations.

Suppose *a priori* we have no informative beliefs about the propensity (i.e., $P(x \in f \mid x \in k) \sim \text{Uniform}(0, 1)$).
Observing some number of positive instances of $k$ with $f$ will update this prior belief distribution into a posterior belief distribution.
As a concrete example, consider the prevalence of \textsc{being friendly} ($f$) among \textsc{dogs} (k); call this $p = P(x \in \{\text{is friendly}\} \mid x \in \{\text{is a dog}\})$.
We encounter some individual dogs, note how many of them were friendly, and form a posterior belief distribution about the friendliness of dogs.

$$
P(p \mid d) = \frac{P(d \mid p) \cdot P(p)}{\int_{p'}P(d \mid p') \cdot P(p')}
$$
Our ability to use this updated belief $P(p \mid d)$ in order to make predictions about unobserved instances is a computational description of generalization [e.g., @Tenenbaum2011].
Life is only so long, however, and a person can only be in one place at a time.
The amount of data we can gather about any particular category, event, or causal force is extremely limited by these physical constraints.
Thus, it would be useful if language provided a simple way to communicate these generalizations to one another.

The linguistic outlet for generalizations about categories---so called, *generic language*---has received tremendous attention from psychologists, linguists, and philosophers.
Generic language is one of the earliest emerging forms of complex, compositional language (CITE).
Somewhere between 2 and 3 years of age, children recognize that generics convey a generalization about a category, not directly tied to concrete instances in a scene [@Cimpian2008].
More on development.

What is perhaps surprising from a formal perspective is that generic language is far from trivial to characterize. 
If generics convey something about the prevalence of feature, we would expect them to be in some way comparable to quantifier statments (e.g., "some", "most", "all", ...).
Rehash linguistic puzzles.
The extreme flexibility of generic meaning stands in stark contrast to its early emergence in development.
In fact, quantified statements, whose formal meaning is much more straight-forward, emerge much *later* in development.
This has led some to conclude that the normal tools for describing the semantics of quantified utterances (namely, a truth-functional threshold) will not work for generic language [@Leslie2008].

This may be throwing out the baby with the bathwater.
Consider the acquisition problem, there are two aspects of meaning that a language learner must acquire for the semantics of a quantified utterance: (1) that the meaning is a threshold-function (i.e., $\denote{u}(p, \theta) = \{p :p > \theta\}$ and (2) the value of the threshold (e.g., $\theta = 0$ for "some", $\theta = 0.5$ for "most").
Perhaps, then, the difficulty in acquiring the meaning of quantifiers is a difficulty in learning the value of the threshold (aspect 2) and not in recognizing that the utterance conveys something about the quantity (e.g., via a semantic threshold; aspect 1).
This would lead to the prediction that generics could be amenable to a threshold-semantics, but with the value of the threshold left underspecified in the semantics.
The problem of generic language interpretation then reduces to the problem of inferring the threshold in context.

We can formalize this hypothesis with a "literal listener" model from the Rational Speech Act framework [@Frank2012; @Goodman2013; @Goodman2016].

\begin{eqnarray}
L_{0}(p \mid u) &\propto& {\delta_{\denote{u}(p, \theta)} \cdot P(p) \cdot P(\theta)} \label{eq:L0}
\end{eqnarray}

The $\delta_{\denote{u}(x)}$ likelihood is the Kronecker delta function returning $1$ for states $p$ compatible with utterance $u$ and $0$ otherwise. 
For the case of a generic utterance, these are states of the world $p$ that are greater than the semantic threshold $\theta$.

For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$.
The prior distribution $P(x)$ is a distribution over likely states of the world. 
For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly).

The literal listener is a hypothetical agent who knows the threshold $\theta$.




# A Formal Theory of Generalizations in Language

We find both the statistical and conceptual accounts compelling, but an important perspective is missing.
Linguistic generalizations are not unique in their flexibility.
Understanding language in general depends upon assumptions interlocutors make about each other and what content is under discussion. 
Viewing language understanding as a social reasoning process reveals that utterances carry a mosaic of interpretations with a complex sensitivity to context [@Clark1996; @Grice1975; @Levinson2000]. 
Can the puzzles of generic language be understood as effects of pragmatic reasoning?
If so, we may be able to get away with a relatively simple, statistical,  semantic theory.
Abstract mental representations, then, may be the conceptual backdrop against which generic language is interpreted. 

We pursue such a line of inquiry, assuming the simplest truth-conditional meaning: a threshold on prevalence $P(F\mid K)>\theta$ [c.f., @Cohen1999].
No fixed value of the threshold, $\theta$, would allow for the extreme flexibility generics exhibit (e.g. \emph{Mosquitos carry malaria}; \emph{Robins lay eggs} v. \emph{Robins are female}), 
so instead we allow this threshold to be established in context through pragmatic reasoning.
Such an inference would depend on background knowledge about properties and categories---potentially structured, conceptual knowledge.
This inference, nonetheless, is a general mechanism of understanding language, not specific to interpreting generic statements.
We formalize this hypothesis in the Rational Speech Act (RSA) theory---a formal, probabilistic theory of language understanding.
RSA is derived from social reasoning, formalized as recursive Bayesian inference between speaker and listener [@Frank2012; @Goodman2013; @Goodman2016; see also, @Franke2009; @Franke2015].
@Goodman2016 provides a good introduction to the RSA framework and Appendix A presents a brief tutorial on RSA for the reader unfamiliar.

```{r eval = F}
l0.model <- '
var literalListener = Infer({
  model: function(){
    var state = uniform(0,1)
    var statePrior = uniform(0,1)
    var theta = uniform(0,1)
    var thetaPrior = uniform(0,1)
    condition(state > theta)
    return {state_posterior: state, state_prior: statePrior, theta_prior: thetaPrior}
 }, method: "rejection", samples: 50000
})

literalListener
'

l0.rs <- webppl(l0.model)

l0.rs %>% select(Parameter, value) %>%
  separate(Parameter, into = c("Parameter","Distribution")) %>%
  ggplot(., aes(x = value, fill = Parameter, lty = Distribution))+
    geom_density(alpha = 0.3, adjust = 1.2)+
    theme_few() + 
    theme_solarized(light = FALSE)+
    scale_fill_solarized("red")
```


The model begins with a simple Bayesian agent---a literal listener---who updates her prior beliefs $P(x)$ according to the truth-functional meaning of the utterance heard $\denote{u}$.
\begin{eqnarray}
L_{0}(x \mid u, \theta) &\propto& {\delta_{\denote{u}(x, \theta)} \cdot P(x)} \label{eq:L0}
\end{eqnarray}
For the generic sentence, we assign the simplest possible meaning, a threshold on the prevalence: $\denote{u_{generic}} = P(F\mid K)>\theta$.
The prior distribution $P(x)$ is a distribution over likely states of the world. 
For the case of understanding a generic (e.g., *Dogs are friendly*), the relevant dimension for the adopted semantics is the prevalence of property within the category (e.g., the \% of dogs that are friendly).
The likelihood $\delta_{\denote{u}(x)}$ is the Kronecker delta function returning $1$ for states $x$ compatible with utterance $u$ (i.e., where the prevalence $x$ is above the threshold $\theta$), and $0$ otherwise.
The literal listener is a hypothetical agent who knows the threshold $\theta$.

Next is a hypothetical speaker, who reasons about the literal listener when deciding which utterance to produce.
\begin{eqnarray}
S_{1}(u \mid x, \theta) &\propto& \exp{(\lambda_1 \cdot \ln {L_{0}(x \mid u, \theta)} ) }\label{eq:S1}
\end{eqnarray}
This speaker $S_1$ also knows the threshold $\theta$ and chooses utterances to convey information to the literal listener $L_0$ .
Following RSA, this speaker is an approximately rational (speech-)actor with degree with rationality governed by parameter $\lambda_1$.

To understand how language is interpreted pragmatically, we model a pragmatic listener $L_1$ who updates her prior beliefs $P(x)$ by reasoning about the generative process of the utterance, the speaker model $S_1$.
\begin{eqnarray}
L_{1}(x , \theta \mid u) &\propto& S_{1}(u \mid x, \theta) \cdot P(x) \cdot P(\theta) \label{eq:L1}
\end{eqnarray}
Following @Lassiter2013, we model the pragmatic listener as having uncertainty about the threshold: $P(\theta)$.
Though the listener doesn't know the threshold *a priori*, she resolves the threshold (i.e., the meaning) in context by integrating her prior beliefs $P(x)$ with the basic principles of communication to be truthful and informative, instantiated in her model of the speaker $S_1$ [@Lassiter2015; @GoodmanLassiter].
In principle, thresholds could be learned over time for different contexts, but here we assume the listener has no informative knowledge about the semantic variable $P(\theta) = \text{Uniform([0, 1])}$.
The pragmatic listener $L_1$ (Eq. \ref{eq:L1}) is thus a model of *generic interpretation*: Upon hearing a generic, how should a listener update her beliefs?

To model *generic production*, we introduce a model of the speaker who reasons about this kind of listener:
\begin{equation} 
S_{2}(u \mid k) \propto \exp{(\lambda_2 \cdot {\mathbb E}_{x\sim P_{k}} \ln{ \int_{\theta} L_1(x, \theta \mid u)}  \diff \theta ) }
\label{eq:S2}
\end{equation}
This speaker model is similar to the simpler $S_1$ speaker model but instead reasons about the pragmatic listener $L_1$ with two additional modifications. 
First, the speaker is not assumed to have a particular state of the world $x$ that he wants to communicate to the listener.
Instead, this speaker has a belief distribution governed by his knowlege of category $k$ over the likely states of the world $x\sim P_k$, and information gain is calculated with respect to communicating that belief distribution to the listener.^[
As a special case of this belief distribution, one could imagine a delta function at some particular level of prevalence (e.g., 50\% of birds lay eggs).
We use a more general formulation to allow for the case when the speaker only has a vague sense of the prevalence of the property in the category.
]
Second, the speaker is not assumed to know the value of the threshold $\theta$.
All that is assumed is that the listener infers the value of this variable; the speaker thus integrates over the likely values the listener will infer.

The proportionalities in Eqs. \ref{eq:S1}, \ref{eq:S2} imply normalization over a set of alternative utterances.
As a first test of this theory, we consider the simplest set of alternatives, including only the possibility of staying quiet in addition to saying the generic (and we assume no difference in production cost between these alternatives). 
We adopt the simplest meaning of a generic utterance in terms of prevalence, a threshold function: $\denote{u_{gen}}(x, \theta)=x>\theta$.
The silent or \emph{null} utterance alternative carries no information (i.e., $\denote{u_{null}}(x, \theta)=T$) and produces a posterior distribution in the listener identical with the prior.^[
This alternative can be realized in at least two other ways: the speaker could have said the negation of the utterance (i.e., *It is not the case that Ks F.*) or the negative generic (i.e., *Ks do not F.*). All results reported are similar for these two alternatives, and we use the alternative of the *null* utterance for simplicity.
]
Since the only alternative to producing the generic is to stay silent, $S_2$ is interpreted as a model of endorsement, felicity, or truth judgments [@Degen2014].

A similar model structure has been used to explain the context-sensitivity of gradable adjectives like *tall* [@Lassiter2013]. 
The model thus makes the claim that generic language is *vague language*, and that the same principles that govern interpreting vague language also are at play in interpreting generic language.
This model assumes a threshold semantics which has been argued before as a simple and useful semantics for generics [@Cohen1999], but doesn't require us to specify the threshold \emph{a priori}: It is inferred with pragmatic reasoning.
We implemented this model in the probabilisitic programming language WebPPL [@dippl], and a fully specified version of the model can be found at \url{http://forestdb.org/models/generics.html}

<!--S_{2}(u \mid k) \propto  \exp{(\lambda_2 \cdot \ln \int_{\theta} L_{1}(x , \theta \mid u) \diff \theta ) }-->

This model makes the prediction that the interpretation of a generic statement (and hence, it's corresponding truth judgment) depends on the prior on prevalence $P(x)$.
We can explore the model by running simulations of $L_1$ and $S_2$ under various schematic priors.
$P(x)$ is a distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across animal categories. 
In Figure \ref{fig:schematic-unif}, are $L_1$ (Eq.~\ref{eq:L1}) posterior prevalence distributions on $x$ (red solid line) for several example prior prevalence distributions $P(x)$ (blue dashed line), as well as the $S_2$ generic endorsement probability for different levels of prevalence.
We name these example prior distributions to suggest properties that might be associated with such priors. 
(Later, we will empirically measure these priors for properties of interest.)
First, we explore generic interpretation and endorsement for priors of three different shapes (left column). 
For each of these, the $L_1$ posterior distribution (red solid) over prevalence is heavily driven by the prior (blue dashed).
$S_2$ endorsement probabilities for the generic (black solid) increase as a function of prevalence, and what counts as "true" (in terms of the prevalence) depends on the prior. 

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/schematics_s2.pdf}
    \caption{Schematic prior distributions for prevalence $x$ (blue dashed), the pragmatic listener $L_1$ model's posterior distribution over prevalence upon hearing a generic utterance (red), and speaker $S_2$ model's endorsement of a generic utterance for different levels of prevalence (black).
    The names given to these priors are meant to be suggestive of what kinds of properties these distributions might correspond to.
    The left column uses prevalence priors modeled as Beta(15,15), Beta(4,1), and Beta(4,16) distributions.
      The right column uses a prior distribution that is a mixture of the distribution to the left of it with a second component, modeled as Beta(0.5, 4.5), reflecting categories with 0\% property prevalence.
    Horizontal dashed line at 0.5 is for convenience of comparing the point at which an utterance becomes judged as more true than false for $S_2$.
	Note that the prior distribution over prevalence will be the same as $L_1$'s posterior distribution upon hearing the "null" utterance.
    }
  \label{fig:schematic-unif}
\end{figure}


To take one example, consider the distribution over what might be the property "are female" (top-left facet).
The \emph{a priori} prevalence is centered around 0.5.
Because of pragmatics, the pressure to be \emph{truthful} will drive likely threshold values below 0.5 (lower values are more likely to be true). 
At the same time, the pressure to be \emph{informative} will drive the threshold values up: Higher values are more likely to be informative. 
The result is a posterior over prevalence that is only marginally greater than the prior, making higher prevalence values more likely after hearing the generic. 
But the relative information gain is very little (the posterior is not very different from the prior), and thus the $S_2$ model is reluctant to endorse the generic unless the property is exceedingly prevalent. 
The same basic phenomenon can be observed for the other two example priors (left column): The posterior over prevalence heavily depends upon the prior, but is also not very different from the prior.

We now imagine what would happen if there were some kinds where the property was completely absent, while being present at some rate in other kinds.
Figure \ref{fig:schematic-unif} (right column) shows this possibility: mixing the distribution to the left of it with a second component at 0\% prevalence.
Consider the schematic prior over "lay eggs".
We see the pragmatic listener $L_1$'s posterior prevalence distribution is not very different from the interpretation that doesn't include the second component in the prior (here, "are female"), but it is dramatically different from the prior with two components (compare red with blue dashed line for the right column).
This suggests that when many categories have 0\% prevalence, lower thresholds are informative. 
Indeed, the $S_2$ model predicts that the generic becomes felicitous at lower prevalence levels (compare black line of left v. right column).
For "lay eggs", when the property is prevalent in 50\% of the kind (e.g., 50\% of birds lay eggs), endorsement of the generic (e.g., \emph{Birds lay eggs}) by the $S_2$ is roughly 0.85; for "are female", with the same prevalence (50\% of birds are female), endorsement for \emph{Birds are female} is only 0.50: It is judged to be neither true nor false. 
The important result is the asymmetry: the first generic can be endorsed more strongly than the second, at the same prevalence level; the exact endorsement rates depend on quantitative aspects of the priors, which must be determined empirically.

The model thus predicts differences in truth judgments depending on the prevalence prior. 
The first test of our theory, then, will be to see if these predictions correspond to human truth judgments of familiar generic sentences. 

# Communicating generalizations about events

*Habitual* language (e.g,. "Mary smokes.") conveys generalizations about events. 
In this first set of experiments, we test whether or not the context-sensitivity of these generalizations can be explained by our computational model of pragmatic reasoning. 
We focus on a special kind of event: a person doing an action. 

The internal structure of events can differ in several ways [CITE event people?].
For example, the event of *smoking a cigarette* takes between 5 - 10 minutes, while the event of *wearing socks* probably lasts for several hours. 
In these experiments, we focus on the dimension of the likely frequency of the event (how often a person does it), taking for granted that observers can segment time into discrete events. 

In order for the computational model described in Eq. \ref{eq:S2} to make predictions, we must first measure the prior over the frequency of different events $P(p)$.

## Prior elicitation

In this experiment we elicit the prior $P(p)$ for different events in order to generate model predictions for corresponding habitual statements.
Given that some individuals rarely or never engage in an action (e.g., "non-smokers" for smoking), while others do quite frequently, we would expect the prior to be a mixture distribution between (at least) these two possibilities, similar in spirit to Zero-inflated or Hurdle Models of epidemiological data \cite{hurdleModels}.
Indeed, there may be more than these two possibilities, corresponding to individuals with different traits or demographics (e.g., different expected frequencies depending on age or gender). 

### Method

#### Participants

We recruited 40 participants from Amazon's Mechanical Turk.
Participants were restricted to those with U.S. IP addresses and who had at least a 95\% work approval rating.
The experiment took on average 12 minutes and participants were compensated \$1.25 for their work.

#### Materials

We created thirty-one events organized into pairs or triplets from 5 different conceptual categories: 
food and drug (e.g. \emph{eats caviar}, \emph{eats peanut butter}), 
work (e.g. \emph{sells things on eBay}, \emph{sells companies}), 
clothing (e.g. \emph{wears a suit}, \emph{wears a bra}), 
entertainment (e.g. \emph{watches professional football}, \emph{watches space launches}), and 
hobbies (e.g. \emph{runs}, \emph{hikes}). 
Items were chosen to intuitively cover a range of likely frequencies of action, as well as to provide a minimal comparison to another item by having a common superordinate action (e.g. \emph{eating} caviar vs. peanut butter).

#### Procedure

For each event, participants were asked two questions, with associated dependent measures:
\begin{enumerate}
\item "How many \{men, women\} have \textsc{done action} before?" \\
Participants responded "N out of every J." by entering a number for N and choosing J from a drop-down menu (options for J: \{1000 - 10 million\}; default: 1000).
\item For a typical \{man, woman\} who has \textsc{done action} before, how frequently does he or she \textsc{do action}?"\\  
Participants responded "M times in K." by entering a number for M and choosing K from a drop-down menu (options for K: \{week, month, year, 5 years\}; default: year).
\end{enumerate}

For example, one set read: "How many men have smoked cigarettes before?"; "For a typical man who has smoked cigarettes before, how frequently does he smoke cigarettes?"

We anticipated there might be different beliefs about the frequency of events depending on whether the actor is male or female, so we asked about both genders.
Participants answered both questions for each gender on each slide (4 questions total per slide, order of male / female randomized between-subjects), and every participant completed all 31 items in a randomized order.
The experiment can be viewed at \url{http://stanford.edu/~mtessler/habituals/experiments/priors/priors-2.html}.

\begin{figure*}[t]
\centering
  \includegraphics[width=0.84\textwidth]{figs/tj-scatters-3}
  \caption{
  Human acceptability judgments as a function of the log frequency of action (left) and speaker $S_2$ model predictions (right) for ninety-three unique items (event \textsc{x} frequency). 
  Color denotes target-individual frequency of action (log scale), with lighter colors indicating more frequent actions. 
  Actual frequency noted on x-axis for examples (left).
  Error bars correspond to 95\% bootstrapped confidence intervals for the participant data and 95\% Bayesian credible intervals for the model predictions. 
  Error bars suppressed and points jittered on left facet for visual clarity.
  }
  \label{fig:tjScatters}
\end{figure*}

### Data analysis and results

We built a Bayesian data analysis model for this prior elicitation task.
Question 1 elicits the proportion of people who have done an action before. 
We model this data as coming from a Beta distribution: $d_{1} \sim \text{Beta}(\gamma_{1}, \xi_{1})$. 
Question 2 elicits the rate, or relative frequency, with which a person does the action.
This was modeled by a log-normal distribution: $\ln d_{2} \sim \text{Gaussian}(\mu_{2}, \sigma_{2})$. 
Each item was modeled independently for each gender.
We implemented this model using the probabilistic programming language WebPPL \cite{dippl}, and found the credible values of the parameters by running MCMC for 100,000 iterations, discarding the first 50,000 for burnin.

The priors elicited cover a range of possible parameter values as intended (Figure \ref{fig:priorScatter}, scatter), resulting in parametrized distributions of dramatically different shapes (insets).  
We observe a correlation in our items between the mean \% of Americans who have \textsc{done action} before (Question 1) and the mean log-frequency  of action (Question 2) ($r_{1,2} = 0.74$).
Items that tend to be more popular actions also tend to be more frequent actions (e.g. \emph{wears socks}) and visa-versa (e.g. \emph{steals cars}), though there are notable exceptions (e.g. \emph{plays the banjo} is not popular but done frequently when done at all, as is \emph{smokes cigarettes}; \emph{goes to the movies} is a popular activity though not done very often). 
This diversity is relevant because the speaker model (Eq.~\ref{eq:S2}) will produce habitual sentences (e.g. \emph{Sam goes to the movies vs. the ballet.}) contingent on the shape of the prior distribution. 

From the inferred parameters and assumed functional forms, we get an inferred $P(p)$ modeled as a mixture of individuals with the possibility of carrying out the action and those without the possibility of doing it. 
That is, $P(p)$ was constructed by sampling $p$ as follows:

\begin{align}
\theta & \sim \text{Beta}(\gamma_{1}, \xi_{1}) \nonumber \\ 
\ln \lambda & \sim \begin{cases}
		\text{Gaussian}(\mu_{2}, \sigma_{2}) &\mbox{if } \text{Bernoulli}(\theta) = \textsc{t} \label{eq:priorModel}  \\
				\delta_{\lambda=-\infty} &\mbox{if } \text{Bernoulli}(\theta) = \textsc{f} \\
		\end{cases}
\end{align}

In addition to specifying the correct way to combine our two prior-elicitation questions, using this inferred prior in our language model resolves two technical difficulties: (1) It smooths effects that are clearly results of the response format^[
For example, a very common rating is *1 time per year*. Presumably participants would be just as happy reporting *approximately* 1 time per year; the raw data does not reflect this due to demands of the dependent measure.
]
and (2) it better captures the tails of the prior distribution which have relatively little data and need to be regularized by the analysis.
Figure \ref{fig:priorScatter} (right) shows example inferred priors.

Some items show substantial differences between the genders (e.g., *wears a bra*) and some show subtle differences (e.g., *watches professional football*). 
We will explore the possibility of different truth conditions for habituals of different gendered characters in Experiment 2, for select items with priors that differ substantially by gender.

## Experiment 1: Endorsing generalizations about events

A present-tense habitual sentence is of the form \textsc{singular noun phrase} $+$ \textsc{present tense simple verb phrase} (e.g. \emph{Bill smokes cigarettes.}).  
We next explore the endorsements of habituals of this form made from the items whose propensity priors were measured in Experiment 1. 

### Method

#### Participants

We recruited 150 participants from MTurk.
To arrive at this number, we performed a Bayesian precision analysis to determine the minimum sample size necessary to reliably ensure 95\% posterior credible intervals no larger than 0.3 for a parameter whose true value is 0.5 and for which the data is a 2 alternative forced choice.
This analysis revealed a minimum sample size of 50 per item; since participants only completed about one third of the items, we recruited 150 participants.
The experiment took 4 minutes on average and participants were compensated \$0.55 for their work.

#### Procedure and materials

On each trial, participants were presented with a \emph{past frequency statement} for a given event of the form: "In the past M \{weeks, months, years\}, \textsc{person} \textsc{did x} 3 times".
For example, \emph{In the past month, Bill smoked cigarettes 3 times}.
The particular intervals used (number M and window \{weeks, months, years\}) were selected after examining the predictions of the speaker model (Eq.~\ref{eq:S2}), for each item independently, to yield a variety of predicted endorsement rates.
The items were the same as in Expt. ~1.

Participants were asked whether they agreed or disagreed with the corresponding habitual sentence: "\textsc{person does x}" (e.g. \emph{Bill smokes cigarettes}).
Participants saw 25 out of the 31 items paired randomly with a male or female character name; the other 6 trials were presented with both male and female names (on separate trials; 37 trials total) to explore the nature of the contrast class (see Model section). 
The experiment can be viewed at \url{http://stanford.edu/~mtessler/habituals/experiments/truth-judgments/tj-2.html}.

### Results

#### Behavioral results

On each trial of the experiment, the participant was told a person did a particular action 3 times during some time window. 
Figure \ref{fig:tjScatters} (left) shows the correspondence between the frequency of the event (normalizing to a 5-year time scale and taking the logarithm) and the felicity of the corresponding habitual sentence. 
It is clear that a habitual sentence can receive strong agreement even when the actions are very infrequent (log frequency $\sim$ 1; 3 times in a 5-year interval; e.g. \emph{writes novels}, \emph{steals cars}).
We also see even when actions are done relatively frequently (e.g. 3 times in a one month interval; log frequency $\sim$ 5), there are habitual sentences participants are reluctant to endorse completely (e.g. \emph{wears socks}, \emph{drinks coffee}). 
In our data, actions completed with a high frequency (3 times in a one week interval; log frequency $\sim$ 6.5) receive at least 75\% endorsement, though there is still variability among them (e.g. between 10-25\% of people disagree with \emph{wears a watch} and \emph{wears a bra}). %, suggesting that even actions that are completed almost everyday can insufficient to generalize.
Overall, frequency of action predicts only a fraction of the variability in responses ($r^2(93) = 0.33$).
For actions that are done on the time scale of years or longer (lower median of frequency), frequency itself no longer explains the endorsements ($r^2(50) = 0.07$)

We further examined the six items for which we observed gender differences in the prior elicitation task (Expt.~1).
We find no differences between endorsements of the habitual of characters with male and female names, and overall, the mean endorsements by gender are strongly correlated $r(93) = 0.91$. 
This may be because the felicity of habitual sentences depends on a comparison to individuals of both genders (i.e, the \emph{contrast class} is other people; not just other men or women). 
Less interestingly, the lack of a difference may be the result of gender being not very salient in our paradigm, perhaps because the names used were not sufficiently gendered.

We observe that none of our items receive less than 25\% endorsement (i.e. only 75\% of participants disagree with the felicity of the utterance).
This may be due to the fact that actor has done the action in the past a plurality of times; we would expect to get strong disagreement with the habitual when the person has never done the action, or perhaps done it only once.


\begin{figure*}[t]
\centering
  \includegraphics[width=\textwidth]{figs/expt3-4-scatters-camera.pdf}
  \caption{Left: Predicted log frequency as a function of past log frequency given to the participant (Expt. 3a; CIs suppressed and jitter added for visual clarity).
  Middle: Human endorsements of habitual sentences (Expt. 3b) vs. Predicted log frequency (Expt. 3a), with data for corresponding items from Expt. 2 (assumed to have the same predictive log frequency as baseline). 
  Right: Endorsements (Expt. 3b) vs. Speaker $S_2$ model predictions using empirically elicited predictive frequencies (Expt. 3a).}
  \label{fig:tj3}
  \vspace{-7pt}
\end{figure*}

#### Model fit and results

We used the pragmatic speaker model $S_2$ (Eq.~\ref{eq:S2}) with the priors elicited above (Expt.~1) to predict felicity judgments in Expt.~2, assuming the target propensity (to be conveyed by $S_2$) is the provided frequency.
Because we observe no difference between the felicity judgments for habituals of male and female characters, we use a 50\% mixture of the inferred priors for each gender to construct a single frequency distribution $P(\lambda)$ across individuals.
The model has two free parameters---the speaker optimality parameters, $\alpha_i$, in Eqs.~\ref{eq:S2} \& \ref{eq:S1}. 
We use Bayesian data analytic techniques to integrate over these parameters \cite{LW2014}, comparing the posterior predictive distribution to the empirical data in Expt.~2.
To construct the posterior predictive distribution over responses, we collected 2 MCMC chains of 100,000 iterations, discarding the first 50,000 iterations for burn in.
The Maximum A-Posteriori value and 95\% highest probability density interval for $\alpha_1$ is 19.3 [14.9,19.9] and $\alpha_2$ is 1.5 [1.4,1.6].
%The MAP and HPD interval for the data-analytic guessing parameter is 0.004 [0.0003, 0.03], suggesting that there is not a substantial amount of the data that is better explained by a model of random guessing than by our pragmatic speaker model.

As shown in Figure \ref{fig:tjScatters}, right, the probabilistic pragmatics model does a good job of accounting for the variability in responses ($r^2(93) = 0.94$), including actions done on the time scale of years or more  ($r^2(50) = 0.92$).
The model decides when the habitual is a useful way to describe the person's behavior, assuming that what the person did in the past is representative. 
This raises an interesting question: Does the propensity communicated by the habitual indicate an objective, past frequency or a subjective, future expectation?


# Communicating generalizations about causes

# Communicating generalizations about categories


# Empirical Test 1: Flexible Truth Conditions

Any theory of generic language must explain their puzzling flexibility of usage with respect to prevalence.
That is, \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} are reasonable things to say, but \emph{Birds are female} is not.
The pragmatic speaker model $S_2$, Eq.~\ref{eq:S2}, is a model of truth judgments. 
We test our model on thirty generic sentences 
that cover a range of conceptual distinctions discussed in the literature  [@Prasada2013]: characteristic (e.g. \emph{Ducks have wings.}), minority (e.g. \emph{Robins lay eggs.}), striking (e.g. \emph{Mosquitos carry malaria.}), false generalization (e.g. \emph{Robins are female.}), and false (e.g. \emph{Lions lay eggs.}).
In additional to the canonical cases from the linguistics literature, we selected sentences to elicit the full range of acceptability judgments (intuitively, "acceptable", "unacceptable", and "uncertain") with low-, medium-, and high-prevalence properties. 

The pragmatic speaker model $S_2$ is fully-specified except for the prior distribution over prevalence $P(x)$, which plausibly varies by the type of property in question.
To compare the model to empirical truth judgments, we thus first measure the prior distribution over prevalence of these properties (Expt.~1a).
In Expt.~1b, we collect human judgements about the acceptability of the generic sentences. 

## Experiment 1a: Prevalence Priors

The prior $P(x)$ (in Eqs.~\ref{eq:L1}, \ref{eq:L0}) describes the belief distribution on the prevalence of a given property (e.g. \textsc{lays eggs}) across relevant categories. 
In exploring the model, we saw that the shape of this distribution affects model predictions, and this shape may vary significantly among different properties.
We thus measured this distribution empirically for the set of properties (e.g. \textsc{lays eggs, carries malaria}; 21 in total) used in our target sentences. 
 
### Method

#### Participants

We recruited 60 participants over Amazon's crowd-sourcing platform Mechanical Turk (MTurk).  
Participants were restricted to those with US IP addresses and with at least a 95\% MTurk work approval rating (the same criteria apply to all experiments reported). 
3 participants where unintentionally allowed to do the experiment for a second time; we excluded their second responses (resulting in $n=57$).
2 participants self-reported a native language other than English; removing their data ($n=55$) has no effect on the results reported. 
The experiment took about 10 minutes and participants were compensated \$1.00.

#### Procedure and materials

On each trial of the experiment, participants filled out a table where each row was an animal category and each column was a property. 
In order to alleviate the dependence of the distribution on our animal categories of interest, participants generated half of the animal categories before viewing the properties; the other half were randomly sampled from a set corresponding to the generic sentences used in Expt. 1b (e.g. \textsc{robins, mosquitos}).

Participants began the experiment by seeing a list of 6 animal kinds and were asked to list 5 of their own.
A column then appeared to the right of the animal names with a property in the header (e.g. "lays eggs").
Participants were asked to fill in each row with the percentage of members of each of the species that had the property (e.g. "50\%").
Eight property--columns appeared in the table, and this whole procedure was repeated 2 times.
In total, each participant generated 10 animal names and reported on the prevalence of sixteen properties for 22 animals (their own 10 and the experimentally-supplied 12). 
Properties were randomly sampled from a set of 21 properties associated with generics of theoretical interest, as described above.
For a full list of the properties, and generic sentences used in Expt.~1b, see Table 2 (Appendix).
The experiment can be viewed at \url{http://stanford.edu/~mtessler/experiments/generics/experiments/real-kinds/prior-2.html}.

### Data analysis and model predictions

To process the priors data, we discretize the prevalence judgments to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$, and look at the counts within each bin, after doing add-1 Laplace smoothing, as the relative probability of that prevalence. 
Using these priors, we can explore how $L_{1}(x , \theta \mid u)$, the pragmatic listener model, interprets a generic utterance (Figure \ref{fig:commongenerics}a, insets). 
The prior beliefs over the prevalence of the property, $P(x)$, can also be interpreted as the pragmatic listener's posterior upon hearing the null utterance, because the null utterance has no information content.
We see the interpretation of the generic is quite variable across our empirically measured priors.
For instance, in the case of \textsc{carries malaria}, the prior is very left-skewed; here, the threshold $\theta$ can plausibly be quite low while still being informative, since a low threshold still rules out many possible alternative kinds (and their corresponding degree of prevalence).
Properties like \textsc{doesn't attack swimmers} are very right-skewed; here, even a relatively high threshold would not result in an informative utterance (intuitively, as not many kinds would be ruled out), and so the generic is unlikely to be used by speaker $S_2$ unless the property is practically-universal within the target category. 
Some properties have priors that are unimodal with low variance (e.g. \textsc{is female}); these properties are present in every kind in almost exactly the same proportion and thus are too obvious and certain to allow for an informative generic utterance: The posterior is not very different from the prior. 
With $P(x)$ now empirically established, we can test if our speaker model predicts human truth judgments of generic statements about these properties.

## Experiment 1b: Truth Judgments

### Method

#### Participants

We recruited 100 participants over MTurk. 
4 participants were excluded for failing to pass a catch trial.
5 participants self-reported a native language other than English; removing their data has no effect on the results reported. 
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

Participants were shown thirty generic sentences in bare plural form, one after another.
They were asked to press one of two buttons (randomized between-participants) to signify whether they agreed or disagreed with the sentence (see Table 2 in Appendix for complete list). 
The thirty sentences were presented in a random order between participants and covered a range of conceptual categories described above.
Approximately 10 true, 10 false, and 10 uncertain truth-value generics were selected.

As an attention check, participants were asked at the end of the trials which button corresponded to "Agree".
4 participants were excluded for failing this trial.

#### Data analysis and results

As a manipulation check, the first author assigned an \emph{a priori} truth-judgment (true/false/indeterminate) to each stimulus item.
This was a significant predictor of the empirical truth judgments: true generics were significantly more likely to be agreed with than the indeterminate generics ($\beta = 3.14; SE = 0.15; z = 21.5$), as revealed by a mixed-effect logistic regression with random by-participant effects of intercept.
Indeterminate generics were agreed with \emph{less} likely than chance ($\beta = -0.49; SE = 0.09; z = -5.3$) but significantly more than false generics ($\beta = 2.09; SE = 0.14; z = 14.5$).

From the prevalence prior data (Expt.~1a), we estimate participants' beliefs about the prevalence of a property \emph{for a given kind} (e.g.~the percentage of \textsc{robins} that \textsc{lay eggs}; see green intervals on Figure \ref{fig:commongenerics}a insets, and Table 2 in Appendix).
As a simple baseline hypothesis, we first explore whether these prevalence values themselves predict generic endorsement (e.g.~does the fraction of \textsc{robins} that \textsc{lay eggs} predict the felicity of \emph{Robins lay eggs}?).
We find a little over half of the variance in truth judgments data is explained this way ($r^2 = 0.599$; MSE=0.065; Figure \ref{fig:commongenerics}b, right). 
This is not surprising given that our stimulus set included generics that are true with high-prevalence properties (e.g. \emph{Leopards have spots.}) and  generics that are false with low prevalence properties (e.g. \emph{Leopards have wings.}). 
However, large deviations from an account based purely on target-category prevalence remain: Generics in which the target-category has intermediate prevalence (prevalence quartiles 2 and 3: $ 20\% < prevalence < 64\%$), are not at all explained by prevalence within those categories ($r_{Q2,3}^2 = 0.029$; MSE = 0.110).

The pragmatic speaker model, $S_2$ in Eq.~\ref{eq:S2}, predicts an endorsement probability for a generic sentence, given prior beliefs about the property, $P(x)$, and a prevalence-level,  $x$, within the kind-of-interest. (That is, $S_2$ provides a model of whether someone who knows $x$ would say the generic to someone who doesn't, but shares prior $P(x)$; as discussed above we adopt this as a model of agreement judgments.)  
We use the empirically estimated within-kind prevalence as the $x$ that the speaker $S_2$ is trying to communicate, and use the empirically measured priors from Expt.~1a as the listener's prior $P(x)$. 
The $S_2$ model is then fully specified after setting the speaker optimality parameters $\lambda_1$ and $\lambda_2$ (in Eqs.~\ref{eq:S1}, \ref{eq:S2}).
%In order to get estimates of the $S_2$ model's predictive probability for endorsing generic sentences, we performed a bootstrap procedure on the empirical prior data. For each sample of the bootstrapped prior, we performed a Bayesian data analysis to infer the likely values of the model parameters. 
Rather than fitting the parameters, we inferred them and integrated over their plausible values using Bayesian data analysis [@LW2014]. We put uninformative priors over these parameters, with a range consistent with previous literature using the same model class: $\lambda_1 \sim \text{Uniform}(0,20)$, $\lambda_2 \sim \text{Uniform}(0,5)$.
We learn about the \emph{a posteriori} credible values of our model parameters by collecting samples from MCMC chains of 10,000 iterations removing the first 5,000 iterations, using the Metropolis-Hastings algorithm. 

The above analysis provides a single estimate for model predictions, but it is based on noisy empirical measurements of $P(x)$. In order to estimate the impact of this empirical noise on our model predictions, we resampled the prior data (with replacement) for 57 participants worth of data, discretizing and binning as we did above and then inferring parameters.
This procedure (re-sample prior, discretize and bin, infer parameters) was repeated 500 times to bootstrap the model predictions.
The Maximum A-Posteriori (MAP) estimate and 95\% Highest Probability Density (HPD) interval for $\lambda_1$ is 0.5 [0.004, 12.7] and $\lambda_2$ is 1.7 [1.3, 2.1].
^[
The fact that $\lambda_1$ is credibly less than 1 suggests that the generic utterance may be more costly than "staying silent" (our model assumes equal cost).
We maintain using only the two $\lambda$ parameters in the model for simplicity.
]

We compare the model's posterior predictive distribution of generic endorsement to the empirical truth judgments.
(The posterior predictive distribution marginalizes over the inferred parameter values to produce predictions about what the data \emph{should look like} given the pragmatics model and the observed data. 
This is akin to fitting the parameters and is the critical step in model validation: It shows what data is actually predicted by the model.) 
As we see in Figure \ref{fig:commongenerics}b, the pragmatic speaker model $S_2$, using empirically measured priors, explains nearly all of the variance in human truth judgments ($r^2=0.98$; MSE=0.003; Figure \ref{fig:commongenerics}b). 

Generics that received definitive agreement or disagreement are predicted to be judged as such by the model (corners of Figure \ref{fig:commongenerics}b), including items for which target-category prevalence is not a good indicator of the acceptability (e.g. \emph{Mosquitos carry malaria}, for prevalence quartiles 2 and 3, $r_{Q2,3}^2=0.955$; MSE=0.005; Figure \ref{fig:commongenerics}b, intermediate shades).
We also see the generics truth judgment model predicts uncertain truth judgments: for instance, \emph{Robins are female} is judged by both the model and human participants to be neither true nor false.
\emph{Sharks don't attack swimmers}, while true of most sharks, is judged to be not a good thing to say by both participants and the model.
This is strong evidence that the puzzling flexibility of generic truth-conditions can be understood with a simple semantic theory coupled with basic communicative principles (\emph{be truthful}, \emph{be informative}) operating over diverse prior beliefs about the properties, all of which are at play in understanding language. 

\begin{figure}
\centering
    \includegraphics[width=\columnwidth]{figs/generics-prior-prevalence-tj.pdf}
    \caption{Endorsing familiar generics. (a) 
    Prevalence prior distributions empirically elicited for twenty-one animal properties.
    Prior distributions summarized by two parameters of a structured Bayesian model: $\phi$----a property's potential to be present in a category----and $\gamma$----the mean prevalence when it is possible for the property to be present in a category.
    Inset plots display example empirical prior distributions over prevalence and corresponding $L_1$ model predictions: the posterior after hearing a generic utterance. 
    Intervals on the top of insets show human judgments about the prevalence of the property within a target category.
    (b)
    Human acceptability judgments compared with model predictions (left) and the target-category prevalence (right) for thirty generic utterances about familiar animals and properties. 
    Color denotes target-category prevalence of the property, with lighter colors indicating higher prevalence. 
     Error bars denote 95\% Bayesian credible intervals.
    }
  \label{fig:commongenerics}

\end{figure}



## Extended Analysis of Priors: Conceptual Structure 

The pragmatics model only has a prevalence-based semantics yet it is able to explain the flexibility in truth judgments for a diverse range of generic statements.
Conceptual accounts of generic statements have looked beyond prevalence, to structured knowledge representations as the critical factor in generic meaning [@Leslie2007; @Prasada2013]. 
We can interrogate our formal model to see what is driving its predictions, and, in particular, we ask whether structured representations might effect our model after all.

For a given property,  the prior distribution on prevalence $P(x)$ is a single distribution.
However, the distribution may be structured as the result of deeper conceptual knowledge. 
For instance, if participants believe that some kinds have a causal mechanism that \emph{could} give rise to the property, while kinds others do not, then we would expect $P(x)$ to be structured as a mixture distribution [cf., @Griffiths2005].
We know that prior knowledge plays a fundamental role in leading the pragmatics model to endorse or reject generics. 
We now explore the hypothesis that this is at least partly because these priors are structured into two components: kinds that \emph{can} have the property and other kinds that \emph{cannot}.
We explore this possibility by formulating a mixture model for the prevalence priors, and exploring how well it fits the prior data elicited in Expt.~1a.

### Data analysis

If a kind can have the property, we assume the prevalence follows a Beta distribution with mean $\gamma$ and concentration $\xi$. 
If a kind cannot, we assume the prevalence is a Delta distribution, with all probability mass at 0\%. ^[There are other ways to formulate the second component ("the kind doesn't have a causal mechanism that would give rise to  the property") of the prior. 
It could reflect accidental causes of the property, in which case, the prevalence could be a distribution that allows for non-zero prevalence. 
While an interesting possibility, its full consideration is beyond the scope of this article.
]
The relative contribution of these two components is governed by mixture parameter $\phi$, inferred from the data.^[This is similar in spirit to Hurdle Models of epidemiological data, where the observed count of zeros is often substantially greater than one would expect from standard models, such as the Poisson [e.g., adverse events to vaccines; @hurdleModels])]
We think of $\phi$ the \emph{potential of a property to be present in a kind} and $\gamma$ is the \emph{mean prevalence of the property among the kinds with the potential to have it}.\footnote{
We note that $\phi$ is not what other authors have described as \emph{cue validity} [@Beach1964; Khemlani2012], or $P(K \mid F)$. 
 $\phi$ is a mixture component in the prior distribution over prevalence: $P(F\mid K$). 
Cue validity and prevalence are related via Bayes Rule': $P(K \mid F) \propto P(F \mid K) \cdot P(K)$. 
}
If this model is correct, the prevalences given by participants would then be distributed as: $P(d) = \phi \cdot \text{Beta}(d \mid \gamma,\xi)+ (1 - \phi) \cdot \delta_{d=0}$. 

We performed Bayesian inference over this model, given the observed prevalence data, to examine how well the model's posterior predictive distribution reconstructs the prevalence prior data.
We put uninformative priors over all the parameters, $\phi \sim \text{Uniform}(0,1)$, 
$\gamma \sim \text{Uniform}(0,1)$, $\xi \sim \text{Uniform}(0, 50)$, 
and performed Bayesian inference separately for each property using the Metropolis-Hastings algorithm 
collecting 50,000 samples removing the first 25,000 iterations for burn-in.

### Results

Estimates of the mixture parameter $\phi$ and the mean of the "has the potential" component $\gamma$ for each property are shown in Figure \ref{fig:commongenerics}a.
We see significant diversity among our properties in both parameters, corresponding to priors over prevalence with dramatically different shapes (insets). 

Again, we look to the posterior predictive distribution to validate the structured prior model.
Using the model with its inferred parameters, we generate prevalence judgments for different properties and compare that to the empirical counts. 
We discretize the prevalence values of both the model and the data to 12 discrete bins: $\{[0-0.01), (0.01-0.05), (0.05-0.15), (0.15-0.25),  ..., (0.75-0.85), (0.85-0.95), (0.95-1]\}$.
This statistical model reproduces the prior elicitation data very well ($r^2 = 0.94$), while a model that assumes just a single generative component fails ($r^2 = 0.14$). This is strong evidence in support of a structured prior. 

The other test of this hypothesis is to re-examine the truth judgments from Expt.~1b using the pragmatics model with the inferred structured priors (as opposed to bootstrapping the raw empirical counts). 
We find the same correspondence to the empirical truth judgments data ($r^2 = 0.98$).
This provides further evidence that the prior distribution over prevalence $P(x)$ is structured.
The implication of this finding is that conceptual structure may indeed find its way into generic judgments, but via the prevalence prior, rather than directly in the semantics of the generic. We return to this idea in the General Discussion.

\begin{figure}
\centering
    \includegraphics[width=0.6\columnwidth]{figs/postPred-priorModel.pdf}
    \caption{Posterior predictive distribution of the structured, statistical model thought to give rise to the human data in the prior elicitation task. The close alignment between model and data suggests the assumption of a structured prior is warranted.}
  \label{fig:pp-priorModel}
\end{figure}



# Empirical Test 2: Interpreting Novel Generics

One of the most important roles for generic language is to provide learners information about new or poorly understood categories. 
This role depends on how unfamiliar generic sentences are interpreted [e.g., @Gelman2002; @Cimpian2010].
The pragmatic theory we present includes such a theory of generic comprehension: the listener model (Eq.~\ref{eq:L1}) describes interpretation of a generic utterance---\emph{Kind \textsc{has property}}---without previously knowing the prevalence of the property within this kind.
In our theory, the meaning is uncertain, but the pressure to be informative operates over \emph{a priori} beliefs about properties to produce an interpretation. 
Classic work in generalization suggests beliefs about the prevalence of properties differ by type of property, including relatively fine distinctions among properties that are all biological in nature [@Nisbett1983]. 
We leverage these diverse expectations, using properties that explore a wide range of \emph{a priori} beliefs about prevalence. 

Measuring \emph{a priori} beliefs is tricky when the kinds are unknown.
We cannot, as before, have participants fill out a table with rows corresponding to different animal kinds and columns corresponding to different properties:  Nothing would distinguish the rows.
Instead, we leverage the latent structure uncovered in our extended model analysis of Expt.~1 and decompose prevalence priors into 2 components: the property's potential to be present in a kind and the mean prevalence when present.

We use this novel method for measuring \emph{a priori} beliefs about the prevalence of these properties for unfamiliar kinds (Expt.~2a).
We then test the predictions of the pragmatic listener model $L_1$ using these empirically derived priors against human \emph{interpretations} of novel generic sentences (Expt.~2b).
Finally, we explain a previously reported empirical asymmetry between truth conditions and interpretations by comparing the speaker $S_2$ and listener $L_1$ models in the same experimental context (Expt.~2c).

## Experiment 2a: Prevalence Priors for Unfamiliar Kinds

### Method

#### Participants

We recruited 40 participants over MTurk.  
All participants were native English speakers. 
The experiment took about 5-7 minutes and participants were compensated \$0.75.

#### Procedure and materials

We constructed forty different properties to explore a wide range of \emph{a priori} beliefs about prevalence. 
These items make up four categories of properties: body parts of a particular color (e.g. \textsc{has green feathers}), described vaguely (e.g. \textsc{has small wings}), in accidental or disease states (e.g. \textsc{has wet fur}, \textsc{has swollen ears}), and without modification (e.g. \textsc{has claws}).
Because pilot testing revealed more variability for items in the accidental category relative to the other types of properties, we used twice as many exemplars of accidental properties, yielding a more thorough test of the quantitative predictive power of the $L_1$ interpretation model. 
We used 8 exemplars of each of the three non-accidental properties ("parts", "colored parts", "vague parts"), and 16 exemplars of accidental properties, building on a stimulus set from @Cimpian2010.
All materials are shown in Table 3 in the Appendix.

In the task, participants were introduced to a "data-collection robot" that was tasked with learning about properties of animals. 
Participants were told the robot randomly sampled an animal to ask the participant about (e.g. The robot says: "We recently discovered animals called feps."). 
We then used a two-stage elicitation procedure, aimed to measure the two components of the structured prior model: (1) the potential of the property to be present in a kind and (2) the expected prevalence when present.
To get at (1), the robot asked how likely it was that "there was \emph{a} fep with \textsc{property}" (potential to be present), to which participants reported on a scale from "unlikely" to "likely".
For example, it is very likely that there is a fep that is female, less likely that there is a fep that has wings, and even less likely that there is a fep that has purple wings. 
To get at (2), the robot then asked, "Suppose there is a fep that has wings. What percentage of feps do you think have wings?" (expected prevalence when present). 
Participants completed a practice trial to make sure they understood the meanings of these two questions.

### Data analysis and results

We used the same structured, statistical model for the prior data from Expt.~1.
The only difference from Expt.~1a. is that our experimental data comes from inquiring about the parameters of the priors directly, as opposed to asking about particular samples from the prior (i.e. particular kinds) as was done in Expt.~1a. 
We assume these two measurements follow Beta distributions ($d_{potential} \sim \text{Beta}(\gamma_{1}, \xi_{1})$; $d_{expected} \sim \text{Beta}(\gamma_{2}, \xi_{2})$), and construct single prevalence distributions, $P(x)$, by sampling from the posterior predictive distribution of prevalence as we did before: $P(x) = \int [ \phi\cdot \text{Beta} (x \mid \gamma_{2}, \xi_{2}) + (1 -  \phi) \cdot \delta_{x=0} ] \cdot \text{Beta}(\phi \mid \gamma_{1}, \xi_{1}) d\phi$.
We used the same uninformative priors over parameters $\phi, \gamma_{i}, \xi_{i}$ as in Expt.~1a.

Figure \ref{fig:prior2}a shows a summary of the elicited priors, in terms of the diversity of $d_{potential}$ and $d_{expected}$.
Biological properties are expected to be \emph{a priori} more prevalent within a kind when present than accidental properties, with additional fine-grained differences within biological and accidental properties.
Like the priors elicited using familiar categories, these priors elicited using unfamiliar categories have diverse shapes (see insets). 
Biological properties ("biological", "vague", and "color" body parts) have prevalence distributions that are bimodal with peaks at 0\% and near-100\% prevalence. 
Interpretations of generics about these properties ($L_1$ model, Eq.~\ref{eq:L1}) update these distributions to concave posteriors peaked at 100\% (Figure \ref{fig:prior2}a; red, blue and green insets); the model predicts these novel generics will be interpreted as implying the property is widespread in the category.
By contrast, accidental properties (both "rare" and "common") follow unimodal prior distributions and update to convex posterior distributions, predicting weaker and more variable interpretations of novel generics for these properties. 

\begin{figure*}
\centering
    \includegraphics[width=\columnwidth]{figs/prevalence-implied-wPriors}
    \caption{Understanding novel generics. (a) Prevalence prior distributions empirically elicited for 40 animal properties.
    Parameters of the structured statistical model---$\phi$ and $\gamma$---reveal quantitative differences in beliefs about the prevalence of conceptually different types of properties (scatterplot). 
    Inset plots show differences in shapes between biological properties (red, green, blue; bimodal) and accidental properties (orange, purple; unimodal).   
  These differences in the prior (darker shade) give rise to the variability of $L_1$ interpretations of generic utterances (lighter shade).
  (b)
  Human interpretation of prevalence upon hearing a generic compared with the $L_1$ model posterior predictive. 
    Participants and the model interpret generics differently for different property types: Generics of biological properties (red, blue, green) have  strong interpretations while generics of accidental properties (purple, orange) are weaker. 
      Error bars denote Bayesian 95\% credible intervals.
  }
  \label{fig:prior2}
\end{figure*}


## Experiment 2b: Interpretations of Novel Generics

Our model of generic interpretation, the pragmatic listener model $L_1$ (Eq.~\ref{eq:L1}), predicts that the interpretations of generics in terms of prevalence should vary as a function of the prevalence prior.
Here, we test the degree to which the predictions based on the empirically elicited prevalence priors for 40 items (from Expt.~2a) match human judgments of how the widespread the property is upon hearing a generic.

### Method

#### Participants

We recruited 40 participants over MTurk to determine how widespread different properties are believed to be upon hearing a novel generic.  
The experimental design is very similar to @Cimpian2010, and we chose to have a sample size at least twice as large as the original study (original n=15). 
All participants were native English speakers. 
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

In order to get participants motivated to reason about novel kinds, they were told they were the resident zoologist of a team of scientists on a recently discovered island with many unknown animals; their task was to provide their expert opinion on questions about these animals.
Participants were supplied with the generic (e.g., "Feps have yellow fur.") and asked to judge prevalence: "What percentage of feps do you think have yellow fur?". 
Participants completed in randomized order 25 trials: 5 for each of the biological properties and 10 for the accidental (described in Expt.~2a).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/asymmetry/asymmetry-2.html}. 

### Analysis and results

The pragmatic listener $L_1$ model provides posterior beliefs about prevalence, given prior beliefs and a generic utterance.
This model has one parameter governing the optimality of the hypothetical speaker $S_1$ in Eq.~2. 
We put the same uninformative prior over this parameter as previously: $\lambda_1 \sim \text{Uniform}(0, 20)$.
We learned about the parameter's \emph{a posteriori} credible values by running 3 MCMC chains of 100,000 samples (removing 50,000 for burn-in) using the Metropolis-Hastings algorithm.
The MAP and 95\% credible interval for $\lambda_1$ are $14.8 [6.4, 19.9]$.

We look at the posterior predictive distribution of $L_1$, integrating out the model parameter.
We first explore two important trends predicted by the pragmatic listener model.
In Figure \ref{fig:exp2b} (solid lines) we see the implied prevalence judgments are predicted (at the property class level) to vary linearly with the \emph{a proiri} expected prevalence. 
A mixed-effects linear model with random by-participant effects of intercept and slope indeed reveals the more prevalent a property is expected to be \emph{a priori}, the stronger the implications of a generic statement ($\beta = 0.57; SE = 0.08; t(39) = 7.12; p < 0.001$).
The prevalence implied by a generic is also predicted to be greater than the \emph{a proiri} expected prevalence (i.e., greater than the prevalence expected among the kinds with the potential to have the property).
A mixed-effects linear model with random by-participant effects of intercept and random by-item effects of intercept and condition reveals implied prevalence after hearing a generic is significantly greater than the \emph{a priori} prevalence ($\beta = 0.17; SE = 0.018; t(39) = 9.7; d = 0.64; p < 0.001$).
As for the quantitative accuracy of the model, on a by-item level, the pragmatic listener model predictions closely align with the human judgments of prevalence for novel generics ($r^2(40)=0.94$, MSE=0.002).
Human participants and our model display the same sensitivity of generic interpretation to details of the property (Figure \ref{fig:prior2}b). 
We now have strong support for both of the major predictive components of our model: generic endorsement, modeled as a speaker $S_2$, and generic interpretation, modeled as a listener $L_1$.

## Experiment 2c: The Asymmetry Between Truth Conditions and Interpretations

There is a surprising d\'{e}colage between the truth conditions and interpretations of generic language: Interpretations are often strong while truth conditions are flexible. 
@Cimpian2010 found that upon reading a generic (e.g. \emph{Glippets have yellow fur.}), participants infer (in an \emph{implied prevalence} task) that the property is widespread (e.g. almost all glippets have yellow fur).
By contrast, participants endorse generics (in a \emph{truth conditions} task) for a wide range of prevalence levels (e.g. even when "30\% of glippets have yellow fur."), thus showing an asymmetry between truth conditions and implied prevalence. 
However, this mismatch is not found for the behavior of quantified statements involving "all" or "most," and 
is significantly reduced for generics of accidental properties  (e.g. \emph{Glippets have wet fur.}).

Below we replicate the basic asymmetry findings of @Cimpian2010 and reveal even more variability in the mismatch between \emph{truth conditions} and \emph{implied prevalence} using the expanded stimulus set from Expt.~2a.
In addition, we now test both our models (generic endorsement [speaker $S_2$] and generic interpretation [listener $L_1$]) in the same experimental paradigm. %models predicts the asymmetry and the context-sensitivity of this phenomenon.

%Expt.~2b revealed how generics imply the property is more widespread than what would be expected \emph{a priori}. 
%Comparing to the average prevalence required to assent is another way to explore how generics exaggerate the evidence. 

### Method

We re-analyze the data from Expt.~2b as the \emph{implied prevalence} data.
The following paradigm is to measure the corresponding \emph{truth conditions}.

#### Participants

We recruited 40 participants over MTurk.  
All participants were native English speakers. 
None of the participants completed Expt.~2b (interpretations of novel generics).
The experiment took about 5 minutes and participants were compensated \$0.60.

#### Procedure and materials

The cover story and materials were the same as in Expt.~2b.
On each trial, participants were given a statement about a property's prevalence within a novel kind (e.g. \emph{50\% of feps have yellow fur.}). Participants were then asked whether or not they agreed or disagreed with the corresponding generic sentence (e.g. \emph{Feps have yellow fur.}). Prevalence varied between 10, 30, 50, 70, and 90\%.

The experiment consisted of 25 trials: 5 trials for each of 5 types of properties measured in Expt.~2a (part, color part, vague part, common accidental, rare accidental). 
Each prevalence level appeared once for each property type (5 prevalence levels x 5 property types). 

### Analysis and results

For both behavioral data and model predictions (Eq.~\ref{eq:S2}) we computed the average prevalence that led to an assenting judgment (the \emph{average prevalence score}), for each property type and participant, following the procedure used by @Cimpian2010.
For example, if a participant agreed with the generic whenever the prevalence was 70\% or 90\% and disagreed at the other prevalence levels, that participant received an \emph{average prevalence score} of 80\%.

For our pair of models, there are two parameters (the two speaker optimality parameters).
We infer them using the same Bayesian data analytic approach as before. 
The MAP and 95\% HPD intervals for $\lambda_1$ is $19.5 [10.5, 19.9]$ and $\lambda_2$ is $0.4 [0.34, 0.49]$.
We then subjected the generic endorsement model to the same procedure as the human data. % subjected our model to the same procedure. 
The speaker model $S_2$ returns a posterior probability of producing the generic, for each level of prevalence. 
We sample a response (\emph{agree} / \emph{disagree}) from this posterior distribution for each prevalence level, simulating a single subject's data.
As with the human data, we took the trials where the model agreed with the generic, and took the mean of the prevalence levels corresponding to those trials, giving us the average prevalence at which the model assented to the generic.
We repeated this for each type of property 40 times to simulate a sample of 40 participants. 
We repeated this procedure 1000 times to bootstrap 95\% confidence intervals.

The generic endorsement model (speaker $S_2$) predicted that \emph{average truth conditions} should not vary appreciably across the different types of properties, consistent with the fact that generics are acceptable for broad range of prevalence levels for all property types.
A similar absence of a gradient was observed in the human data ($\beta = 2.82; SE = 4.02; t(39) = 0.70; p = 0.49$; Figure \ref{fig:exp2b}, dotted lines). 
Interpretations of generic utterances are stronger than their average truth conditions for the biological properties but not for the accidental properties (Figure \ref{fig:exp2b}) with both human data, replicating @Cimpian2010, and the model; the extent of the difference is governed by prior property knowledge (mean prevalence when present $\gamma$, from Expt.~2a).
The listener and speaker pair of models predicts human endorsements and interpretations of novel generic utterances well ($r^2(10) = 0.87$, MSE = 0.008).
Thus, our model predicts that the asymmetry between truth conditions and implied prevalence should hold, but only for properties with the most extreme prior beliefs.

\begin{figure*}
\centering
    \includegraphics[width=\columnwidth]{figs/unfamiliar-asymmetry-predictive-data.pdf}
    \caption{The asymmetry between truth conditions and interpretations. Human judgments and model predictions of prevalence implied by novel generic utterances (implied prevalence task; solid line) and average prevalence that leads to an acceptable generic utterance (truth conditions task; dotted line) as it relates to the \emph{a priori} mean prevalence when present $\gamma$.
    Expectations of prevalence are higher after hearing a generic than before hearing it (solid line compared to $y=x$ line; both for human data and model).
    Generic statements about biological properties, imply that the property is widespread in the category, for both human participants and the model (solid line: red, blue and green). 
    Generics about accidental properties do not result in such a high implied prevalence (solid line: purple and orange).  
	While the implications of generic utterances are highly variable across the different types of properties, the average prevalence that leads to an acceptable generic does not vary, for participants nor the model.
}
  \label{fig:exp2b}
\end{figure*}


# Prevalence is a Predictive Probability

So far, we have shown that property prevalence is sufficient to formalize the semantics of generic statements as an underspecified scalar denotation.
But what is property prevalence?
If generic language is truly conveying generalizations, it would be useful for it to reflect expectations, not just the current statistics in the world.
The current frequency of a property is often a good indicator of future frequency, yet statistics can be distorted by spurious events.
The causal history of a property may be more or less important for implying the property will be present in future situations.
Does generic language communicate prevalence in terms of past frequency or future expectations?

To answer this, we adopt an experimental paradigm used by @Gelman2007 to show that generic language is sensitive to theory-based considerations.
In the original paradigm, participants are told a story about a novel creature (e.g. \emph{dobles}) and a property of that kind (e.g., \emph{having claws}).
Participants are then either told that the creature was born with the property or that it acquired the property through extrinsic means (e.g., by finding claws and putting them on). 
Then, participants are told about an event that either causes the property to disappear (e.g., they drank a chemical and their claws fell off) or that leaves the property intact, and are asked whether or not the generic (e.g. \emph{Dobles have claws}) applies.
The original finding was that adult judgments were sensitive to the origins of the property (i.e., born vs. acquired), and insensitive to the outcome of the event (i.e., property maintained vs. lost): Participants fully-endorsed the generic when it was inborn, and rejected it when it was acquired, regardless of the current prevalence of the property.

In Experiment 3a, we use the same basic paradigm to measure \emph{predictive prevalence}: participants' expectations about future instances of the kind.
We explore the predictions of our truth judgments model, assuming that \emph{predictive prevalence} is what is being communicated.
In Experiment 3b, we use a truth judgment task similar to @Gelman2007 and compare participants' judgments to the model's predicted endorsements.

## Experiment 3a: Predictive Prevalence Elicitation

The design of this experiment is based on a study reported in @Gelman2007 with some slight modification.
%The original study was done on 14 undergraduates.

### Method

#### Participants
We recruited 80 participants over MTurk.  
The experiment took about 3 minutes and participants were compensated \$0.35.

#### Procedure and materials

On each trial, participants read a vignette about a novel creature. For instance,
\begin{quote}
These are dobles. [picture of 10 dobles with claws] Here is how they grew. They grew up with claws. First they were born, then they got bigger, then they were full size. [picture of a doble with claws, getting bigger and bigger; in some vignettes, the animal was first shown hatching out of an egg with the relevant property already visible] Then one day they drank a bad chemical. They got very sick and this is how they looked. [picture of 10 dobles without claws]
\end{quote}
The trial proceeded by participants reading the text, and clicking a button to continue to the next part of the story (at which time, the images changed according to the example above).

Participants saw 4 trials: 2 in which the creatures are born with the property (\textbf{intrinsic origins}), and 2 in which the creatures are shown discovering and acquiring the property (\textbf{extrinsic origins} e.g., painting themselves brown).
This was crossed with either the creatures drinking a "bad chemical" and losing the feature, or drinking a "yummy drink" and maintaining the feature.
The outcome of this event determined the final presentation of images that the participant saw (e.g., either 10 dobles with claws or 10 without).

While this final screen was present, we measured \emph{predictive prevalence} by telling participants: "A new doble was born today. When it becomes full grown, how likely is it that it would have claws?"
Participants responded using sliders ranging from "very unlikely" to "very likely".

We used 2 different types of properties: colors (e.g. \emph{Lorches are green.}) and body parts (e.g. \emph{Dobles have claws}).
For each type of property, there were approximately 8 different exemplars (different colors or different body parts for different creatures).
The creatures were either birds, bugs, or fish, with randomly sampled physical dimensions (e.g., sizes of body or tail).
The experiment in full can be viewed at \url{http://stanford.edu/~mtessler/generics/experiments/predictive/predictive-elicitation-1-elicitation.html}.


### Results and truth judgment predictions

The average predicted prevalences for the 4 experimental conditions are shown in Table \ref{tab:predictive}.
We observe a main effect of origins, such that when participants read that the creatures had the property from birth, future creatures  are much more likely to have the property as compared to when the property is acquired.
We see that, in our paradigm, participants are also sensitive to the outcome of the event. %This is surprising because @Gelman2007 observed no effect of the event outcome on generic endorsement.
When participants observe a creature who loses the property by drinking a chemical, they report future members of the category are less likely to have the property.
This inference may be driven by inferences about the property (e.g., that the property could be an unstable property, if you can lose it simply by drinking something) or by inferences about the event (e.g., participants may believe this "chemical drinking" event is a relatively normal event, and thus it could happen in the future).

\begin{table}
\centering
\begin{tabular}{l|l|r|r|r}
\hline
Origins & Event Outcome & Collapsed Mean [95\% CI]  & Color  [95\% CI] & Body Part [95\% CI] \\
\hline
Extrinsic & Lost  & 0.15 [0.10, 0.21]& 0.13 [0.07, 0.21] & 0.17 [0.09, 0.26] \\
\hline
Extrinsic & Maintained & 0.32 [0.24, 0.39] & 0.24 [0.16, 0.35] & 0.38 [0.27, 0.50] \\
\hline
Intrinsic & Lost  & 0.69 [0.62, 0.76] & 0.73 [0.63, 0.83] & 0.66 [0.56, 0.75] \\
\hline
Intrinsic & Maintained  &0.95 [0.94, 0.97] & 0.96 [0.94, 0.98] & 0.94 [0.91, 0.97] \\
\hline
\end{tabular}
\caption{Predicted prevalence for the four experimental conditions of Expt.~3a. Right two columns show the summaries broken down by the two types of properties used in the experiment.}
\label{tab:predictive}
\end{table}

We use these predicted probabilities as the prevalence $x$ that the speaker model is trying to communicate: $S_2(u\mid x)$, and examine the model's predicted truth judgments.
We explore the model's predictions for each origin and event outcome, as well as when the data is split by property type (color vs. body parts). 
For priors $P(x)$, we use the body part and color priors elicited in Expt.~2a.
We see that the model predictions track closely the predicted prevalence (Figure \ref{fig:dobles}a, top, compare with predicted prevalence in Table \ref{tab:predictive}).
This is because both color and body part priors are relatively broad, and hence when the property is (predicted to be) more prevalent, the generic has a higher probability of applying (see schematic predictions from Figure \ref{fig:schematic-unif} "have wings" for comparison).
We also see that the model predicts a subtle by-item difference, such that the influence of the event outcome (lost or maintained) on generic endorsement is predicted to be stronger for body parts than for color terms (Figure \ref{fig:dobles}a, bottom).
This prediction is mostly due to the predicted prevalence for the conflict conditions (intrinsic-lost and extrinsic-maintained) being subtly different (Table \ref{tab:predictive}, right-most columns).

<!--
%\footnote{Note that our model's predictions do not deviate substantially from the predicted prevalence because there are only two different priors being used, and the shapes of those distributions do not vary appreciably (see Figure \ref{fig:prior2}). \ndg{huh? two priors does not explain why the model tracks predicted prevalence. instead it's that the priors are broad and unimodal or something right?}
%}.
-->

Our model thus makes two novel predictions for generic endorsement in the paradigm by @Gelman2007.
We predict that in addition to the main effect of origins, we should see a second main effect of event outcome.
Second, we predict that this effect should be slightly stronger in the case of color properties than in the case of body part properties.


## Experiment 3b: Truth Judgment Task

In this experiment, we test the predictions of our model using a truth judgment measure in the same paradigm.

### Method

#### Participants
We recruited 80 participants over MTurk.  
The experiment took about 3 minutes and participants were compensated \$0.35.
None of the participants completed Experiment 3a as well.

#### Procedure and materials
The procedure and materials are exactly the same as in Expt.~3a, with the exception of the dependent measure.
After reading each vignette, participants were asked: "Do you agree or disagree that: \textsc{generic statement} (e.g. Dobles have claws)".
Participants responded by choosing one of two radio buttons corresponding to agree or disagree.

### Results and discussion

<!--
%                                         Estimate Std. Error z value        Pr(>|z|)    
%(Intercept)                               -2.9444     0.5130  -5.740 0.0000000094624 ***
%event_outcomemaintained                    2.6931     0.5603   4.807 0.0000015342262 ***
%originsintrinsic                           3.6753     0.5658   6.496 0.0000000000824 ***
%event_outcomemaintained:originsintrinsic   0.2396     0.9400   0.255           0.799    
-->

Our pragmatics model with the elicited predicted prevalence from Expt.~3a made two novel predictions for this experiment: (1) in addition to a main effect of origins, we would find a main effect of event outcome; (2) this effect would be stronger for body part properties than for color properties.
As predicted, we found two main effects (Figure \ref{fig:dobles}b, top).
The main effect of property origins replicated ($\beta=3.6, SE=0.57, z=6.5, p<1\text{e}10$): participants were more likely to endorse the generic when it was about a property that the creature was born with.
In addition, we find a second main effect of event outcome 
 ($\beta = 2.69, SE = 0.56, z=4.8, p<1\text{e}5$): participants were more likely to endorse the generic when the property was maintained than when it was lost.\footnote{The fact that we find a second main effect of event outcome in addition to origins, whereas the original only found a main effect of origins, makes it worth noting the differences between our paradigm and the original study by Gelman \& Bloom.
In the original study, the first sentence of each vignette used the possessive "my": "These are my dobles.".
At the end of each vignette, the original study had participants judge two statements in counterbalanced order: "Do my dobles have claws?" and "Do dobles have claws?"
Finally, the original sample size was 14; ours was 80. 
}


When we break down the results by item, we see that this effect is  stronger for body part properties than for color properties (Figure \ref{fig:dobles}b, bottom).
The endorsement of a generic for color properties (e.g. \emph{Lorches are green}) seems to be less sensitive to the outcome of the event (i.e. Lorches losing their color as a result of drinking a chemical).
This may be due to participants' intuitive theories of properties and their stability (skin color is more stable than body parts like feathers).
Indeed this difference is apparent in the predictive prevalence task (Expt.~3a).
For the 8 data points of generic endorsement based on origins, outcome, and property type, our model's predictions match the data well ($r^2(8) = 0.96$).
We, thus, elaborate our theory:
The semantics of generics can be understood as a threshold on property prevalence, and this prevalence is a speaker's subjective belief about what is likely to be the case in the future.

\begin{figure*}
\begin{tabular}{l l}
(a) $S_2$ model predictions & (b) Human endorsement of generic statements \\
\\
\centering
    \includegraphics[width=0.5\columnwidth]{figs/dobles-model.pdf} & 
        \includegraphics[width=0.5\columnwidth]{figs/dobles-results.pdf} \\
      \includegraphics[width=0.5\columnwidth]{figs/dobles-model-byItem.pdf} & 
      \includegraphics[width=0.5\columnwidth]{figs/dobles-byItem-results.pdf} \\

\end{tabular}
    \caption{
    Prevalence is a predictive probability.
    (a) Truth judgment model predictions given the predicted prevalence elicited in Expt.~3a.
    (b) Average endorsement of the generic statement in Expt.~3b (replication of Gelman and Bloom, 2007).  
    Bottom row shows data and predictions broken down by property type.
  }
  \label{fig:dobles}
\end{figure*}

# General Discussion

Generic language is the simple and ubiquitous way by which generalizations are conveyed between people.
Yet the dramatic flexibility of generic language has confounded psychologists, linguists and philosophers who have tried to articulate what exactly generic statements mean. 
We evaluated a theory of generic language derived from general principles of language understanding using a simple, but uncertain, basic meaning---a threshold on property prevalence.
Our formal model is a minimal extension of the RSA theory of language understanding, together with an underspecified threshold semantics.
The model was able to explain two major puzzles of generics: their extreme flexibility in truth conditions and the contrastingly strong interpretation of many novel generics.
Both of these phenomena were revealed to depend in systematic ways on prior knowledge about properties.
This prior knowledge was revealed through Bayesian model analysis to be structured, providing a promising bridge to conceptual accounts of generic language.
To understand the nature of the underlying prevalence scale, we showed that generic language is about speakers' expectations of future prevalence, and not necessarily what the current state of the world is like. 
Across all experiments, the formal model predicted the quantitative details of participants' judgments with high accuracy.

There have been numerous demonstrations arguing that statistics (e.g., property prevalence) are insufficient to explain generic meaning [@Gelman2002; @Gelman2007; @Cimpian2010; @Cimpian2010c; @Khemlani2012; @Prasada2013].
In these experiments, the prevalence considered is only the prevalence of the property for the target category [e.g., the percentage of birds that lay eggs; @Khemlani2012; @Prasada2013], what we have referred to as \emph{within-kind prevalence}. 
Indeed, this simple statistic also fails to explain our data (Figure \ref{fig:commongenerics}b, right).
Our formal model of pragmatics, by contrast, considers not only within-kind prevalence, but a listener's prior beliefs about prevalence across kinds in order to arrive at a meaning for a generic utterance.
By establishing the validity of a semantics based on prevalence alone, we provide a formalism to learn about categories from generic statements. 
Further, since prevalence is a probability, our model can take information conveyed with a generic and be naturally extended to make predictions about entities in the world or support explanations of events or behavior.

## The Comparison Class

In this paper, we proposed a model for understanding generic language that relies upon interlocutors' belief distribution over the statistics of the property to accurately arrive at generic interpretation identical to that of human participants. 
The prior belief distribution for a property (e.g., lays eggs), is a distribution over kinds (and their associated prevalence of the property).
These other kinds form a \emph{comparison class} against which the target kind is evaluated. 
$P(x)$ is always relative to a category of categories, a comparison class $K$: $P_K(x)$.

Throughout our experiments in this work, we have focused on sentences about animals. 
In addition to being the main focus of past theoretical and empirical work, focusing on animals is methodologically convenient as the comparison class for generics about animals is quite naturally \emph{animals}.
When we look beyond generics about animals, deciding what goes into a comparison class becomes less clear.
There are some hints that the comparison class can be derived with respect to the property [@Keil1979], but may involve pragmatic reasoning as well. 
For example, the statement "iPhones are useful" could be in comparison to other forms of technology (like a desktop computer), while "iPhones are heavy" could really only be informative relative to other handheld devices.

The incorporation of a comparison class into the study of generic language might help elucidate other puzzles concerning generics.
Recent work in philosophy and linguistics, for instance, suggest generic language is context-sensitive [@Nickel2008; Sterken2015].
@Nickel2008 points out that \emph{Dobermans have floppy ears} may be true in the context of a discussion of evolutionary biology but that \emph{Dobermans have pointy ears} is true in a discussion of dog breeding.
Our theory provides a hint from where to begin to understand this context sensitivity: the comparison class.
Different conversational contexts could bring to mind different comparison classes, in a way analogous to the context-sensitivity of  gradable adjectives (e.g., \emph{tall}). 
Hearing that "Abigail is tall" means different things is Abigail is 20 years old or if she is 4. 
Future work will be needed to explore whether a pragmatic inference approach is also relevant to establishing the comparison class, and what background knowledge about properties, categories, and context is relevant.


## Generic Identification

Throughout this paper we treated the bare plural construction as a generic utterance with a threshold semantics: $\denote{\text{K F}}(x, \theta)=x>\theta$.
The bare plural construction can also indicate a specific plural predication.
For example, "Dogs are on my lawn" picks out a specific group of dogs, while "Dogs have fur"  does not [@Carlson1977].
The problem of \emph{identifying} a generic meaning from a bare plural construction is itself a challenging problem because generic meaning can be signaled using a diverse array of morphosyntactic cues.

@Declerck1991 suggests that generic and non-generic bare plurals can be treated in the same way, and that pragmatic considerations alone may resolve interpretative differences. 
Indeed it does appear that knowledge of the properties under discussion (e.g., the state of being on a front lawn; the state of having fur) could facilitate the generic identification process.
Other pragmatic factors, like knowledge of the identity of the speaker (e.g., a teacher vs. a veterinarian), can also disambiguate generic and non-generic meaning [@Cimpian2008].
Recent work suggests that utterances that fail to refer to specific entities or events could pragmatically imply generic meaning [@Crone2016cogsci].
Incorporating these insights about generic identification into an information-theoretic, communicative perspective is a natural extension of this work.

## Implications for Conceptual Structure

Previous psychological and philosophical work on generics has looked beyond prevalence and focused on conceptual distinctions and relations [@Gelman2003; @Prasada2013; @Leslie2007; @Leslie2008]. 
Prasada has argued for a distinction between \emph{characteristic} properties (e.g.~\emph{Diapers are absorbent.}) and \emph{statistical} properties (e.g.~\emph{Diapers are white.}).
Leslie suggests information that is striking (e.g.~\emph{Tigers eat people.}) is useful and thus permitted to be a generic.
Gelman outlines how generics tend to express \emph{essential} qualities that are relatively timeless and enduring. 
Where in the prevalence-based semantics could such conceptual distinctions come into play?

Our approach makes the strong claim that beliefs about predicted prevalence are the connective tissue between conceptual knowledge and generic language.
That is, the effect of conceptually meaningful differences on generic language is predicted to be mediated by differences in corresponding prevalence distributions.
It is important to note that our approach is based on \emph{subjective} probability, and not mere frequency.
Indeed, we elucidated in Expt.~3 that using participants' \emph{predictions} of probability in our formal model perfectly track generic endorsement, when the present frequency would make the wrong prediction.

The focus on subjective, predictive probability casts new light on puzzles surrounding accidentally-true situations.
An example is the statement "Supreme Court Justices have even social security numbers", which is predicted by linguists to be rejected even if every single Supreme Court Justice has an even social security number [@Cohen1999].
Our explanation is that abstract intuitive theories lead us to reject observed frequencies in forming our subjective probabilities.
That is, because we may believe selection for the Supreme Court is not influenced by one's social security number, we would assign roughly 50\% subjective probability to the \emph{next} justice having an even number.
Thus, we would still reject the generic \emph{Supreme Court Justices have even social security numbers}, because the predictive prevalence would not be any different for Supreme Court Justices than any other profession.^[Our perspective makes the intriguing prediction that if we learned much more surprising information, we might be compelled to revise our theory and then accept the generic. For instance, if every justice in history had \emph{prime numbered} social security numbers (a more suspicious coincidence), one might appeal to a conspiracy, which \emph{would} have predictive consequences.]

Turning back to conceptual relations and structure, it is natural to ask when subjective probabilities might reflect conceptual knowledge?
We found that empirical prevalence distributions are structured in a way that reflects intuitions about causal mechanisms underlying different properties; the differences in shape of these distributions in turn led to variable endorsements and interpretations of generic sentences. 
It is plausible that richer conceptual knowledge also influences these distributions, such as higher-order conceptual knowledge about the nature of properties and categories [@Gelman2003; @Keil1992].
Indeed, it has been argued that conceptual structure in general, including higher-order abstractions, can be captured by probabilisitic causal models and their generalizations [@pearl1988probabilistic; @Gopnik2003theory; @Goodmanconcepts].
Future work will be needed to explore whether probabilistic representations of conceptual knowledge can capture the relations identified in other accounts of generics (such as characteristic, essential, and striking properties), and whether the effect of these relations can then be adequately captured via their impact on subjective prevalence.


## Communicating probabilities

-- KT saw that people don't do well reasoning about explicit probabilities, and concluded that people are bad with probabilities. But what if people are good, and just the way of conveying them is more vague? We can communicate 75% (assuming the priors are consistent with that), and we can communicate 30% (when the priors are consistent). So people do show sensitivity to these probabilities, suggesting that people are good with probabilities (CITE all developmental work)


## Relationship to other theories of generics

-- "default generalization" -- very close, the listener is doing a generalization, but not generalization based on evidence, but by reasoning about the threshold
-- "normalcy" approaches -- prevalence in a cognitive model is a posterior predictive probability, which incorporates background knowledge in order to make predictions about the future. it's likely that what is "normal" is captured in much of our intuitive theories of other domains
-- generic as indexical (sterken) -- She has two free variables: quantificational force & lexical restriction. Quantificational force is like the uncertain threshold
-- majority-based approaches using domain restriction: Birds lay eggs means "Most female birds lay eggs" (Cohen, 1999). Our model didn't have to rely upon domain restriction at all, but it's possible there's a mathematically similar model that uses domain restriction and a fixed threshold. You would need a theory of how the domain gets retricted; this would likely rely upon the same principles (true + informative)




### The Problems with Communicating Generalizations

Thus, in order to define a generalization about a category, there must be some corresponding concrete particular instance of that category from which the generalization is formed. 
For example, if an agent observes $n$ instances of \textsc{dog} ($d_1, d_2, ..., d_n$)
For example, if an observer forms a generalization about \textsc{dogs}, she must some way of determining when she is in an instance of the category \textsc{dogs}: She must be able to *individuate*.
In Hume's words, generalizations are predictions about "instances of which we have had no experience resemble those of which we have had experience" [@HumeTHN].


Generalizations can be made about almost anything.
Here, we restrict our focus to generalizations about categories (whose linguistic expression is known as *generic language*), which have been the primary focus of psychologists, linguists, and philosophers.
We posit that our analysis should expect to generalizations about events (habitual language) as well as generalizations about causal forces (causal language).



Generics express a relation between a kind K (e.g., \textsc{robins}) and a property F (e.g., \textsc{lays eggs}), such that the property can also be said to be applicable of an individual (i.e., the bird in my backyard lays eggs).
Bare plural statements (e.g., \emph{Robins lay eggs}) tend strongly to yield a generic meaning [@Carlson1977], though other forms can express such a meaning sometimes (e.g., \emph{A mongoose eats snakes.}).

Given that generics express a property that can be applied to individuals, it would seem intuitive that the number of individuals with the property would be what makes the statement true or false.
Counter-examples like \emph{Mosquitos carry malaria} and \emph{Birds lay eggs} v. \emph{Birds are female} stifle such intuition. 
Semantic theories that appeal to the statistics of the world (i.e., how many Ks have F) try to rescue the notion that a generic expresses something like \emph{Ks, in general, have F}, where \emph{in general} is often either restricted to particular domains or is calculated with particular distortions.
Alternative, \emph{conceptually}-based theories take a generic to mean \emph{Ks, in virtue of being the kind of things that Ks are, have F}.
These accounts appeal to structured, conceptual representations in deciding what kinds of generic statements are true or false.
Statistical and conceptual theories express the major contrasting views of the truth conditions of generic statements [@Carlson1995essay].^[We use the terms statistical and conceptual to refer to what @Carlson1995essay referred to as "inductive" and "rules and regulations" views, respectively.]


### Statistical Accounts of Generics

Statistical accounts take the \textbf{property prevalence} to be fundamental: \emph{Birds lay eggs} means \emph{Birds, in general, lay eggs}. 
Of course, birds do not in general lay eggs (it's only the adult, female ones that do).
The primary way of dealing with such issues is to posit domain restrictions ("implicitly, we are only talking about the females") when there are "salient partitions" [@Carlson1995].
The most fully-developed theory on this front is due to @Cohen1999. 
Let's first introduce some notation:

For a given kind $K$ (e.g.~\textsc{robins}) and property $F$ (e.g.~\textsc{lays eggs}), we refer to the probability that an instance of kind $K$ has property $F$, that is $P(F\mid K)$, as the \emph{prevalence} of $F$ within $K$.
Logical quantifiers can be described as conditions on prevalence (i.e.~\emph{some} is $P(F\mid K)>0$, \emph{all} is $P(F\mid K)=1$). 
Assuming the generic relates to the property prevalence, then the simplest meaning would similarly be a threshold on prevalence: $P(F\mid K)>\theta$. @Cohen1999 takes $\theta = 0.5$; that is, \emph{Robins lay eggs} is roughly taken to mean \emph{More than half of (relevant) robins lay eggs}. 

Cohen introduces constraints into the computation of prevalence: $P(F\mid K)$. In particular, prevalence is computed with respect to a \emph{partition set}. 
For example, the property \emph{lays eggs} induces a set of alternatives that all have to do with procreation (e.g. \emph{gives birth to live young}, \emph{undergoes mitosis}, ...).
The individuals that enter into the prevalence computation are only those individuals that would satisfy one or another alternative. 
That is, the only individuals under consideration are the female members of kinds because only the female members can plausibly satisfy one or another of the alternatives.
The inference that this sort of domain restriction is applied, as well as other constraints posited in the theory, are thought to be a pragmatic inference, though specifying the details of the pragmatics is beyond the scope of Cohen's theory.
Without a well specified theory of pragmatics, however, we have no general solution to map from property prevalence to truth judgment.
Conceptual information seems to be behind the pragmatic inference, but how Cohen's and other prevalence-based accounts relate to such knowledge remains obscure [@Carlson1995essay].
^[However, see @Cohen2004 for a discussion of how his semantic constraints relate to different kinds of generics and different kinds of conceptual representational frameworks found in cognitive science.]

### Conceptual Accounts of Generics

Conceptual accounts of generics emphasize the structure of generic knowledge [@Prasada2000], and view generic utterances as the way of expressing special mental relationships between kinds and properties [@Leslie2008; @Prasada2012].
From this perspective, \emph{Bishops move diagonally} because those are the rules of the game, not because they \emph{tend to} move diagonally.
How do we come to know the rules of the game, though?

@Leslie2007's influential theory posits that generics are tied to a "default mode of generalization". 
This "default mode" comes equipped with the ability to single-out \emph{striking properties} (e.g. properties which are dangerous or appalling) as particularly useful aspects of the world to know about. 
Hence, \emph{Mosquitos carry malaria} is true because carrying malaria is striking, and thus, a useful bit of information to convey.
The default mode can also distinguish "negative" counter-instances of a property   (e.g. a bird that doesn't lay eggs) from "positive" counter-instances (e.g. a hypothetical bird that bears live young).
Generics are much less reasonable when positive counter-instances exist. Hence, \emph{Birds are female} seems weird because "being male" is a positive counter-instance of "being female", but since there are no birds that bear live young,  \emph{Birds lay eggs} is fine.

Parallel work in the psychology of concepts supports this perspective.
@Prasada2006 and later @Prasada2013 distinguish between \emph{principled}, \emph{statistical}, and \emph{causal} relations within concepts. 
Generics like \emph{Birds lay eggs} (in which only a minority of K have F) exhibit characteristics of principled connections (operationalized by endorsement of the phrase \emph{In general, Ks have F}), supporting @Leslie2007's typology.
Striking generics (e.g. \emph{Mosquitos carry malaria}) show characteristics of \emph{causal connections} (operationalized using the phrase \emph{There is something about Ks that cause them to F}). 
The fact that generics about different properties license different kinds of inferences is taken as evidence that the generics themselves represent different kinds of relations. Statistical information takes a backseat to the conceptual structure.







## Other philosophical puzzles

-- Books are paperbacks. (failure of the constrast class)
-- Birds lay eggs and are female vs. Elephants live in Asia and Africa
--> can use pragmatics to resolve if the property is conjunctive or not. 
-- Women are submissive. (generics + politeness mechanism, some states have lower value than others)
-- Muslims are terrorists (vs. Mosquitos carry malaria)



# Conclusion

It might seem paradoxical that a part of language that is so common in communication and central to learning should be vague. 
Shouldn't speakers and teachers want to express their ideas as crisply as possible?
To the contrary, underspecification can be efficient, given that context can be used to resolve the uncertainty [@Piantadosi2012].
In our work, context takes the form of a listener and speaker's shared beliefs about the property in question. 
By leveraging this common ground, generics provide a powerful way to communicate and learn generalizations about categories, 
which would otherwise be difficult or costly information to learn through direct experience.

The dark side of this flexibility is the potential for miscommunication or deceit: A speaker might assert a generic utterance that he himself would not accept, conveying a too-strong generalization to a na\"{i}ve listener.  
Our model predicts this potential particularly for properties which, when present, are widespread in a category---we showed that biological properties are believed to have this distribution, but many properties of social categories may as well [@Cimpian2011a; @Cimpian2012b; @Rhodes2012].
Disagreements are also predicted when interlocutors fail to share background assumptions:
differences in the within-kind prevalence, the prior distributions on prevalence, or the comparison class.
For example, there is considerable disagreement as to whether or not "Humans cause global warming".
Our theory predicts this disagreement may be the result of differences in the estimated \emph{causal power} of humans influencing global warming as well as the causal power of \emph{other forces} (e.g., plate tectonics) on climate change.
This is a promising area for future research.


Categories are inherently unobservable. 
You cannot see the category \textsc{dog}, only some number of instances of it.
Yet we easily talk about these abstractions, conveying hard-won generalizations to each other and down through generations.
The theory presented here gives one explanation of how we do so, providing a computational perspective on how category generalizations are conveyed and how beliefs play a central role in understanding language.



# References 

<!-- \setlength{\parindent}{-0.1in}  -->
<!-- \setlength{\leftskip}{0.125in} -->
<!-- \noindent -->

<div id = 'refs'></div>

```{r child = 'appendix-prevalencePrior.Rmd'}
```

```{r child = 'appendix-cueValidity.Rmd'}
```

