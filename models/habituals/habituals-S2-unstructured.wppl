// time ~/webppl-fork/webppl habituals-S2-unstructured.wppl --require utils 1

var chain = last(process.argv) // load index as last command line index

var responseDictionary = { "agree-key": 1, "disagree-key": 0 };

var dataPath = "data/"
var targetUtterance = "habitual"

var priorFile = dataPath + "ff-prior-n50.csv";
var truthJudgmentDataFile = dataPath+"tj-2-logtimes.csv";

var d0 = dataFrame(utils.readCSV(priorFile).data, ["-2","-1.5","-1","-0.5","0","0.5","1","1.5","2","2.5","3","3.5","4","4.5","5","5.5","6","6.5","7","7.5","8","8.5","9"]);
var d2 = dataFrame(utils.readCSV(truthJudgmentDataFile).data,
				["n_times", "log_times"]);

var data = {
	speaker: map(function(d){
			extend(d, {
				// rate: d.n_times / 5,
				// roundedRate: utils.closest(midBins, d.n_times / 5),
				rate: Math.log(d.n_times / 5),
				roundedRate: utils.closest(midBins, Math.log(d.n_times / 5)),
				alignedResponse : responseDictionary[d.response]
			})
		}, d2.slice(0, d2.length - 1)),
		prior: _.fromPairs(map(function(d){
				return [d.action, map(function(b){
					d[b] + 0;
				},["-2","-1.5","-1","-0.5","0","0.5","1","1.5","2","2.5","3","3.5","4","4.5","5","5.5","6","6.5","7","7.5","8","8.5","9"])]
			}, d0.slice(0, d0.length -1)))
};

var items = _.keys(data.prior);
var utterancePrior = Infer({model: function(){
	// return uniformDraw(["habitual","opposite habitual"])
	// return uniformDraw(["habitual","habitual is false"])
	return uniformDraw([targetUtterance,"silence"])
}});

var meaning = function(utt,state, theta) {
  // return utt=="habitual"? state > theta[utt] :
  //        utt=="habitual is false"? state <= theta[utt] :
	// 			 utt=="opposite habitual"? state < theta[utt] :
 return utt=="habitual"? state > theta :
        utt=="habitual is false"? state <= theta :
				 utt=="opposite habitual"? state < theta :
         utt=='silence'? true :
         utt=='some'? state > 0 :
         true
}

// data["prior"]["wears socks"]


// display(priorUnnormed)
// display(sum(priorUnnormed))
// display(sum(prior))
// prior


var model = function(){

	var speakerOptimality = {
		s1: uniformDrift({a: 0, b: 20, width: 2}),
		s2: uniformDrift({a: 0, b: 5, width: 0.5})
	}

	// var c = 0.5;//uniformDrift({a: 0 , b: 1, width: 0.1});
	//
	// var utterancePrior = Infer({model: function(){
	// 	return flip(c) ? "habitual" : "silence"//"habitual is false"
	// }});

	var alpha = uniformDrift({a: 0, b: 5, width:0.5})

	foreach(items, function(i){

		var prior = normalize(map(function(b){
			return uniformDrift({a: 0, b: 1, width:0.1})
		}, midBins))

		var scaledPrior = normalize(map(function(pi){
			return Math.pow(pi, alpha)
		}, prior))

		var observedCounts = data["prior"][i];

		// display(Multinomial({n: sum(observedCounts), ps: prior}).score(
			// observedCounts
		// ))
		observe(Multinomial({n: sum(observedCounts), ps: scaledPrior}),
				observedCounts)

		var itemData = _.filter(data.speaker, {habitual: i});

		var statePrior = Categorical({ vs: midBins, ps: prior })

		/// RSA model
		var listener0 = cache(function(utterance, theta) {
		  Infer({model: function(){
		    var state = sample(statePrior)
		    var m = meaning(utterance, state, theta)
		    condition(m)
		    return state
		 }})}, 10000)

		var speaker1 = cache(function(state, theta) {
			Infer({model: function(){
		    var utterance = sample(utterancePrior);
		    var L0 = listener0(utterance, theta);
		    factor(speakerOptimality.s1 * L0.score(state))
		    return utterance
			}})}, 10000)

		var listener1 = cache(function(utterance) {
			Infer({model: function(){
		    var state = sample(statePrior);
		    var theta = targetUtterance === "habitual" ? sample(thetaPrior) : -99;
		    var S1 = speaker1(state, theta)
		    observe(S1, utterance)
		    return state
			}})}, 10000)

		var speaker2 = function(freq){
			Infer({model: function(){
				var utterance = sample(utterancePrior);
		    var L1 = listener1(utterance)
		    factor(speakerOptimality.s2 * L1.score(freq))
				// var _kl = KL(speakerBeliefs, L1, speakerBeliefs.support());
				// factor(speakerOptimality.s2  * -1 * _kl)
		    return utterance === targetUtterance ? 1 : 0
		 }})}

		var observedFrequencies = levels(itemData, "roundedRate");

		foreach(observedFrequencies, function(freq){
			// display(freq)
			var freqData = _.filter(itemData, {roundedRate: freq});
			var s2prediction = speaker2(freq);
			// display(map(function(s){return [s, s1prediction.score(s)]},
			//  s1prediction.support()))

			mapData({data:freqData}, function(d){
				// display(d)
				// display("speaker score = " + s1prediction.score(d.alignedResponse))
				observe(s2prediction, d.alignedResponse)
			})

			// query.add(["predictive", i, freqData[0]["time_period"]], [
			// 	freq, "s2", expectation(s2prediction),
			// 	"NA",  "NA",  "NA",  "NA" ])

			query.add(["predictive", i, "s2", freqData[0]["time_period"], freq], expectation(s2prediction));

		})

		foreach(_.range(0, prior.length), function(pi){
			query.add(["prior",i, midBins[pi], pi, "NA"], prior[pi])
		})

	})

	// query.add(["param","speakerOptimality","s1","na"], speakerOptimality.s1)
	query.add(["param","speakerOptimality","s1","NA", "NA"], speakerOptimality.s1)
	query.add(["param","speakerOptimality","s2","NA", "NA"], speakerOptimality.s2)
	// query.add(["param","habUttPrior","NA","NA", "NA"], c)
	query.add(["param","alpha","NA","NA", "NA"], alpha)

	// query.add(["param", "speakerOptimality", "s1"], [
	// 	 speakerOptimality.s1, "NA", "NA",
	// 	"nullParams_mu",  nullParams.mu,
	// 	"nullParams_sigma",  nullParams.sigma])

	return query
}
// // data.speaker
// // Infer({model: function() { sample(Categorical({vs: midBins, ps: data["prior"]["wears socks"]}))}})
// // Infer({model: function() { sample(Categorical({
// // 	vs: midBins,
// // 	ps: map(function(p){ Math.pow(p, 0.3) }, data["prior"]["wears socks"])
// // }))}})
//
//
var totalIterations = 100000, lag = 20;
var mhiter = totalIterations/lag, burn = totalIterations / 2;
// var lag = 0;
var outfile = 'results-habituals-S2-ffPriorDirScaling-silenceAlt-'+ totalIterations+'_burn'+burn+'_lag'+lag+'_chain'+chain+'.csv'

var posterior = Infer({
  model: model,
  method: "incrementalMH",
	// kernel: {HMC: {steps:5, stepSize: 0.01}},
  samples: mhiter, burn: burn, lag: lag, verbose: T,
	verboseLag: totalIterations/100,
	stream: {
		path: "results/" + outfile,
		header: [
			"type", "item", "cat", "param", "gender", "val"
		]
	}
})

// utils.writeQueryERP(posterior, "results/" + outfile,
	// ["type", "item", "cat", "param", 'freq', "val"])
//
display("written to " + outfile)
