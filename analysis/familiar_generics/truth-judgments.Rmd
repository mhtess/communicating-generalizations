---
title: "familiar-truthjudgments"
author: "mht"
date: "June 15, 2016"
output: html_document
---

```{r helper_functions}
library(coda)
library(data.table)
library(langcog)
library(lme4)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)

histToSamples <- function(df, samples){
  rows <- rep.int(seq_len(nrow(df)), times = round(df$Probability * samples))
  cols <- names(df) != "Probability"
  df[rows, cols, drop = FALSE]
}

## for Bayesian credible intervals
estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
options("scipen"=10)  

setwd("~/Documents/research/generics-paper/")
```

# Human judgments

```{r human_data}

data.path <- "data/familiar_generics/"

tj <-read.csv(file = paste(data.path,'truth-judgments.csv',sep=''))
# catch trials
c <- read.csv(file = paste(data.path,'truth-judgments_catch-trials.csv', sep=''))

# subject info-- to find native language
s<- read.csv(file = paste(data.path,'truth-judgments_subject-information.csv', sep=''))

# excluding the nonenglish native speakers has no effect on results
# nonenglish<-c(1,4,19,38,39)

# participants who failed the catch trials
catch.ids <- c[c$pass==0,]$workerid

#tj <- tj %>% filter(!(workerid %in% c(nonenglish, catch)))
tj <- tj %>% filter(!(workerid %in% catch.ids))
```

Bootstrap confidence intervals

N.B.: uses functions from [library(langcog)] (https://github.com/langcog/langcog)

```{r}
tj.bs <- tj %>% 
  mutate(response = as.numeric(response=='agree-key')) %>%
  group_by(sentence) %>%
  multi_boot_standard(column = "response")


# adjust naming of sentence to correspond to that used in the prior data
tj.bs$sentence <- gsub('&quotechar','', tj.bs$sentence)
tj.bs$sentence <- gsub('lyme','Lyme', tj.bs$sentence)

# order sentences by increasing endorsement
tj.bs$sentence<-with(tj.bs, reorder(sentence, mean, function(x) x))


ggplot(data=tj.bs, aes(x = sentence, y = mean-0.5,
                       ymin = ci_lower-0.5,ymax = ci_upper-0.5))+
  geom_bar(stat='identity',position=position_dodge(), alpha=0.8,
           fill='grey19')+
  geom_errorbar(width=0.5, size = 1.5,
                color='black')+
  xlab("")+
  ylab("\n proportion of participants who agree")+
  scale_y_continuous(breaks=c(-0.5,0,0.5),labels=c("0","0.5","1"))+
  coord_flip()
```

Empirically, we observe a continuum of endorsements, consistent with similar tasks that use Likert scales (e.g. Prasada, et al. 2013). 


## Manipulation check

Do true generics get endorsed more?
```{r manipulation.check}

# MHT's truth judgments of sentences
sentence.class<- data.frame(sentence = levels(tj$sentence),
                            class = c("t","t","t","f","i",'t',
                                      'f','i','t','f','i','t',
                                      'f','f','t','i','f','t',
                                      'i','t','f','f','i','i',
                                      't','t','f','f','t','f'))

tj.s<-left_join(tj, sentence.class, by='sentence')
tj.s$class<-factor(tj.s$class, levels=c('i','t','f'),
                   labels = c("indeterminate", "true", "false"))

rs0<-glmer(data=tj.s, response~ class + (1  | workerid), family='binomial')
summary(rs0)
```


The 30 generic sentences fell into 3 categories as predicted: definitely true, definitely false, and neither true nor false. We entered participants' agreement judgments into a mixed-effect logistic regression with random by-participant effects of intercept. This \emph{a priori} distinction was a significant predictor of the eventual truth judgments: true generics were significantly more likely to be agreed with than the indeterminate generics ($\beta = 3.14; SE = 0.15; z = -20.9$) and false generics were significantly less likely to be agreed with than the indeterminate generics ($\beta = -2.07; SE = 0.15; z = -14.1$). Rather interesting, indeterminate generics were agreed with \emph{less} likely than chance ($\beta = -0.49; SE = 0.09; z = -5.3$).





# Baseline hypothesis

Does within-kind prevalence (e.g., the % of robins that lay eggs) predict truth judgment?

```{r truthJudge.vs.prevalence}
prevalence.est.path <- "model_results/familiar_generics/prior/within_kind_continuous/"

# Load prevalence data
prev.files <- list.files(prevalence.est.path)
samples = 50000

df.prev <- data.frame()
for (i in prev.files){
  item.prev <- as.data.frame(fread(paste(prevalence.est.path, i, sep = '')))
  df.prev <- bind_rows(df.prev, histToSamples(item.prev, 50000))
  print(i)
}

ggplot(df.prev, aes(x = Prevalence))+
  geom_histogram() + 
  facet_wrap(Property ~ Category)

prev.summary<- df.prev %>%
  group_by(Property, Category) %>%
  summarise(map = estimate_mode(Prevalence),
            credLow = hdi_lower(Prevalence),
            credHigh= hdi_upper(Prevalence)) %>%
  ungroup() %>%
  unite(sentence, Category, Property, sep =' ') %>%
  mutate(sentence = paste(sentence, '.', sep=''))

tj.wPrev<-left_join(tj.bs, 
                    m.summary %>% 
                      rename(prev = map, 
                             prevHigh = credHigh,
                             prevLow = credLow), 
                    by = "sentence")
```



# Model

Load bootstrapped priors model results

```{r}
samples <- 10000

m.path <- "~/Documents/research/generics-paper/model_results/familiar_generics/truth_judgments/bootstrap_prior/"

prefix <- "generics-tj-bootstrapPrior-so1-so2-_IncrMH10000_burn5000_vi"
model.files <- list.files(m.path)

c <- 2 # which subset of 100 runs to examine (c = 1, 2, 3, 4, 5)

# load data from 100 runs
m.samp <- data.frame()
for (i in seq(from = (c-1)*100+1, to = c*100)){
  m.i <- as.data.frame(fread(paste(m.path, model.files[i], sep="")))
  m.samp <- bind_rows(m.samp, histToSamples(m.i, samples))
  print(i)
}
```


## Posterior over model parameters

```{r model.params}

m.params <- m.samp %>% 
  filter( Parameter %in% c("s1_optimality", "s2_optimality") )

ggplot(m.params, aes(x=Value))+
  theme_paper()+
  geom_histogram(aes(y=..count../sum(..count..)), binwidth = 0.1)+
  facet_wrap(~Parameter, scales='free')+
  xlim(0,5)+
  ylab("Posterior probability")

#ggsave(file="~/Documents/research/generics/manuscript/figures/familiar-truthjudgments-parameters.pdf", width = 8, height =4)


m.params %>% 
  group_by(Parameter) %>%
  summarise(postMode = estimate_mode(Value),
            credHi = hdi_upper(Value),
            credLo = hdi_lower(Value))
```